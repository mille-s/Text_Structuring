{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0BvThbbOWguw",
        "0wlGgCYRVs04",
        "eORSytr9VLej",
        "vEQ6kkJWnTgW",
        "L_2w56PwZoxr",
        "zGFmnSGmXwIe",
        "miTw8hM9h7_w"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/Text_Structuring/blob/main/WebNLG_TextStructuring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Shared packages and functions\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Package for parsing xml files (WebNLG 23 and Enhanced WebNLG)\n",
        "! pip install xmltodict\n",
        "\n",
        "# Install SPARQLWrapper for making queries to DBpedia/Wikidata\n",
        "! pip install SPARQLWrapper\n",
        "\n",
        "# datasets is for loading datasets from HuggingFace (WebNLG 17, 18, 20)\n",
        "! pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "! pip install --upgrade gdown\n",
        "\n",
        "! pip install dicttoxml\n",
        "\n",
        "# Clone repos containing WebNLG processing modules and processed data\n",
        "! git clone 'https://github.com/mille-s/Mod-D2T.git'\n",
        "! git clone 'https://github.com/mille-s/M-FleNS_NLG-Pipeline.git'\n",
        "! git clone 'https://github.com/mille-s/UD_Converter.git'\n",
        "# Delete locally to avoid confusion\n",
        "! rm '/content/UD_Converter/UD_Converter_release.ipynb'\n",
        "\n",
        "clear_output()\n",
        "\n",
        "def extractTripleElements(dataset, element):\n",
        "  \"\"\" Returns a list of unique subjects, objects or properties extracted from triple sets\"\"\"\n",
        "  n = ''\n",
        "  if element == 'subject':\n",
        "    n = 0\n",
        "  elif element == 'property':\n",
        "    n = 1\n",
        "  elif element == 'object':\n",
        "    n = 2\n",
        "  else:\n",
        "    print('Error, the second argument of extractTripleElements must be \"subject\", \"property\" or \"object\".')\n",
        "  element_list = []\n",
        "  for entry in dataset:\n",
        "    for input_triple in entry[0]:\n",
        "      element_name = input_triple.split(' | ')[n]\n",
        "      if element_name not in element_list:\n",
        "        element_list.append(element_name)\n",
        "  return(element_list)"
      ],
      "metadata": {
        "id": "VmOq8W6Qcb2G",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Functions for wikidata and dbpedia queries\n",
        "# Function list\n",
        "\n",
        "import requests\n",
        "import csv\n",
        "import re\n",
        "import progressbar\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "bar = ''\n",
        "def createProgressBar(bar, max):\n",
        "  bar = progressbar.ProgressBar(max_value=max)\n",
        "  return(bar)\n",
        "\n",
        "def format_entity_dbp(entity):\n",
        "  \"\"\"\n",
        "  Used for the 2024 experiments\n",
        "  \"\"\"\n",
        "  # Add this line so all lines below have the same variable name on the right\n",
        "  clean_entity = entity\n",
        "  # Remove what is between parentheses; in the end better to keep and escape them\n",
        "  # clean_entity = clean_entity.split('_(',1)[0]\n",
        "  # clean_entity = clean_entity.split(' (',1)[0]\n",
        "  # Replace underscores by spaces (for wikidata)\n",
        "  # clean_entity = re.sub('_', ' ', clean_entity)\n",
        "  # Replace ampersands by 'and' (for dbpedia, seems to affect results from wikidata though)\n",
        "  # clean_entity = re.sub('&', 'and', clean_entity)\n",
        "  # Escape other reserved characters\n",
        "  clean_entity = re.sub('/', '\\/', clean_entity)\n",
        "  clean_entity = re.sub('\\.', '\\.', clean_entity)\n",
        "  clean_entity = re.sub('\\+', '\\+', clean_entity)\n",
        "  clean_entity = re.sub('\\,', '\\,', clean_entity)\n",
        "  clean_entity = re.sub('\\&', '\\&', clean_entity)\n",
        "  clean_entity = re.sub('\\-', '\\-', clean_entity)\n",
        "  clean_entity = re.sub('\\(', '\\(', clean_entity)\n",
        "  clean_entity = re.sub('\\)', '\\)', clean_entity)\n",
        "  # Remove quotes, semi-colons and other things which are usually errors or hacks\n",
        "  clean_entity = re.sub('\"', '', clean_entity)\n",
        "  clean_entity = re.sub(';', '', clean_entity)\n",
        "  clean_entity = re.sub('~', '', clean_entity)\n",
        "  clean_entity = re.sub('<', '', clean_entity)\n",
        "  clean_entity = re.sub('>', '', clean_entity)\n",
        "  # Other\n",
        "  # I checked, it works like this...\n",
        "  clean_entity = re.sub(\"'\", \"\\\\'\", clean_entity)\n",
        "  return clean_entity\n",
        "\n",
        "def format_entity_wkd(entity):\n",
        "  \"\"\"\n",
        "  Used for the GEM 2023-2024 data\n",
        "  \"\"\"\n",
        "  # Remove what is after commas and between parentheses\n",
        "  clean_entity = entity.split(',',1)[0].split('_(',1)[0]\n",
        "  # Replace underscores by spaces (for wikidata)\n",
        "  clean_entity = re.sub('_', ' ', clean_entity)\n",
        "  # Remove quotes\n",
        "  clean_entity = re.sub('\"', '', clean_entity)\n",
        "  return clean_entity\n",
        "\n",
        "def assign_classRegEx(entity):\n",
        "  classRegEx = ''\n",
        "  if re.search('gramPerCubicCentimetres', entity):\n",
        "    classRegEx = 'concentration_gPerCubCm'\n",
        "  if re.search('kilogramPerCubicMetres', entity):\n",
        "    classRegEx = 'concentration_kgPerCubM'\n",
        "  elif re.search('inhabitants per square kilometre', entity):\n",
        "    classRegEx = 'populationDensity'\n",
        "  elif re.search('[0-9\\.,]+.*square.*metre', entity):\n",
        "    classRegEx = 'area_measurement'\n",
        "  elif re.search('bombing', entity):\n",
        "    classRegEx = 'event'\n",
        "  elif re.search('[Uu]niversity', entity):\n",
        "    classRegEx = 'university'\n",
        "  elif re.search('Dodge', entity):\n",
        "    classRegEx = 'car'\n",
        "  elif re.search('^\"*[0-9]{4}-[0-9]{2}-[0-9]{2}\"*$', entity):\n",
        "    classRegEx = 'date'\n",
        "  elif re.search('^\"*[0-9]+\\s*-*(January|Jan|February|Feb|March|Mar|April|Apr|May|June|Jun|July|Jul|August|Aug|September|Sept|October|Oct|November|Nov|December|Dec)\\s*-*[0-9]+\"*$', entity):\n",
        "    classRegEx = 'date'\n",
        "  elif re.search('^\"*(January|February|March|April|May|June|July|August|September|October|November|December)\\s*-*[0-9]{4}\"*$', entity):\n",
        "    classRegEx = 'month'\n",
        "  elif re.search('^\"*(January|February|March|April|May|June|July|August|September|October|November|December)\"*$', entity):\n",
        "    classRegEx = 'month'\n",
        "  elif re.search('^\"*[0-9\\.,]+.*(litres|cubic)', entity):\n",
        "    classRegEx = 'volume_measurement'\n",
        "  elif re.search('^\"*[0-9\\.,]+\\s*m\"*$', entity):\n",
        "    classRegEx = 'distance_meters'\n",
        "  elif re.search('^\"*[0-9\\.,]+\\s*in\"*$', entity):\n",
        "    classRegEx = 'distance_inches'\n",
        "  elif re.search('^\"*[0-9\\.,]+\\s*yd\"*$', entity):\n",
        "    classRegEx = 'distance_yards'\n",
        "  elif re.search('^\"*[0-9\\.,]+\\s*ft\"*$', entity):\n",
        "    classRegEx = 'distance_feet'\n",
        "  elif re.search('[0-9\\.,]+.*millimetres', entity):\n",
        "    classRegEx = 'distance_millimetres'\n",
        "  elif re.search('[0-9\\.,]+.*centimetres', entity):\n",
        "    classRegEx = 'distance_centimetres'\n",
        "  elif re.search('[0-9\\.,]+.*metres', entity):\n",
        "    classRegEx = 'distance_metres'\n",
        "  elif re.search('[0-9\\.,]+.*inches', entity):\n",
        "    classRegEx = 'distance_inches'\n",
        "  elif re.search('[0-9\\.,]+.*yards', entity):\n",
        "    classRegEx = 'distance_yards'\n",
        "  elif re.search('[0-9\\.,]+.*feet', entity):\n",
        "    classRegEx = 'distance_feet'\n",
        "  elif re.search('[0-9\\.,]+.*seconds', entity):\n",
        "    classRegEx = 'duration_seconds'\n",
        "  elif re.search('[0-9\\.,]+.*minutes', entity):\n",
        "    classRegEx = 'duration_minutes'\n",
        "  elif re.search('[0-9\\.,]+.*hours', entity):\n",
        "    classRegEx = 'duration_hours'\n",
        "  elif re.search('[0-9\\.,]+.*days', entity):\n",
        "    classRegEx = 'duration_days'\n",
        "  elif re.search('[0-9\\.,]+.*weeks', entity):\n",
        "    classRegEx = 'duration_weeks'\n",
        "  elif re.search('[0-9\\.,]+.*months', entity):\n",
        "    classRegEx = 'duration_months'\n",
        "  elif re.search('[0-9\\.,]+.*years', entity):\n",
        "    classRegEx = 'duration_years'\n",
        "  elif re.search('[0-9\\.,]+.* (engine|horsepower)', entity):\n",
        "    classRegEx = 'engine'\n",
        "  elif re.search('[0-9\\.,]+.*euros', entity):\n",
        "    classRegEx = 'moneyQuantity_euros'\n",
        "  elif re.search('[0-9\\.,]+.*dollars', entity):\n",
        "    classRegEx = 'moneyQuantity_dollars'\n",
        "  elif re.search('[0-9\\.,]+.*kilometrePerSeconds', entity):\n",
        "    classRegEx = 'speed_kmPerSec'\n",
        "  elif re.search('[0-9\\.,]+.*degreeCelsius', entity):\n",
        "    classRegEx = 'temperature_celsius'\n",
        "  elif re.search('[0-9\\.,]+.*kelvins', entity):\n",
        "    classRegEx = 'temperature_kelvin'\n",
        "  elif re.search('[0-9\\.,]+-speed', entity):\n",
        "    classRegEx = 'transmission'\n",
        "  elif re.search('^\"*[0-9\\.,]+.*(\\sg|grams)', entity):\n",
        "    classRegEx = 'weight_grams'\n",
        "  elif re.search('^\"*[0-9\\.,]+.*\\skg', entity):\n",
        "    classRegEx = 'weight_kilograms'\n",
        "  elif re.search('^\"*[0-9\\.,]+.*tonnes', entity):\n",
        "    classRegEx = 'weight_tonnes'\n",
        "  elif re.search('^\"*[0-9\\.,]+.*pounds', entity):\n",
        "    classRegEx = 'weight_pounds'\n",
        "  elif re.search('^\"*[0-9]+/[0-9]+\"*$', entity):\n",
        "    classRegEx = 'fraction'\n",
        "  elif re.search('^\"*[0-9a-zA-Z]+/[0-9a-zA-Z\\s\\']+\"*$', entity):\n",
        "    classRegEx = 'runwayName'\n",
        "  elif re.search('^\"*[0-9]{4}[-–][0-9]{4}\"*$', entity):\n",
        "    classRegEx = 'issnNumber'\n",
        "  elif re.search('^\"*[0-9]+[-–][0-9]+[-–][0-9]+[-–][0-9]+[-–]*[0-9]*\"*$', entity):\n",
        "    classRegEx = 'isbnNumber'\n",
        "  elif re.search('^\"*[0-9]+[-–][0-9]+[-–]*[0-9]*[-–]*[0-9]*[-–]*[0-9]*\"*$', entity):\n",
        "    classRegEx = 'unknownIdentifier'\n",
        "  elif re.search('^\"*[0-9]{2} [a-zA-Z]+\"*$', entity):\n",
        "    classRegEx = 'celestialBody'\n",
        "  elif re.search('^\"*[0-9]{3} [a-zA-Z]+\"*$', entity):\n",
        "    classRegEx = 'celestialBody'\n",
        "  elif re.search('^\"*[0-9]+_[a-zA-Z]+\"*$', entity):\n",
        "    classRegEx = 'celestialBody'\n",
        "  elif re.search('_FC_', entity):\n",
        "    classRegEx = 'footballClub'\n",
        "  elif re.search('(season|EPSTH|league|League|Liga|Season|Bundesliga|Eredivisie|Football_Conference|Lega_Pro|Regionalliga|Serie_A|Serie_B|Topklasse|Campeonato)', entity):\n",
        "    classRegEx = 'sportsSeason'\n",
        "  elif re.search('[Mm]onument', entity):\n",
        "    classRegEx = 'monument'\n",
        "  elif re.search('^\"*[0-9]{4} [a-zA-Z]+', entity):\n",
        "    classRegEx = 'celestialBody'\n",
        "  elif re.search('^\"*[0-9-]+[stndr]*[\\s\\-_][^:]*[\\(\\)a-zA-Z\\']+\"*$', entity):\n",
        "    if not re.search('JD2457600', entity):\n",
        "      classRegEx = 'address'\n",
        "    else:\n",
        "      classRegEx = 'date_epoch'\n",
        "  elif re.search('^\"*[\\+-]*[0-9\\.,]+\"*$', entity):\n",
        "    classRegEx = 'unknownQuantity'\n",
        "  elif re.search('[0-9\\.,]+, [0-9\\.,]+', entity):\n",
        "    classRegEx = 'unknownQuantity_multiple'\n",
        "\n",
        "  return(classRegEx)\n",
        "\n",
        "# Code below adapted from ChatGPT\n",
        "def get_wikidata_id(entity_label):\n",
        "  # Define the Wikidata API endpoint\n",
        "  wikidata_api_url = \"https://www.wikidata.org/w/api.php\"\n",
        "\n",
        "  # Set the parameters for the API request\n",
        "  params = {\n",
        "    \"action\": \"wbsearchentities\",\n",
        "    \"format\": \"json\",\n",
        "    \"language\": \"en\",  # You can change the language if needed\n",
        "    \"search\": entity_label,\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    # Send a GET request to the Wikidata API\n",
        "    response = requests.get(wikidata_api_url, params=params)\n",
        "    response.raise_for_status()\n",
        "    # Parse the JSON response\n",
        "    data = response.json()\n",
        "    # Check if any entities were found\n",
        "    if \"search\" in data and data[\"search\"]:\n",
        "      # Get the first entity (assuming it's the most relevant)\n",
        "      entity_id = data[\"search\"][0][\"id\"]\n",
        "      return entity_id\n",
        "    return None  # Entity not found\n",
        "\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(\"Error connecting to the Wikidata API:\", e)\n",
        "    return None\n",
        "\n",
        "# Example usage:\n",
        "# entity_label = '23 g'\n",
        "# wikidata_id = get_wikidata_id(entity_label)\n",
        "# if wikidata_id:\n",
        "#   print(f\"The Q-ID for {entity_label} is {wikidata_id}.\")\n",
        "# else:\n",
        "#   print(f\"No entity found for {entity_label}.\")\n",
        "# print(assign_classRegEx(entity_label))\n",
        "\n",
        "def get_wikidata_id_bulk(rows, list_entities, bar):\n",
        "  bar = createProgressBar(bar, len(list_entities)-1)\n",
        "  for count, entity in enumerate(list_entities):\n",
        "    bar.update(count)\n",
        "    row = []\n",
        "    clean_entity = ''\n",
        "    if entity == 'School of Business and Social Sciences at the Aarhus University':\n",
        "      clean_entity = 'Aarhus School of Business'\n",
        "    else:\n",
        "      clean_entity = format_entity_wkd(entity)\n",
        "    wikidata_id = get_wikidata_id(clean_entity)\n",
        "    # wikidata_id = None\n",
        "    if wikidata_id:\n",
        "      # print(f\"The Q-ID for {entity} is {wikidata_id}.\")\n",
        "      row.append(wikidata_id)\n",
        "      row.append(clean_entity)\n",
        "      row.append(entity)\n",
        "      row.append(assign_classRegEx(entity))\n",
        "    else:\n",
        "      # print(f\"No entity found for {entity}.\")\n",
        "      row.append('???')\n",
        "      row.append(clean_entity)\n",
        "      row.append(entity)\n",
        "      row.append(assign_classRegEx(entity))\n",
        "    rows.append(row)\n",
        "\n",
        "# ChatGPT prompt: Please write some Python code to get the value of an named entity's \"gold:hypernym\" property according to DBpedia\n",
        "def get_dbpedia_hypernym(entity_name):\n",
        "  entity_name = format_entity_dbp(entity_name)\n",
        "  # For DBpedia specifically, we need to replace spaces by underscores in entity names to avoid query errors\n",
        "  entity_name = ('_').join(entity_name.split(' '))\n",
        "\n",
        "  # Set up the SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query\n",
        "  query = f\"\"\"\n",
        "  SELECT ?hypernym\n",
        "  WHERE {{\n",
        "    dbr:{entity_name} gold:hypernym ?hypernym\n",
        "  }}\n",
        "    \"\"\"\n",
        "\n",
        "  # Set the query and response format\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  # Execute the query\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  # Extract and return the hypernym value\n",
        "  if 'results' in results and 'bindings' in results['results']:\n",
        "    bindings = results['results']['bindings']\n",
        "    # print(bindings)\n",
        "    # Return the first value only\n",
        "    if bindings:\n",
        "      if re.search('/', bindings[0]['hypernym']['value']):\n",
        "        return bindings[0]['hypernym']['value'].rsplit('/',1)[1]\n",
        "      else:\n",
        "        return bindings[0]['hypernym']['value']\n",
        "\n",
        "    return None\n",
        "\n",
        "# ChatGPT prompt: Please write some Python code to get the value of an named entity's \"gold:hypernym\" property according to Wikidata\n",
        "def get_wikidata_hypernym(entity_ID):\n",
        "  # Define the Wikidata Query Service endpoint URL\n",
        "  wikidata_endpoint = \"https://query.wikidata.org/sparql\"\n",
        "\n",
        "  # Define the SPARQL query\n",
        "  query = f\"\"\"\n",
        "  SELECT ?hypernymLabel\n",
        "  WHERE {{\n",
        "    wd:{entity_ID} wdt:P31 ?hypernym.\n",
        "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
        "  }}\n",
        "  \"\"\"\n",
        "\n",
        "  # Set up the request headers\n",
        "  headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "    'Accept': 'application/json'\n",
        "  }\n",
        "\n",
        "  # Set up the request parameters\n",
        "  params = {\n",
        "    'query': query,\n",
        "    'format': 'json'\n",
        "  }\n",
        "\n",
        "  # Make the API request\n",
        "  response = requests.get(wikidata_endpoint, headers=headers, params=params)\n",
        "\n",
        "  # Parse the JSON response\n",
        "  data = response.json()\n",
        "\n",
        "  # Extract and return the hypernym value\n",
        "  if 'results' in data and 'bindings' in data['results']:\n",
        "    bindings = data['results']['bindings']\n",
        "    if bindings:\n",
        "      return bindings[0]['hypernymLabel']['value']\n",
        "\n",
        "  return None\n",
        "\n",
        "def get_Wikidata_id_property(dbpedia_prop):\n",
        "  # Set up the SPARQL endpoint\n",
        "\n",
        "  wikidata_ids = []\n",
        "\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query to get the Wikidata ID for a DBpedia property\n",
        "  query = f\"\"\"\n",
        "  SELECT ?wikidataProperty\n",
        "  WHERE {{\n",
        "    <{dbpedia_prop}> owl:equivalentProperty ?wikidataProperty .\n",
        "    FILTER(STRSTARTS(STR(?wikidataProperty), \"http://www.wikidata.org/entity/\"))\n",
        "  }}\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  try:\n",
        "    # Execute the query\n",
        "    results = sparql.query().convert()\n",
        "\n",
        "    # Extract and print the Wikidata property ID\n",
        "    wikidata_properties = [result[\"wikidataProperty\"][\"value\"] for result in results[\"results\"][\"bindings\"]]\n",
        "    # print(wikidata_properties)\n",
        "    for prop in wikidata_properties:\n",
        "      # Extract the Wikidata ID (e.g., \"P569\") from the full URL\n",
        "      wikidata_id = prop.split('/')[-1]\n",
        "      if wikidata_id not in wikidata_ids:\n",
        "        wikidata_ids.append(wikidata_id)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "  return(wikidata_ids)"
      ],
      "metadata": {
        "id": "GoIxUIfCpvqz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhanced WebNLG"
      ],
      "metadata": {
        "id": "L_2w56PwZoxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make text planning dataset"
      ],
      "metadata": {
        "id": "1dnmV2OTtT6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load and download data\n",
        "# For enhanced WebNLG\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import os.path\n",
        "import shutil\n",
        "\n",
        "def clear_folder(folder):\n",
        "  \"Function to clear whole folders.\"\n",
        "  if os.path.exists(folder) and os.path.isdir(folder):\n",
        "    try:\n",
        "      shutil.rmtree(folder)\n",
        "    except Exception as e:\n",
        "      print('Failed to delete %s. Reason: %s' % (folder, e))\n",
        "\n",
        "dataset = load_dataset(\"enriched_web_nlg\", \"en\", trust_remote_code=True)\n",
        "\n",
        "# Clone original data\n",
        "!git clone https://github.com/ThiagoCF05/webnlg.git\n",
        "path_data_en = '/content/webnlg/data/v2.0/en'\n",
        "\n",
        "splits = ['train', 'dev', 'test']\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# Problem: the HuggingFace dataset does not contain the sentence groupings...\n",
        "# data = dataset['dev']\n",
        "\n",
        "# # Accessing by brackets does not have a default but the \"get\" method does and the default is None.\n",
        "# for c, entry in enumerate(data):\n",
        "#   if c == 401:\n",
        "#     print(f'Entry #{c}')\n",
        "#     texts = entry.get('lex').get('text')\n",
        "#     print(texts)\n",
        "#     sorted_triple_sets = entry.get('lex').get('sorted_triple_sets')\n",
        "#     for sorted_triple_set in sorted_triple_sets:\n",
        "#       print(sorted_triple_set)\n",
        "#       #extractTripleElements(sorted_triple_set)"
      ],
      "metadata": {
        "id": "_4941woNZuAS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clean data\n",
        "# There seems to be a problem with the original data, so I need to clean it\n",
        "import re\n",
        "import codecs\n",
        "import glob\n",
        "\n",
        "new_path = '/content/webnlg/data/v2.1/en'\n",
        "if os.path.exists(new_path):\n",
        "  clear_folder(new_path)\n",
        "else:\n",
        "  os.makedirs(new_path)\n",
        "  for split in splits:\n",
        "    new_subfolder = os.path.join(new_path,split)\n",
        "    os.makedirs(new_subfolder)\n",
        "\n",
        "path_original_data = '/content/webnlg/data/v2.0/en'\n",
        "# Get to the 3 subfolders for each data split\n",
        "for split in splits:\n",
        "  path_to_open = os.path.join(path_original_data, split)\n",
        "  # Get to the (up to) 7 subfolders for each input size\n",
        "  for triple_size_folder in glob.glob(os.path.join(path_to_open,'*triples')):\n",
        "    # create folder for each triple\n",
        "    new_triple_size_folder = re.sub('2.0', '2.1', triple_size_folder)\n",
        "    os.makedirs(new_triple_size_folder)\n",
        "    # Get to the input files\n",
        "    for xml_file in glob.glob(os.path.join(triple_size_folder, '*.xml')):\n",
        "      out_path = re.sub('2.0', '2.1', xml_file)\n",
        "      lines = codecs.open(xml_file, 'r', 'utf-8').readlines()\n",
        "      # Create new file\n",
        "      with codecs.open(out_path, 'w', 'utf-8') as fo:\n",
        "        for line in lines:\n",
        "          # Do not write lines that look suspicious\n",
        "          if re.search('^\\s*<[^\\s]+/>\\n', line):\n",
        "            pass\n",
        "          else:\n",
        "            fo.write(line)"
      ],
      "metadata": {
        "id": "r6Un5Q0G2oX7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save a version of the test data for running Thiago's evaluation\n",
        "import glob\n",
        "import codecs\n",
        "import os\n",
        "import re\n",
        "\n",
        "#  The test data should contain inputs of size 2 and above, ordered by triple size, and for each size ordered alphabetically by category, and for each category ordered by eid.\n",
        "# I don't remember if this was used in the en or not; not trace of out_path_testFile_tcf or of Enhanced_WebNLG17 in my code...\n",
        "\n",
        "# All the cleaned stuff came from the lex field; in the last version of the data we remove this field so no more difference between 2.0 and 2.1 now.\n",
        "data_to_use = 'v2.0'#@param['v2.0', 'v2.1']\n",
        "\n",
        "in_path_testFile_tcf = os.path.join('/content/webnlg/data', data_to_use, 'en', 'test')\n",
        "out_path_testFile_tcf = os.path.join('/content/webnlg/data', data_to_use, 'en', 'test', 'Enhanced_WebNLG17-test_2-7.xml')\n",
        "\n",
        "lines_to_ignore = ['<?xml version=\"1.0\" ?>', '<benchmark>', '</benchmark>', '<entries>', '</entries>', '']\n",
        "\n",
        "with codecs.open(out_path_testFile_tcf, 'w', 'utf-8') as fo_test_tcf:\n",
        "  # Keep folders sorted to maintain alignment with Thiago's files\n",
        "  fo_test_tcf.write('<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\\n<benchmark>\\n  <entries>\\n')\n",
        "  for triple_size_folder in sorted(glob.glob(os.path.join(in_path_testFile_tcf,'*triples'))):\n",
        "    # Exclude 1 triples (not useful for sentence packaging).\n",
        "    if not re.search('1triple', triple_size_folder):\n",
        "      # Get to the input files (keep them sorted to maintain alignment with Thiago's files)\n",
        "      for xml_file_tcf in sorted(glob.glob(os.path.join(triple_size_folder, '*.xml'))):\n",
        "        print(f'------------------------\\n{xml_file_tcf}\\n------------------------')\n",
        "        lines_xml_file_tcf = codecs.open(xml_file_tcf, 'r', 'utf-8').readlines()\n",
        "        # Need to replace <otriple> by <mtriple> in <modifiedtripleset>\n",
        "        within_modified_triple_field = False\n",
        "        within_lex_field = False\n",
        "        for line_xml_file_tcf in lines_xml_file_tcf:\n",
        "          # Process lines\n",
        "          if re.search('<modifiedtripleset>', line_xml_file_tcf):\n",
        "            within_modified_triple_field = True\n",
        "          if re.search('</modifiedtripleset>', line_xml_file_tcf):\n",
        "            within_modified_triple_field = False\n",
        "          if re.search('<lex ', line_xml_file_tcf):\n",
        "            within_lex_field = True\n",
        "          if re.search('</lex>', line_xml_file_tcf):\n",
        "            within_lex_field = False\n",
        "          if re.search('</modifiedtripleset>', line_xml_file_tcf):\n",
        "            within_modified_triple_field = False\n",
        "          if re.search('<otriple>', line_xml_file_tcf) and within_modified_triple_field == True:\n",
        "            line_xml_file_tcf = re.subn('<otriple>', '<mtriple>', line_xml_file_tcf)[0]\n",
        "          if re.search('</otriple>', line_xml_file_tcf) and within_modified_triple_field == True:\n",
        "            line_xml_file_tcf = re.subn('</otriple>', '</mtriple>', line_xml_file_tcf)[0]\n",
        "          while re.search('\\t', line_xml_file_tcf):\n",
        "            line_xml_file_tcf = re.subn('\\t', '  ', line_xml_file_tcf)[0]\n",
        "          # Write out file\n",
        "          if line_xml_file_tcf.strip() in lines_to_ignore :\n",
        "            pass\n",
        "          elif within_lex_field == True :\n",
        "            pass\n",
        "          elif re.search('</lex>', line_xml_file_tcf) :\n",
        "            pass\n",
        "          else:\n",
        "            fo_test_tcf.write(line_xml_file_tcf)\n",
        "  fo_test_tcf.write('  </entries>\\n</benchmark>')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WlB8r5y69wly",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract sentence groupings\n",
        "import glob\n",
        "import xmltodict\n",
        "import xml.etree.ElementTree as ET\n",
        "import xmltodict\n",
        "import json\n",
        "\n",
        "# splits = ['dev']\n",
        "splits = ['dev', 'test', 'train']\n",
        "exclude_input_size = 'size1'#@param['none', 'size1', 'size1&2']\n",
        "\n",
        "def getOriginalInputTriples(entry, split):\n",
        "  orig_triples = []\n",
        "  if split == 'train' or split == 'dev':\n",
        "    if isinstance(entry['modifiedtripleset']['mtriple'], list):\n",
        "      for original_input_triple in entry['modifiedtripleset']['mtriple']:\n",
        "        orig_triples.append(original_input_triple)\n",
        "    else:\n",
        "      orig_triples.append(entry['modifiedtripleset']['mtriple'])\n",
        "  # In the test data, the mtriple field was replaced by otriple\n",
        "  elif split == 'test':\n",
        "    if isinstance(entry['modifiedtripleset']['otriple'], list):\n",
        "      for original_input_triple in entry['modifiedtripleset']['otriple']:\n",
        "        orig_triples.append(original_input_triple)\n",
        "    else:\n",
        "      orig_triples.append(entry['modifiedtripleset']['otriple'])\n",
        "  return orig_triples\n",
        "\n",
        "def extract_groups(text_planning_data, split, data_point, dtp_count, count_input_num, xml_file, metadata):\n",
        "  #To catch ill-formed data points (see else of this condition)\n",
        "  if 'sortedtripleset' in data_point:\n",
        "    if 'sentence' in data_point['sortedtripleset']:\n",
        "      text_planning_data[split].append([])\n",
        "      # For each datapoint, there is one or more sentence(s); if there are 2 or more sentences, we have a list in data_point['sortedtripleset']['sentence']\n",
        "      if isinstance(data_point['sortedtripleset']['sentence'], list):\n",
        "        sent_count = 0\n",
        "        for ref_sentence in data_point['sortedtripleset']['sentence']:\n",
        "          # For each sentence, there is one or more triple(s); if there are a least 2 triples, ref_sentence['striple'] is a list\n",
        "          if 'striple' in ref_sentence:\n",
        "            text_planning_data[split][dtp_count].append([])\n",
        "            if isinstance(ref_sentence['striple'], list):\n",
        "              for triple in ref_sentence['striple']:\n",
        "                # print(f\"List sentences list triple: {triple}\")\n",
        "                text_planning_data[split][dtp_count][sent_count].append(triple)\n",
        "            # If there is only one triple in a sentence, no list\n",
        "            else:\n",
        "              # print(f\"List sentences individual triple: {ref_sentence['striple']}\")\n",
        "              text_planning_data[split][dtp_count][sent_count].append(ref_sentence['striple'])\n",
        "            sent_count += 1\n",
        "          else:\n",
        "            print(f'ERROR Data \"striple\" field not found: {xml_file}, input#{count_input_num}')\n",
        "      # On the contrary, if there is only one sentence, there is no list in data_point['sortedtripleset']['sentence']\n",
        "      else:\n",
        "        # Same as above: for each sentence, there is one or more triple(s); if there are a least 2 triples, data_point['sortedtripleset']['sentence']['striple'] is a list\n",
        "        if 'striple' in data_point['sortedtripleset']['sentence']:\n",
        "          # Adding a list although there will be only one text to maintain consistent formatting in the whole dataset\n",
        "          text_planning_data[split][dtp_count].append([])\n",
        "          if isinstance(data_point['sortedtripleset']['sentence']['striple'], list):\n",
        "            for triple in data_point['sortedtripleset']['sentence']['striple']:\n",
        "              # print(f\"Single sentence list member: {triple}\")\n",
        "              text_planning_data[split][dtp_count][0].append(triple)\n",
        "          # If there is only one triple in a sentence, no list\n",
        "          else:\n",
        "            # print(f\"Single sentence individual triple: {data_point['sortedtripleset']['sentence']['striple']}\")\n",
        "            text_planning_data[split][dtp_count][0].append(data_point['sortedtripleset']['sentence']['striple'])\n",
        "        else:\n",
        "          print(f'ERROR Data \"striple\" field not found: {xml_file}, input#{count_input_num}')\n",
        "      # print(f'   {text_planning_data[split][dtp_count]}')\n",
        "      # add metadata\n",
        "      text_planning_data[split][dtp_count].append(metadata)\n",
        "      # Update counter after each text\n",
        "      dtp_count += 1\n",
        "    else:\n",
        "      print(f'ERROR Data \"sentence\" field not found: {xml_file}, input#{count_input_num}')\n",
        "  else:\n",
        "    print(f'ERROR Data \"sortedtripleset\" field not found: {xml_file}, input#{count_input_num}')\n",
        "  # Check if an empty list was created because no sentence was found in the dataset:\n",
        "  if len(text_planning_data[split][dtp_count-1]) == 0:\n",
        "    print(f'AAAAAAAAAAAAAAH empty data point (no text): {xml_file}, input#{count_input_num}')\n",
        "  # If the list (text) is not empty, check if it contains a list (sentence) that is empty (shouldn't happen given how these lists are built)\n",
        "  else:\n",
        "    for sentence in text_planning_data[split][dtp_count-1]:\n",
        "      if len(sentence) == 0:\n",
        "        print(f'OOOH empty sentence: {xml_file}, input#{count_input_num}')\n",
        "  return(dtp_count)\n",
        "\n",
        "# This will be a dictionary with 3 keys, one per split, and for each key, a list of text contents; each list element is a list of sentences, each of which is a list of one or more triple(s).\n",
        "# Metadata of the input triple the sentence comes from is stored at the end of each list of sentences in another list\n",
        "# E.g. text_planning_data['train'][0] = [['103_Colmore_Row | location | \"Colmore Row, Birmingham, England\"', '103_Colmore_Row | completionDate | 1976', '103_Colmore_Row | architect | John_Madin'], ['103_Colmore_Row | floorCount | 23'], ['Building', 'Id1', '4']]\n",
        "text_planning_data = {}\n",
        "\n",
        "path_data_en = '/content/webnlg/data/v2.1/en'\n",
        "# Get to the 3 subfolders for each data split\n",
        "for split in splits:\n",
        "  text_planning_data[split] = []\n",
        "  path_to_open = os.path.join(path_data_en, split)\n",
        "  # Initialise a counter of data points (one data point per reference text, as opposed to one data point per input, since each text is a different data point due to different text plan)\n",
        "  # I don't separate the training data into size batches\n",
        "  dtp_count = 0\n",
        "  # Get to the (up to) 7 subfolders for each input size (keep them sorted to maintain alignment with Thiago's files)\n",
        "  for triple_size_folder in sorted(glob.glob(os.path.join(path_to_open,'*triples'))):\n",
        "    # Exclude 1 triples (not useful for sentence packaging); also excluding 2 triples, which may create noise because in many cases one will want one sentence.\n",
        "    if (exclude_input_size == 'size1' and re.search('1triple', triple_size_folder)) or (exclude_input_size == 'size1&2' and (re.search('1triple', triple_size_folder) or re.search('2triples', triple_size_folder))):\n",
        "      pass\n",
        "    else:\n",
        "      # Get to the input files (keep them sorted to maintain alignment with Thiago's files)\n",
        "      for xml_file in sorted(glob.glob(os.path.join(triple_size_folder, '*.xml'))):\n",
        "        print(f'------------------------\\n{xml_file}\\n------------------------')\n",
        "        # To capture input number for debugging (numbering starts at 1 in files)\n",
        "        count_input_num = 1\n",
        "        # if xml_file == '/content/webnlg/data/v2.0/en/dev/2triples/Building.xml':\n",
        "        #to change the encoding type to be able to set it to the one you need\n",
        "        tree = ET.parse(xml_file)\n",
        "        xml_data = tree.getroot()\n",
        "        xmlstr = ET.tostring(xml_data, encoding='utf-8', method='xml')\n",
        "        input_file_dict = dict(xmltodict.parse(xmlstr))\n",
        "        # If there are several entries in the dataset\n",
        "        if isinstance(input_file_dict['benchmark']['entries']['entry'], list):\n",
        "          # Get to the list that contains every input as a list element\n",
        "          for entry in input_file_dict['benchmark']['entries']['entry']:\n",
        "            # Get metadata for each input (also keep original input as part of the metadata (ugly but easier at this point))\n",
        "            metadata = [str(split), entry['@category'], entry['@eid'], entry['@size'], getOriginalInputTriples(entry, split)]\n",
        "            # For each input, there is one data point per reference text\n",
        "            # If there are several reference texts, entry['lex'] will be a list, otherwise not\n",
        "            if isinstance(entry['lex'], list):\n",
        "              for data_point in entry['lex']:\n",
        "                # Update dtp count when calling the function\n",
        "                dtp_count = extract_groups(text_planning_data, split, data_point, dtp_count, count_input_num, xml_file, metadata)\n",
        "            # If entry['lex'] is not a list, do all the same as above but replacing data_point per entry['lex']\n",
        "            else:\n",
        "              # Update dtp count when calling the function\n",
        "              dtp_count = extract_groups(text_planning_data, split, entry['lex'], dtp_count, count_input_num, xml_file, metadata)\n",
        "            count_input_num += 1\n",
        "        # If there is only one entry in the dataset (e.g. test 5 triples comicsCharacters)\n",
        "        else:\n",
        "          # Get metadata for each input\n",
        "          metadata = [str(split), input_file_dict['benchmark']['entries']['entry']['@category'], input_file_dict['benchmark']['entries']['entry']['@eid'], input_file_dict['benchmark']['entries']['entry']['@size'], getOriginalInputTriples(input_file_dict['benchmark']['entries']['entry'], split)]\n",
        "          if isinstance(input_file_dict['benchmark']['entries']['entry']['lex'], list):\n",
        "            for data_point2 in input_file_dict['benchmark']['entries']['entry']['lex']:\n",
        "              # Update dtp count when calling the function\n",
        "              dtp_count = extract_groups(text_planning_data, split, data_point2, dtp_count, count_input_num, xml_file, metadata)\n",
        "          # If entry['lex'] is not a list, do all the same as above but replacing data_point per entry['lex']\n",
        "          else:\n",
        "            # Update dtp count when calling the function\n",
        "            dtp_count = extract_groups(text_planning_data, split, input_file_dict['benchmark']['entries']['entry']['lex'], dtp_count, count_input_num, xml_file, metadata)\n",
        "          count_input_num += 1\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# for dtpX in text_planning_data['test']:\n",
        "#   print(dtpX)"
      ],
      "metadata": {
        "id": "P283Gn96h1m7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Debug: Print test data\n",
        "for dtpX in text_planning_data['test']:\n",
        "  print(dtpX)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "2nO_zZjFoEf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create raw input/output pairs\n",
        "from collections import Counter\n",
        "\n",
        "print(f\"Sample raw data point: {text_planning_data['train'][0]}\")\n",
        "\n",
        "def camelCaseClean(text):\n",
        "  words = [[text[0]]]\n",
        "  for c in text[1:]:\n",
        "    if words[-1][-1].islower() and c.isupper():\n",
        "      words.append(list(c.lower()))\n",
        "    else:\n",
        "      words[-1].append(c)\n",
        "  words = [''.join(word) for word in words]\n",
        "  cleaned = ' '.join(words)\n",
        "  return cleaned\n",
        "\n",
        "class TP_Datapoint:\n",
        "  def __init__(self, input_triple_list, text_plan, orig_input):\n",
        "    # input: [<__main__.Triple object at 0x7850dbe26590>, <__main__.Triple object at 0x7850dbe265c0>, <__main__.Triple object at 0x7850dbe265f0>, <__main__.Triple object at 0x7850dbe26620>]\n",
        "    # output: [[<__main__.Triple object at 0x7850dbe26590>], [<__main__.Triple object at 0x7850dbe265c0>, <__main__.Triple object at 0x7850dbe265f0>, <__main__.Triple object at 0x7850dbe26620>]]\n",
        "    # original_input: [<__main__.Triple object at 0x7850dbe26590>, <__main__.Triple object at 0x7850dbe265c0>, <__main__.Triple object at 0x7850dbe265f0>, <__main__.Triple object at 0x7850dbe26620>]\n",
        "    self.input = input_triple_list\n",
        "    self.output = text_plan\n",
        "    self.original_input = orig_input\n",
        "\n",
        "class Triple:\n",
        "\n",
        "  def camelCaseCleanProperty(self, triple_property):\n",
        "    word_letters = [[triple_property[0]]]\n",
        "    for c in triple_property[1:]:\n",
        "      if word_letters[-1][-1].islower() and c.isupper():\n",
        "        word_letters.append(list(c.lower()))\n",
        "      else:\n",
        "        word_letters[-1].append(c)\n",
        "    word_letters = [''.join(letter) for letter in word_letters]\n",
        "    cleaned = ' '.join(word_letters)\n",
        "    # Remove underscore too\n",
        "    cleaned = re.subn('_', ' ', cleaned)[0]\n",
        "    return cleaned\n",
        "\n",
        "  def __init__(self, triple, metadata):\n",
        "    # List of common prepositions found on the net\n",
        "    prepositions_list = ['above', 'across', 'against', 'along', 'among', 'around', 'at', 'before', 'behind', 'below', 'beneath', 'beside', 'between', 'by', 'down', 'from', 'in', 'into', 'near', 'of', 'off', 'on', 'to', 'toward', 'under', 'upon', 'with', 'within']\n",
        "    # Metadata of the triple set\n",
        "    self.data_split = metadata[0]\n",
        "    self.category = metadata[1]\n",
        "    self.eid = metadata[2]\n",
        "    self.size = metadata[3]\n",
        "    # Will be used to sort the data as used by Thiago\n",
        "    self.uniqueInputID = f'{metadata[0]}_{metadata[1]}_{ metadata[3]}_{ metadata[2]}'\n",
        "    self.subject_label = triple.split(' | ')[0]\n",
        "    self.property_label = triple.split(' | ')[1]\n",
        "    self.object_label = triple.split(' | ')[2]\n",
        "    self.subject_class_wkd = '_'\n",
        "    self.subject_class_dbp = '_'\n",
        "    self.subject_class_regex = '_'\n",
        "    self.subject_class_merged = '_'\n",
        "    self.subject_class_abstract = '_'\n",
        "    self.subject_id_wkd = None\n",
        "    # To store the entity ID in the input (and keep track of coreference between entities)\n",
        "    self.subject_id_input = None\n",
        "    self.property_class = '_'\n",
        "    self.property_lemma= '_'\n",
        "    self.object_class_wkd = '_'\n",
        "    self.object_class_dbp = '_'\n",
        "    self.object_class_regex = '_'\n",
        "    self.object_class_merged = '_'\n",
        "    self.object_class_abstract = '_'\n",
        "    self.object_id_wkd = None\n",
        "    # To store the entity ID in the input (and keep track of coreference between entities)\n",
        "    self.object_id_input = None\n",
        "    # remove parentheses, replace underscores by spaces, split camel casing and add spaces?\n",
        "    self.property_label_split = self.camelCaseCleanProperty(self.property_label)\n",
        "    # Assign a lemma to the property\n",
        "    # If there are several words in the porperty label\n",
        "    if len(self.property_label_split.split(' ')) > 1:\n",
        "      has_prep = False\n",
        "      for property_label_part in self.property_label_split.split(' '):\n",
        "        # If there is a preposition somewhere in the label, take the first word as the lemma (that's very simplisitc but works in most cases)\n",
        "        if property_label_part in prepositions_list:\n",
        "          # index_part = self.property_label_split.split(' ').index(property_label_part)\n",
        "          has_prep = True\n",
        "          # print(f'{self.property_label_split} -> {self.property_lemma}')\n",
        "      if has_prep == True:\n",
        "        self.property_lemma = self.property_label_split.split(' ')[0]\n",
        "      # Otherwise, take the last word\n",
        "      else:\n",
        "        self.property_lemma = self.property_label_split.split(' ')[-1]\n",
        "    # If there is only one word in the property name, just use that word\n",
        "    else:\n",
        "      self.property_lemma = self.property_label\n",
        "\n",
        "# final_data will be a dictionary with 3 keys, one per split.\n",
        "# Each split is a list of datapoints.\n",
        "# Each datapoint is a TP_Datapoint object with 2 features: input (list of Triple objects), output (list of list of Triple objects)\n",
        "# Sample input/output pair:\n",
        "# \tInput: [<__main__.Triple object at 0x7ef56ed13ac0>, <__main__.Triple object at 0x7ef56ed13af0>, <__main__.Triple object at 0x7ef56ed13b20>, <__main__.Triple object at 0x7ef56ed13b50>, <__main__.Triple object at 0x7ef56ed13b80>, <__main__.Triple object at 0x7ef56ed13bb0>]\n",
        "# \tOutput: [[<__main__.Triple object at 0x7ef56ed13ac0>, <__main__.Triple object at 0x7ef56ed13af0>], [<__main__.Triple object at 0x7ef56ed13b20>, <__main__.Triple object at 0x7ef56ed13b50>, <__main__.Triple object at 0x7ef56ed13b80>, <__main__.Triple object at 0x7ef56ed13bb0>]]\n",
        "\n",
        "final_data = {}\n",
        "# Store subjects and objects to get their hypernyms later on and query dbpedia/wikidata once per entity instead of once per entity instance in an input\n",
        "all_entities_input = []\n",
        "removed_dtp = []\n",
        "for data_split in text_planning_data:\n",
        "  # Create a list for each split\n",
        "  final_data[data_split] = []\n",
        "  for text_data in text_planning_data[data_split]:\n",
        "    # The last element of the list is metadata; if there is nothing before that, remove datapoint\n",
        "    # Update: these datapoints were note removed in Thiago's experiments, so do I need to include them (for my test/dev data, otherwise I have different test/dev files from references?)\n",
        "    if len(text_data[:-1]) == 0:\n",
        "      # print(text_data)\n",
        "      removed_dtp.append(f'{data_split}_{text_data[-1][1]}_{text_data[-1][3]}_{text_data[-1][2]}')\n",
        "    else:\n",
        "      # Create a list that will contain the plan (i.e. sentence groupings) of each text\n",
        "      text_plan = []\n",
        "      input_triples = []\n",
        "      sent_id = 0\n",
        "      # the last element in text_data is the metadata, not a sentence\n",
        "      metadata = text_data[-1]\n",
        "      for sentence_data in text_data[:-1]:\n",
        "        # Create a list for each sentence grouping\n",
        "        text_plan.append([])\n",
        "        for triple_data in sentence_data:\n",
        "          triple_object = Triple(triple_data, metadata[:-1])\n",
        "          # Store the triples in each respective sentence\n",
        "          text_plan[sent_id].append(triple_object)\n",
        "          # Store the triples of all sentences in the \"input\" list\n",
        "          input_triples.append(triple_object)\n",
        "          # store all subjects and objects in a list to get their hypernyms/IDs in the next loop\n",
        "          if triple_object.subject_label not in all_entities_input:\n",
        "            all_entities_input.append(triple_object.subject_label)\n",
        "          if triple_object.object_label not in all_entities_input:\n",
        "            all_entities_input.append(triple_object.object_label)\n",
        "        sent_id += 1\n",
        "      # Append input/output pair in final data structure\n",
        "      orig_input_objects = []\n",
        "      for orig_input_triple in metadata[-1]:\n",
        "        orig_input_objects.append(Triple(orig_input_triple, metadata[:-1]))\n",
        "      final_data[data_split].append(TP_Datapoint(input_triples, text_plan, orig_input_objects))\n",
        "\n",
        "dtp_count_all = []\n",
        "for dt_split in final_data:\n",
        "  dtp_count_all.append(len(final_data[dt_split]))\n",
        "\n",
        "print(f\"Sample input/output pair:\\n\\tInput: {final_data['train'][0].input}\\n\\tOutput: {final_data['train'][0].output}\\n\\tOrig input: {final_data['train'][0].original_input}\")\n",
        "\n",
        "if exclude_input_size == 'size1&2':\n",
        "  print(f'Removed {len(removed_dtp)} empty data points.\\n  Expected: 14; {Counter(removed_dtp)}.')\n",
        "  print(f'{str(dtp_count_all)} input/output pairs were collected (3 triples and more, expected: [1311, 2860, 10363]).')\n",
        "  print(f'There are {len(all_entities_input)} different subject/object values in Enhanced WebNLG (expected: 2356).')\n",
        "\n",
        "elif exclude_input_size == 'size1':\n",
        "  print(f'Removed {len(removed_dtp)} empty data points.\\n  Expected: 18; {Counter(removed_dtp)}.')\n",
        "  print(f'{str(dtp_count_all)} input/output pairs were collected (2 triples and more, expected: [1725, 3834, 13726]).')\n",
        "  print(f'There are {len(all_entities_input)} different subject/object values in Enhanced WebNLG (expected: 2542).')\n",
        "\n",
        "elif exclude_input_size == 'none':\n",
        "  print(f'Removed {len(removed_dtp)} empty data points.\\n  Expected: 28; {Counter(removed_dtp)}.')\n",
        "  print(f'{str(dtp_count_all)} input/output pairs were collected (all triples sizes, expected: [2254, 4916, 18071]).')\n",
        "  print(f'There are {len(all_entities_input)} different subject/object values in Enhanced WebNLG (expected: 2730).')\n",
        "\n",
        "# print(len(final_data['dev']))\n",
        "\n"
      ],
      "metadata": {
        "id": "G-ktyn-alLPr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Debug: Print sample datapoints\n",
        "for itp in final_data['test'][0].input:\n",
        "  print(itp.property_label)\n",
        "print('\\n')\n",
        "for itp in final_data['test'][2].input:\n",
        "  print(itp.property_label)\n",
        "print('\\n')\n",
        "for itp in final_data['test'][5].input:\n",
        "  print(itp.property_label)\n",
        "print('\\n')\n",
        "for itp in final_data['test'][5].original_input:\n",
        "  print(itp.property_label)\n",
        "  print(itp.subject_label)\n",
        "  print(itp.object_label)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2--WZphL1Dg2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get class information from Wikidata and DBpedia for input pairs\n",
        "get_classes_entities = 'Via saved JSON'#@param['Via saved JSON', 'Via live query']\n",
        "\n",
        "import sys\n",
        "import codecs\n",
        "import json\n",
        "\n",
        "# Store here problematic inputs to test them quickly\n",
        "# all_entities_input = ['Albert_Einstein', 'Lippincott_Williams_&_Wilkins', 'Washington,_D.C.', 'Am._J._Math.', '18R/36L', '-3.3528', '30843.8 (square metres)', 'Atatürk_Monument_(İzmir)', '\"52.0\"(minutes)', '\"A894 VA; A904 VD;\"', '~500', '<http://www.ghampara.gov.lk/>', 'Edwin E. Aldrin, Jr.']\n",
        "#  Extract all entities in the dataset (now done during the building of final_data dico)\n",
        "# all_entities_input = []\n",
        "# for split in final_data:\n",
        "#   # print(split)\n",
        "#   for data_point in final_data[split]:\n",
        "#     for triple in data_point.input:\n",
        "#       subject_value = triple.subject_label\n",
        "#       if subject_value not in all_entities_input:\n",
        "#         all_entities_input.append(subject_value)\n",
        "#       object_value = triple.object_label\n",
        "#       if object_value not in all_entities_input:\n",
        "#         all_entities_input.append(object_value)\n",
        "\n",
        "all_entities_classes_by_entity = {}\n",
        "\n",
        "# Use following if loading the json, otherwise run previous cell (takes a while, about 25min) and update variable name to all_entities_classes_by_entity\n",
        "\n",
        "if get_classes_entities == 'Via saved JSON':\n",
        "  ! gdown 11MYilE-SC43dBR1ux_7Yw-iCq9lNk0lO\n",
        "  # ! gdown 1bT4TsE3MjTCsyvc18b_l6z9SmkhMzqyG\n",
        "  with open('webnlg_dbp-wkd-classes.json') as json_file:\n",
        "    all_entities_classes_by_entity = json.load(json_file)\n",
        "    # print(all_entities_classes_by_entity['American_Civil_War'])\n",
        "\n",
        "elif get_classes_entities == 'Via live query':\n",
        "  # First get the DBpedia hypernym/class and add it to the dictionary under the key created by the Wikidata loop\n",
        "  print(f'Querying DBpedia for hypernyms...')\n",
        "  with codecs.open('dbpedia_info.txt', 'w', 'utf-8') as fo2:\n",
        "    for count3, entity_label in enumerate(all_entities_input):\n",
        "      print(f'{count3+1}/{len(all_entities_input)}')\n",
        "      # Only process each entity once\n",
        "      if entity_label not in all_entities_classes_by_entity:\n",
        "        hypernym_value = get_dbpedia_hypernym(entity_label)\n",
        "        all_entities_classes_by_entity[entity_label] = {}\n",
        "        if hypernym_value == None:\n",
        "          # So all empty values for class have the same format\n",
        "          hypernym_value = ''\n",
        "        all_entities_classes_by_entity[entity_label]['class_dbp'] = hypernym_value\n",
        "        line = f'{entity_label}: '\n",
        "        if not hypernym_value == '':\n",
        "          line = line + f'class_dbp[{hypernym_value}]'\n",
        "        line = line + '\\n'\n",
        "        fo2.write(line)\n",
        "\n",
        "  print(f'\\nQuerying Wikidata for QIDs...')\n",
        "  # Now get Wikidata QID\n",
        "  # Wikidata IDs is a list of lists of quadruples, with the Wikidata ID, the clean entity name, the original entity name and a basic class assigned using regex\n",
        "  # ex: [['Q937', 'Albert Einstein', 'Albert_Einstein'], ['Q76', 'Barack Obama', 'Barack_Obama']]\n",
        "  wikidata_ids_and_labels = []\n",
        "  get_wikidata_id_bulk(wikidata_ids_and_labels, all_entities_input, bar)\n",
        "  # print(wikidata_ids_and_labels)\n",
        "  # print(f'\\nThere are {len(all_entities_input):,} different subject and object values in the dataset\\n')\n",
        "\n",
        "  entities_seen_wikidata = []\n",
        "  # Now get the hypernym/class of each entity according to Wikidata\n",
        "  print(f'\\nQuerying Wikidata for hypernyms...')\n",
        "  with codecs.open('wikidata_info.txt', 'w', 'utf-8') as fo:\n",
        "    for count2, wikidata_id_and_labels in enumerate(wikidata_ids_and_labels):\n",
        "      print(f'{count2+1}/{len(wikidata_ids_and_labels)}')\n",
        "      # if entity_label not in entities_seen_wikidata:\n",
        "      if wikidata_id_and_labels[2] not in entities_seen_wikidata:\n",
        "        hypernym_value = ''\n",
        "        entities_seen_wikidata.append(wikidata_id_and_labels[2])\n",
        "        # If there is no QID, make it an empty value for the class\n",
        "        if wikidata_id_and_labels[0] == '???':\n",
        "          hypernym_value = ''\n",
        "        else:\n",
        "          # Otherwise, go check Wikidata\n",
        "          hypernym_value = get_wikidata_hypernym(wikidata_id_and_labels[0])\n",
        "        # Fill in the dictionary with the wikidata and regex info\n",
        "        # wikidata_id_and_labels[2] is the same as entity_label, so there should be a match with the existing dico keys\n",
        "        all_entities_classes_by_entity[wikidata_id_and_labels[2]]['QID'] = wikidata_id_and_labels[0]\n",
        "        all_entities_classes_by_entity[wikidata_id_and_labels[2]]['class_wkd'] = hypernym_value\n",
        "        all_entities_classes_by_entity[wikidata_id_and_labels[2]]['class_regex'] = wikidata_id_and_labels[3]\n",
        "        line = f'{wikidata_id_and_labels[2]}: '\n",
        "        if not hypernym_value == '':\n",
        "          line = line + f'class_wkd[{hypernym_value}] '\n",
        "        if not wikidata_id_and_labels[3] == '':\n",
        "          line = line + f'class_regex[{wikidata_id_and_labels[3]}] '\n",
        "        line = line + '\\n'\n",
        "        fo.write(line)\n",
        "else:\n",
        "  print('Select parameter a the top of the cell!')\n",
        "\n",
        "print(f'--------\\nDone!')"
      ],
      "metadata": {
        "id": "72W4WwRULFV6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Debug: Compare two json files\n",
        "import json\n",
        "\n",
        "all_entities_classes_by_entity17 = {}\n",
        "all_entities_classes_by_entity37 = {}\n",
        "\n",
        "with open('webnlg_dbp-wkd-classes_1-7.json') as json_file:\n",
        "  all_entities_classes_by_entity17 = json.load(json_file)\n",
        "\n",
        "with open('webnlg_dbp-wkd-classes_3-7.json') as json_file:\n",
        "  all_entities_classes_by_entity37 = json.load(json_file)\n",
        "\n",
        "entities_in_common = 0\n",
        "wkd_in_common = 0\n",
        "dbp_in_common = 0\n",
        "regex_in_common = 0\n",
        "entities_only_in_17 = 0\n",
        "for entity_label in all_entities_classes_by_entity17:\n",
        "  if entity_label in all_entities_classes_by_entity37:\n",
        "    entities_in_common += 1\n",
        "    if all_entities_classes_by_entity17[entity_label]['class_wkd'] == all_entities_classes_by_entity37[entity_label]['class_wkd']:\n",
        "      wkd_in_common +=1\n",
        "    else:\n",
        "      print(f\"1-7: {all_entities_classes_by_entity17[entity_label]['class_wkd']}: 3-7: {all_entities_classes_by_entity37[entity_label]['class_wkd']}\")\n",
        "    if all_entities_classes_by_entity17[entity_label]['class_dbp'] == all_entities_classes_by_entity37[entity_label]['class_dbp']:\n",
        "      dbp_in_common +=1\n",
        "    if all_entities_classes_by_entity17[entity_label]['class_regex'] == all_entities_classes_by_entity37[entity_label]['class_regex']:\n",
        "      regex_in_common +=1\n",
        "  else:\n",
        "    entities_only_in_17 += 1\n",
        "\n",
        "print(f'{entities_in_common} entities are in both files.')\n",
        "print(f'{wkd_in_common} wkd classes are in both files.')\n",
        "print(f'{dbp_in_common} dbp classes are in both files.')\n",
        "print(f'{regex_in_common} regex classes are in both files.')\n",
        "print(f'{entities_only_in_17} entities are only in 1-7.')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rs2ocRcia8I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export class info in a json file\n",
        "import json\n",
        "\n",
        "with open('webnlg_dbp-wkd-classes.json', 'w') as fp:\n",
        "  json.dump(all_entities_classes_by_entity, fp)"
      ],
      "metadata": {
        "id": "RYVkUb4ttZEK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get class mapping information into a dico via pandas dataframe\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "sheet_url_props = 'https://docs.google.com/spreadsheets/d/14hnO_ci5LqGIUYoCLAupW6J82gCGlXXTtbOMTauCGrA/edit#gid=947115834'\n",
        "sheet_url_entities = 'https://docs.google.com/spreadsheets/d/14hnO_ci5LqGIUYoCLAupW6J82gCGlXXTtbOMTauCGrA/edit#gid=2130476960'\n",
        "# Change the url to specify export format\n",
        "csv_export_url_props = sheet_url_props.replace('/edit#gid=', '/export?format=csv&gid=')\n",
        "csv_export_url_entities = sheet_url_entities.replace('/edit#gid=', '/export?format=csv&gid=')\n",
        "\n",
        "mappings_properties = pd.read_csv(csv_export_url_props,\n",
        "                               # Set first row as rownames in data frame\n",
        "                               header=0,\n",
        "                               usecols=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "                               )\n",
        "mappings_entities = pd.read_csv(csv_export_url_entities,\n",
        "                               # Set first row as rownames in data frame\n",
        "                               header=0,\n",
        "                               usecols=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
        "                               )\n",
        "\n",
        "def fill_dico_fine2coarse(pd_dataframe, dico_mapping_fine2coarse, propORentity):\n",
        "  for column_prop in pd_dataframe:\n",
        "    # Check if a property in is a column and get property name and column header (=class of property)\n",
        "    # if (mappings_properties[column_prop] == property_label_f).any():\n",
        "    for propentity_label_s in pd_dataframe[column_prop]:\n",
        "      if isinstance(propentity_label_s, str):\n",
        "        if propentity_label_s in dico_mapping_fine2coarse[propORentity]:\n",
        "          print(f'ERROR, key already exists: {propentity_label_s}')\n",
        "        else:\n",
        "          dico_mapping_fine2coarse[propORentity][propentity_label_s] = column_prop\n",
        "        # Also add in the dico mapping a version in which the labels of entity categories are joined with underscores (I use underscores when building the merged entity class)\n",
        "        # Actually do it for both entities and properties, because it looks like Thiago's data used properties with underscores and we need the mapping from these too\n",
        "        # if propORentity == 'entities':\n",
        "        propentity_label_s_u = propentity_label_s.replace(\" \", \"_\")\n",
        "        if propentity_label_s_u not in dico_mapping_fine2coarse[propORentity]:\n",
        "          dico_mapping_fine2coarse[propORentity][propentity_label_s_u] = column_prop\n",
        "\n",
        "# When I check in the dataframe directly for the class of each entity in the next cell, it seems to be very slow, so I make a dictionary to store the info\n",
        "dico_mapping_fine2coarse = {'properties': {}, 'entities': {}}\n",
        "fill_dico_fine2coarse(mappings_properties, dico_mapping_fine2coarse, 'properties')\n",
        "fill_dico_fine2coarse(mappings_entities, dico_mapping_fine2coarse, 'entities')\n",
        "\n",
        "print(f\"Class of property comparable: {dico_mapping_fine2coarse['properties']['comparable']}\")\n",
        "print(f\"Class of property was a crew member of: {dico_mapping_fine2coarse['properties']['was a crew member of']}\")\n",
        "print(f\"Class of property was_a_crew_member_of: {dico_mapping_fine2coarse['properties']['was_a_crew_member_of']}\")\n",
        "print(f\"Class of property air base: {dico_mapping_fine2coarse['entities']['air base']}\")\n",
        "print(f\"Class of property air_base: {dico_mapping_fine2coarse['entities']['air_base']}\")\n",
        "print(f\"Class of entityt class version, edition or translation: {dico_mapping_fine2coarse['entities']['version, edition or translation']}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eRwUcLXjutLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add class info to triple objects for input pairs\n",
        "import re\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# I also add a \"fused\" class that's supposed to take what looks most reliable from the different sources\n",
        "# I use: regex > wkd classes that have n >= 2 dbp classes mapping to them > dbp classes\n",
        "\n",
        "def choose_superClass(propOrEntity_label, dico_mapping_fine2coarse, propORentity):\n",
        "  if propOrEntity_label in dico_mapping_fine2coarse[propORentity]:\n",
        "    return dico_mapping_fine2coarse[propORentity][propOrEntity_label]\n",
        "\n",
        "def choose_merged_class(class_wkd, class_dbp, class_regex):\n",
        "  # list that contains the wikidata classes that generalise a bit over the DBpedia classes\n",
        "  # The idea is that DBpedia classes looks better, but are sometimes too fine-grained, so I use more general Wikidata classes instead when available\n",
        "  wkd_2dbp_classes = ['academic major', 'air base', 'air force', 'aircraft family', 'airport', 'archaeological site', 'architectural firm', 'architectural style', 'army', 'art museum', 'association football club', 'association football league', 'asteroid', 'automobile manufacturer', 'automobile model series', 'automobile model', 'autonomous community of Spain', 'award', 'basketball team', 'battle', 'big city', 'book publisher', 'business', 'capital city', 'car classification', 'city council', 'city in the United States', 'city', 'clade', 'coachwork type', 'color', 'comics character', 'commune of France', 'commune of Italy', 'country', 'county seat', 'division', 'ethnic group in Indonesia', 'ethnic group', 'film', 'fixed-base operator', 'food ingredient', 'food', 'geographic region', 'government agency', 'high-rise building', 'historical country', 'historical region', 'hotel', 'human biblical figure', 'human population', 'human settlement', 'human', 'international airport', 'island', 'language', 'launch site', 'literary work', 'material', 'medallion', 'metropolitan area', 'municipality of Belgium', 'municipality of Spain', 'music genre', 'musical group', 'nation', 'national anthem', 'national association football team', 'Olympic stadium', 'organization', 'pandemic', 'panethnicity', 'parliament', 'peninsula', 'political party', 'position', 'private university', 'profession', 'professional sports league', 'public office', 'record label', 'river', 'rocket model', 'sculpture', 'shipyard', 'skyscraper', 'sovereign state', 'spaceport', 'sports season', 'street', 'subsidiary', 'taxon', 'town', 'township of Illinois', 'township of New Jersey', 'trademark', 'type of food or dish', 'type of musical instrument', 'university', 'urban municipality in Germany', 'village', 'war', 'written work']\n",
        "  class_merged = ''\n",
        "  # By default, use the regex classes, who are targeting some specific values such as dates, quantities, runway names, etc.\n",
        "  if not (class_regex == '' or class_regex == None):\n",
        "    class_merged = class_regex.lower()\n",
        "  # If no regex was assigned, we use the \"general\" wkd classes\n",
        "  elif class_wkd in wkd_2dbp_classes:\n",
        "    class_merged = class_wkd.replace(\" \", \"_\").lower()\n",
        "  # If none of the above, use the dbp class if any\n",
        "  elif not (class_dbp == '' or class_dbp == None):\n",
        "    class_merged = class_dbp.replace(\" \", \"_\").lower()\n",
        "  # If none of the above, use the wkd class\n",
        "  elif not (class_wkd == '' or class_wkd == None):\n",
        "    class_merged = class_wkd.replace(\" \", \"_\").lower()\n",
        "  else:\n",
        "    class_merged = '_'\n",
        "  return class_merged\n",
        "\n",
        "def add_class_info_triple_objects (triple, all_entities_classes_by_entity, list_entities_data_point):\n",
        "  # Get class of property\n",
        "  triple.property_class = choose_superClass(triple.property_label, dico_mapping_fine2coarse, 'properties')\n",
        "  # If the subject is found in the dico that contains classes, retrieve its classes\n",
        "  if triple.subject_label in all_entities_classes_by_entity:\n",
        "    triple.subject_class_wkd = all_entities_classes_by_entity[triple.subject_label]['class_wkd']\n",
        "    triple.subject_class_dbp = all_entities_classes_by_entity[triple.subject_label]['class_dbp']\n",
        "    triple.subject_class_regex = all_entities_classes_by_entity[triple.subject_label]['class_regex']\n",
        "    triple.subject_id_wkd = all_entities_classes_by_entity[triple.subject_label]['QID']\n",
        "    # For the ID of an entity, get its position in the lit of unique entities in the input\n",
        "    triple.subject_id_input = list_entities_data_point.index(triple.subject_label)\n",
        "    # Call function that chooses a class among the available ones\n",
        "    triple.subject_class_merged = choose_merged_class(all_entities_classes_by_entity[triple.subject_label]['class_wkd'], all_entities_classes_by_entity[triple.subject_label]['class_dbp'], all_entities_classes_by_entity[triple.subject_label]['class_regex'])\n",
        "    # Call function that assigns abstract label\n",
        "    triple.subject_class_abstract = choose_superClass(triple.subject_class_merged, dico_mapping_fine2coarse, 'entities')\n",
        "  # If the object is found in the dico that contains classes, retrieve its classes\n",
        "  if triple.object_label in all_entities_classes_by_entity:\n",
        "    triple.object_class_wkd = all_entities_classes_by_entity[triple.object_label]['class_wkd']\n",
        "    triple.object_class_dbp = all_entities_classes_by_entity[triple.object_label]['class_dbp']\n",
        "    triple.object_class_regex = all_entities_classes_by_entity[triple.object_label]['class_regex']\n",
        "    triple.object_id_wkd = all_entities_classes_by_entity[triple.object_label]['QID']\n",
        "    # For the ID of an entity, get its position in the list of unique entities in the input\n",
        "    triple.object_id_input = list_entities_data_point.index(triple.object_label)\n",
        "    # Call function that chooses a class among the available ones\n",
        "    triple.object_class_merged = choose_merged_class(all_entities_classes_by_entity[triple.object_label]['class_wkd'], all_entities_classes_by_entity[triple.object_label]['class_dbp'], all_entities_classes_by_entity[triple.object_label]['class_regex'])\n",
        "    # Call function that assigns abstract label\n",
        "    triple.object_class_abstract = choose_superClass(triple.object_class_merged, dico_mapping_fine2coarse, 'entities')\n",
        "\n",
        "# data_split = train/dev/test\n",
        "for data_split in final_data:\n",
        "  # data_point is an object with two features, input and output\n",
        "  for data_point in final_data[data_split]:\n",
        "    # Create a list to keep track of entities and use the list index as coreferring ID\n",
        "    # Update: We need to shuffle this list so as not to encode the original irder in the IDs\n",
        "    list_entities_data_point = []\n",
        "    # data_point.input contains a list of triples\n",
        "    for triple_input in data_point.input:\n",
        "      # Add subjects to the list of entities\n",
        "      if triple_input.subject_label not in list_entities_data_point:\n",
        "        list_entities_data_point.append(triple_input.subject_label)\n",
        "      # Add objects to the list of entities\n",
        "      if triple_input.object_label not in list_entities_data_point:\n",
        "        list_entities_data_point.append(triple_input.object_label)\n",
        "      # Shuffle the list of entities to detach the ID from the original position in the reference order\n",
        "      random.shuffle(list_entities_data_point)\n",
        "      # Update the triple object with the class information\n",
        "      add_class_info_triple_objects(triple_input, all_entities_classes_by_entity, list_entities_data_point)\n",
        "    # data_point.output contains a list of list of triples; triples are grouped into sentences\n",
        "    for sentence_group in data_point.output:\n",
        "      for triple_output in sentence_group:\n",
        "        add_class_info_triple_objects(triple_output, all_entities_classes_by_entity, list_entities_data_point)\n",
        "    # Also add the class info to the original input triples that we use to repair incomplete data points later on\n",
        "    # I had forgotten that part in the original experiments; not sure it will be used but it's cleaner to have it\n",
        "    for triple_input_orig in data_point.original_input:\n",
        "      # Add subjects to the list of entities\n",
        "      if triple_input_orig.subject_label not in list_entities_data_point:\n",
        "        list_entities_data_point.append(triple_input_orig.subject_label)\n",
        "      # Add objects to the list of entities\n",
        "      if triple_input_orig.object_label not in list_entities_data_point:\n",
        "        list_entities_data_point.append(triple_input_orig.object_label)\n",
        "      # Update the triple object with the class information\n",
        "      # No need to shuffle more the list of entities, since the original input is already independent from the reference order\n",
        "      add_class_info_triple_objects(triple_input_orig, all_entities_classes_by_entity, list_entities_data_point)\n",
        "\n",
        "# Make another version of all the keys where the entity labels have underscores (for matching Thiago's labels)\n",
        "# Build a new dico with entities with underscores as keys\n",
        "entities_with_underscores_classes_by_entity = {re.subn('\\s+', '_', key_orig)[0]:all_entities_classes_by_entity[key_orig] for key_orig in all_entities_classes_by_entity}\n",
        "# Merge old and new dicos\n",
        "# https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression-in-python\n",
        "# The desired result is to get a new dictionary (z) with the values merged, and the second dictionary's values overwriting those from the first.\n",
        "new_all_entities_classes_by_entity = entities_with_underscores_classes_by_entity | all_entities_classes_by_entity\n",
        "\n",
        "# print(len(new_all_entities_classes_by_entity))"
      ],
      "metadata": {
        "id": "GB1y-UdiQg-J",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get number of different merged classes check for missing\n",
        "seen_entities_print_label = []\n",
        "seen_entities_print_triple = []\n",
        "unique_merged_classes = []\n",
        "unique_properties = []\n",
        "unique_properties_train = []\n",
        "unique_properties_dev = []\n",
        "unique_properties_test = []\n",
        "unique_property_classes = []\n",
        "unique_entity_classes_abstract = []\n",
        "unique_property_lemmas = []\n",
        "missing_property_class = []\n",
        "missing_entity_class = []\n",
        "for data_split in final_data:\n",
        "  for data_point in final_data[data_split]:\n",
        "    for triple in data_point.input:\n",
        "      # If the subject hasn't been seen already, save triple in list, mark object as seen too\n",
        "      if triple.subject_label not in seen_entities_print_label:\n",
        "        seen_entities_print_triple.append(triple)\n",
        "        seen_entities_print_label.append(triple.subject_label)\n",
        "        seen_entities_print_label.append(triple.object_label)\n",
        "      #If the subject has been seen already, check if we have seen the object too; if not, add triple to the list\n",
        "      elif triple.object_label not in seen_entities_print_label:\n",
        "        seen_entities_print_triple.append(triple)\n",
        "        seen_entities_print_label.append(triple.object_label)\n",
        "      if triple.subject_class_merged not in unique_merged_classes:\n",
        "        unique_merged_classes.append(triple.subject_class_merged)\n",
        "      if triple.object_class_merged not in unique_merged_classes:\n",
        "        unique_merged_classes.append(triple.object_class_merged)\n",
        "      # Check abstract classes\n",
        "      if triple.subject_class_abstract not in unique_entity_classes_abstract:\n",
        "        unique_entity_classes_abstract.append(triple.subject_class_abstract)\n",
        "      if triple.object_class_abstract not in unique_entity_classes_abstract:\n",
        "        unique_entity_classes_abstract.append(triple.object_class_abstract)\n",
        "      if triple.property_label not in unique_properties:\n",
        "        unique_properties.append(triple.property_label)\n",
        "      if data_split == 'train':\n",
        "        if triple.property_label not in unique_properties_train:\n",
        "          unique_properties_train.append(triple.property_label)\n",
        "      elif data_split == 'dev':\n",
        "        if triple.property_label not in unique_properties_dev:\n",
        "          unique_properties_dev.append(triple.property_label)\n",
        "      elif data_split == 'test':\n",
        "        if triple.property_label not in unique_properties_test:\n",
        "          unique_properties_test.append(triple.property_label)\n",
        "      if triple.property_class not in unique_property_classes:\n",
        "        unique_property_classes.append(triple.property_class)\n",
        "      if triple.property_lemma not in unique_property_lemmas:\n",
        "        unique_property_lemmas.append(triple.property_lemma)\n",
        "      # Check if we have all property and entity mappings to coars-grained classes\n",
        "      if not triple.property_label == None and triple.property_label not in dico_mapping_fine2coarse['properties']:\n",
        "        if triple.property_label not in missing_property_class:\n",
        "          missing_property_class.append(triple.property_label)\n",
        "      if not triple.subject_class_merged == None and triple.subject_class_merged not in dico_mapping_fine2coarse['entities']:\n",
        "        if triple.subject_class_merged not in missing_entity_class:\n",
        "          missing_entity_class.append(triple.subject_class_merged)\n",
        "      if not triple.object_class_merged == None and triple.object_class_merged not in dico_mapping_fine2coarse['entities']:\n",
        "        if triple.object_class_merged not in missing_entity_class:\n",
        "          missing_entity_class.append(triple.object_class_merged)\n",
        "\n",
        "# print(len(seen_entities_print_triple))\n",
        "print(f'{len(unique_properties)} unique properties')\n",
        "print(sorted([str(unique_property) for unique_property in unique_properties]))\n",
        "print(f'  ->{len(unique_properties_train)} unique properties in the train data')\n",
        "print(sorted([str(unique_property_t) for unique_property_t in unique_properties_train]))\n",
        "print(f'  ->{len(unique_properties_dev)} unique properties in the dev data')\n",
        "print(sorted([str(unique_property_d) for unique_property_d in unique_properties_dev]))\n",
        "print(f'  ->{len(unique_properties_test)} unique properties in the test data')\n",
        "print(sorted([str(unique_property_te) for unique_property_te in unique_properties_test]))\n",
        "print(f'{len(unique_property_lemmas)} property lemmas')\n",
        "print(sorted([str(unique_property_lemma) for unique_property_lemma in unique_property_lemmas]))\n",
        "print(f'{len(unique_property_classes)} property classes')\n",
        "print(sorted([str(unique_property_class) for unique_property_class in unique_property_classes]))\n",
        "print(f'{len(unique_merged_classes)} merged entity classes')\n",
        "print(sorted([str(unique_class) for unique_class in unique_merged_classes]))\n",
        "print(f'{len(unique_entity_classes_abstract)} abstract entity classes')\n",
        "print(sorted([str(unique_class_abstract) for unique_class_abstract in unique_entity_classes_abstract]))\n",
        "print('--------------')\n",
        "print(f'Missing property classes: {missing_property_class}')\n",
        "print(f'Missing entity classes: {missing_entity_class}')\n",
        "\n",
        "# print(f'There are {len(seen_entities_print_triple)} entities.')\n",
        "# for triple in seen_entities_print_triple[:10]:\n",
        "#   print('------------------')\n",
        "#   print(triple.object_label)\n",
        "#   print('------------------')\n",
        "#   print(f'wkd: {triple.object_class_wkd}')\n",
        "#   print(f'dbp: {triple.object_class_dbp}')\n",
        "#   print(f'regex: {triple.object_class_regex}')\n",
        "#   print(f'merged: {triple.object_class_merged}')\n",
        "#   print(f'qid: {triple.object_id_wkd}')\n",
        "#   print(f'coref_id: {triple.object_id_input}')\n"
      ],
      "metadata": {
        "id": "Gep6CJXajcTp",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get class mappings (and stats)\n",
        "\n",
        "def make_dic_map_wkd_dbp (dico_mapping, key_sbj, key_obj, val_sbj, val_obj):\n",
        "  if key_sbj not in dico_mapping:\n",
        "    dico_mapping[key_sbj] = []\n",
        "    dico_mapping[key_sbj].append(val_sbj)\n",
        "  else:\n",
        "    if val_sbj not in dico_mapping[key_sbj]:\n",
        "      dico_mapping[key_sbj].append(val_sbj)\n",
        "  if key_obj not in dico_mapping:\n",
        "    dico_mapping[key_obj] = []\n",
        "    dico_mapping[key_obj].append(val_obj)\n",
        "  else:\n",
        "    if val_obj not in dico_mapping[key_obj]:\n",
        "      dico_mapping[key_obj].append(val_obj)\n",
        "  return dico_mapping\n",
        "\n",
        "dico_mapping_wkd2dbp = {}\n",
        "dico_mapping_dbp2wkd = {}\n",
        "for data_split in final_data:\n",
        "  for data_point in final_data[data_split]:\n",
        "    for triple in data_point.input:\n",
        "      sbj = triple.subject_label\n",
        "      obj = triple.subject_label\n",
        "      wkd_sbj = triple.subject_class_wkd\n",
        "      dbp_sbj = triple.subject_class_dbp\n",
        "      wkd_obj = triple.object_class_wkd\n",
        "      dbp_obj = triple.object_class_dbp\n",
        "      regex = triple.object_class_regex\n",
        "      dico_mapping_wkd2dbp = make_dic_map_wkd_dbp(dico_mapping_wkd2dbp, wkd_sbj, wkd_obj, dbp_sbj, dbp_obj)\n",
        "      dico_mapping_dbp2wkd = make_dic_map_wkd_dbp(dico_mapping_dbp2wkd, dbp_sbj, dbp_obj, wkd_sbj, wkd_obj)\n",
        "\n",
        "print(dico_mapping_wkd2dbp.keys())\n",
        "print(dico_mapping_dbp2wkd.keys())\n",
        "# print(dico_wkd2dbp['human'])"
      ],
      "metadata": {
        "id": "XgQT2JHHTIwn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Debug: Print sample datapoints\n",
        "for itp in final_data['test'][0].input:\n",
        "  print(itp.property_label)\n",
        "print('\\n')\n",
        "for itp in final_data['test'][2].input:\n",
        "  print(itp.property_label)\n",
        "print('\\n')\n",
        "for itp in final_data['test'][5].input:\n",
        "  print(itp.property_label)\n",
        "  print(f'{itp.subject_label} - {itp.subject_class_abstract} - {itp.subject_id_input}')\n",
        "  print(f'{itp.object_label} - {itp.object_class_abstract} - {itp.object_id_input}')\n",
        "print('\\n')\n",
        "for itp in final_data['test'][5].original_input:\n",
        "  print(itp.property_label)\n",
        "  print(f'{itp.subject_label} - {itp.subject_class_abstract} - {itp.subject_id_input}')\n",
        "  print(f'{itp.object_label} - {itp.object_class_abstract} - {itp.object_id_input}')"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "euBXU-XD287C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export mapping info in a json file (not needed to continue with code)\n",
        "import json\n",
        "\n",
        "with open('webnlg_dbp-wkd-map.json', 'w') as fp1:\n",
        "  json.dump(dict(sorted(dico_mapping_dbp2wkd.items())), fp1)\n",
        "  # json.dump(dico_mapping_dbp2wkd, fp1)\n",
        "# with open('webnlg_wkd-dbp-map.json', 'w') as fp2:\n",
        "  # json.dump(dico_mapping_wkd2dbp, fp2)\n",
        "  # json.dump(dict(sorted(dico_mapping_wkd2dbp.items())), fp2)"
      ],
      "metadata": {
        "id": "KB5NhURTR5J9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract needed info from our final dataset to create training data (parsing style)"
      ],
      "metadata": {
        "id": "zGFmnSGmXwIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create CoNLL line contents\n",
        "from collections import Counter\n",
        "\n",
        "# Keep all input/out pairs (there can be as many as there are reference texts)\n",
        "# or only one per input triple set (i.e. only 1 per reference text)\n",
        "# or only unique ones (if e.g. a 4-triple input has only 3 properties in the data and these 3 properties have already been seen, do no use this input)\n",
        "inOutPairs_kept = '1perTripleSet'#@param['all', '1perTripleSet', 'onlyTrulyUnique']\n",
        "\n",
        "# What info do we use in the input?\n",
        "# Label, for \"form\" column\n",
        "property_label_in_conll = 'original' #@param ['none', 'original', 'split']\n",
        "# Lemma, for \"lemma\" column\n",
        "property_lemma_in_conll = False#@param {type:\"boolean\"}\n",
        "# Class of property, for PoS column\n",
        "property_class_in_conll = False#@param {type:\"boolean\"}\n",
        "# Subject/Domain and Object/Range classes, for feats column\n",
        "subject_in_conll = 'none' #@param ['none', 'original', 'wkd', 'dbp', 'merged', 'abstract']\n",
        "object_in_conll = 'none' #@param ['none', 'original', 'wkd', 'dbp', 'merged', 'abstract']\n",
        "# Coreference information for feats column\n",
        "coreference_in_conll = 'sbj&obj' #@param ['none', 'sbj', 'obj', 'sbj&obj']\n",
        "use_only_conllNode_class = False#@param {type:\"boolean\"}\n",
        "# Some outputs have less properties than in the original input; 'True' here adds the missing properties in the structure\n",
        "add_missing_properties = False#@param{type:\"boolean\"}\n",
        "print_groupings = False#@param {type:\"boolean\"}\n",
        "\n",
        "def checkMismatchProperties(data_point, unique_input_ID):\n",
        "  # Check how many cases of mismatch between original input and actual input there is in the data\n",
        "  list_missing_props = []\n",
        "  if not len(data_point.input) == len(data_point.original_input):\n",
        "    props_orig = []\n",
        "    props_real = []\n",
        "    for orig_triple_object in data_point.original_input:\n",
        "       props_orig.append(f'{orig_triple_object.property_label}')\n",
        "    for real_triple_object in data_point.input:\n",
        "       props_real.append(f'{real_triple_object.property_label}')\n",
        "    # Check which properties is/are missing in the real input\n",
        "    # list_missing_props = [p for p in props_orig if p not in props_real] # Doesn't take into account duplicates\n",
        "    # Difference of list including duplicates\n",
        "    list_missing_props = list((Counter(props_orig) - Counter(props_real)).elements())\n",
        "    # Full debug message\n",
        "    # print(f'Mismatch {unique_input_ID}: Orig={len(data_point.original_input)}, Real={len(data_point.input)}, diff={str(list_missing_props)}')\n",
        "    # print(f'Mismatch props {unique_input_ID}, real input missing {str(list_missing_props)}')\n",
        "  return list_missing_props\n",
        "\n",
        "class CoNLL_Node:\n",
        "  def __init__(self, idx, form, lemma, pos, cpos, feats_list, head, deprel):\n",
        "    self.idx = idx\n",
        "    self.form = form.replace(' ', '_')\n",
        "    self.lemma = lemma\n",
        "    self.pos = pos\n",
        "    self.cpos = cpos\n",
        "    self.feats = ''\n",
        "    if len(feats_list) == 0:\n",
        "      self.feats = '_'\n",
        "    elif len(feats_list) == 1:\n",
        "      self.feats = feats_list[0]\n",
        "    else:\n",
        "      self.feats = '|'.join([str(feat_conll) for feat_conll in feats_list])\n",
        "    self.head = head\n",
        "    self.deprel = deprel\n",
        "\n",
        "def extract_formLemmaPosFeats(unique_properties_train, triple_object, property_label_in_conll, property_lemma_in_conll, property_class_in_conll, subject_in_conll, object_in_conll, coreference_in_conll):\n",
        " # Get form, lemma, pos and feats\n",
        "  node_conll_feats = []\n",
        "  # Form\n",
        "  if property_label_in_conll == 'original':\n",
        "    node_conll_form = str(triple_object.property_label)\n",
        "  elif property_label_in_conll == 'split':\n",
        "    node_conll_form = str(triple_object.property_label_split)\n",
        "  elif property_label_in_conll == 'none':\n",
        "    node_conll_form = '_'\n",
        "  # Lemma\n",
        "  if property_lemma_in_conll == True:\n",
        "    node_conll_lemma = str(triple_object.property_lemma)\n",
        "  elif property_lemma_in_conll == False:\n",
        "    node_conll_lemma = '_'\n",
        "  # PoS\n",
        "  if property_class_in_conll == True:\n",
        "    # Only copy the class of properties seen during training\n",
        "    if triple_object.property_label in unique_properties_train:\n",
        "      node_conll_pos = str(triple_object.property_class)\n",
        "    # In Thiago's files, he replaces spaces by underscores\n",
        "    elif triple_object.property_label.replace('_', ' ') in unique_properties_train:\n",
        "      node_conll_pos = str(triple_object.property_class)\n",
        "    # Need to find a better strategy to assign classes to unknown properties\n",
        "    else:\n",
        "      node_conll_pos = '_'\n",
        "  elif property_class_in_conll == False:\n",
        "    node_conll_pos = '_'\n",
        "  # Feat class subj/domain\n",
        "  if subject_in_conll == 'merged':\n",
        "    if not (triple_object.subject_class_merged == None or triple_object.subject_class_merged == ''):\n",
        "      feat_subj_class = 'dom_class='+str(triple_object.subject_class_merged)\n",
        "      node_conll_feats.append(feat_subj_class)\n",
        "  elif subject_in_conll == 'abstract':\n",
        "    if not (triple_object.subject_class_abstract == None or triple_object.subject_class_abstract == ''):\n",
        "      feat_subj_class = 'dom_class='+str(triple_object.subject_class_abstract)\n",
        "      node_conll_feats.append(feat_subj_class)\n",
        "  elif subject_in_conll == 'dbp':\n",
        "    if not (triple_object.subject_class_dbp == None or triple_object.subject_class_dbp == ''):\n",
        "      feat_subj_class = 'dom_class='+str(triple_object.subject_class_dbp)\n",
        "      node_conll_feats.append(feat_subj_class)\n",
        "  elif subject_in_conll == 'wkd':\n",
        "    if not (triple_object.subject_class_wkd == None or triple_object.subject_class_wkd == ''):\n",
        "      feat_subj_class = 'dom_class='+str(triple_object.subject_class_wkd)\n",
        "      node_conll_feats.append(feat_subj_class)\n",
        "  elif subject_in_conll == 'original':\n",
        "    if not (triple_object.subject_label == None or triple_object.subject_label == ''):\n",
        "      feat_subj_class = 'dom_class='+str(triple_object.subject_label)\n",
        "      node_conll_feats.append(feat_subj_class)\n",
        "  # Feat class obj/range\n",
        "  if object_in_conll == 'merged':\n",
        "    if not (triple_object.object_class_merged == None or triple_object.object_class_merged == ''):\n",
        "      feat_obj_class = 'ran_class='+str(triple_object.object_class_merged)\n",
        "      node_conll_feats.append(feat_obj_class)\n",
        "  elif object_in_conll == 'abstract':\n",
        "    if not (triple_object.object_class_abstract == None or triple_object.object_class_abstract == ''):\n",
        "      feat_obj_class = 'ran_class='+str(triple_object.object_class_abstract)\n",
        "      node_conll_feats.append(feat_obj_class)\n",
        "  elif object_in_conll == 'dbp':\n",
        "    if not (triple_object.object_class_dbp == None or triple_object.object_class_dbp == ''):\n",
        "      feat_obj_class = 'ran_class='+str(triple_object.object_class_dbp)\n",
        "      node_conll_feats.append(feat_obj_class)\n",
        "  elif object_in_conll == 'wkd':\n",
        "    if not (triple_object.object_class_wkd == None or triple_object.object_class_wkd == ''):\n",
        "      feat_obj_class = 'ran_class='+str(triple_object.object_class_wkd)\n",
        "      node_conll_feats.append(feat_obj_class)\n",
        "  elif object_in_conll == 'original':\n",
        "    if not (triple_object.object_label == None or triple_object.object_label == ''):\n",
        "      feat_obj_class = 'ran_class='+str(triple_object.object_label)\n",
        "      node_conll_feats.append(feat_obj_class)\n",
        "  # Feat coreference\n",
        "  if coreference_in_conll == 'sbj' or  coreference_in_conll == 'sbj&obj':\n",
        "    if not (triple_object.subject_id_input == None or triple_object.subject_id_input == ''):\n",
        "      feat_subj_coref = 'dom_ID='+str(triple_object.subject_id_input)\n",
        "      node_conll_feats.append(feat_subj_coref)\n",
        "  if coreference_in_conll == 'obj' or  coreference_in_conll == 'sbj&obj':\n",
        "    if not (triple_object.object_id_input == None or triple_object.object_id_input == ''):\n",
        "      feat_obj_coref = 'ran_ID='+str(triple_object.object_id_input)\n",
        "      node_conll_feats.append(feat_obj_coref)\n",
        "  # print(node_conll_feats)\n",
        "  return node_conll_form, node_conll_lemma, node_conll_pos, node_conll_feats\n",
        "\n",
        "def build_CoNLL_Lines(whole_data, data_split, unique_properties_train, inOutPairs_kept, print_groupings):\n",
        "  conll_contents = []\n",
        "  # Store unique datapoints to avoid duplicates if required\n",
        "  all_triple_combinations_in_dataset = []\n",
        "  # Input ID in this context is split_category_size_eid\n",
        "  all_input_IDs = []\n",
        "  count_duplicate_inputs = 0\n",
        "  for lim, data_point in enumerate(whole_data[data_split]):\n",
        "    # if lim< 50:\n",
        "    # print(list_orig_props)\n",
        "    triple_combination = []\n",
        "    unique_input_ID = f'{data_point.input[0].data_split}_{data_point.input[0].category}_{data_point.input[0].size}_{data_point.input[0].eid}'\n",
        "    for input_triple in data_point.input:\n",
        "      triple_combination.append(input_triple.property_label)\n",
        "    if inOutPairs_kept=='all' or (inOutPairs_kept=='onlyTrulyUnique' and sorted(triple_combination) not in all_triple_combinations_in_dataset) or (inOutPairs_kept=='1perTripleSet' and unique_input_ID not in all_input_IDs):\n",
        "      if inOutPairs_kept=='onlyTrulyUnique' and sorted(triple_combination) in all_triple_combinations_in_dataset:\n",
        "        count_duplicate_inputs += 1\n",
        "      elif inOutPairs_kept=='1perTripleSet' and unique_input_ID in all_input_IDs:\n",
        "        count_duplicate_inputs += 1\n",
        "      all_triple_combinations_in_dataset.append(sorted(triple_combination))\n",
        "      all_input_IDs.append(unique_input_ID)\n",
        "      if print_groupings == True:\n",
        "        print('------------------------')\n",
        "        print(f'{data_split}-{str(len(conll_contents))}: {data_point.input[0].category}-{data_point.input[0].size}-id{data_point.input[0].eid}')\n",
        "        print('------------------------')\n",
        "      missing_properties = checkMismatchProperties(data_point, unique_input_ID)\n",
        "      conll_content = []\n",
        "      # Starting at 1 because the first line in a conll has ID = 1\n",
        "      i_line_struct = 1\n",
        "      # We'll store here the latest element within the same sentence\n",
        "      intra_head = None\n",
        "      # We'll store here the head element from the previous sentence\n",
        "      inter_head = None\n",
        "      for i_sent, output_sentence_grouping in enumerate(data_point.output):\n",
        "        if print_groupings == True:\n",
        "          print('<BOUNDARY>')\n",
        "\n",
        "        # Re-initialise the internal head for each new sentence\n",
        "        intra_head = None\n",
        "        for i_triple, output_triple_object in enumerate(output_sentence_grouping):\n",
        "\n",
        "          if print_groupings == True:\n",
        "            print(f'  {output_triple_object.property_label}')\n",
        "\n",
        "          node_conll_idx = ''\n",
        "          node_conll_form = ''\n",
        "          node_conll_lemma = ''\n",
        "          node_conll_pos = ''\n",
        "          node_conll_feats = []\n",
        "          node_conll_head = ''\n",
        "          node_conll_deprel = ''\n",
        "          # In this block, get dependency information\n",
        "          # First line of the conll (first triple of the first sentence)\n",
        "          if i_sent == 0 and i_triple == 0:\n",
        "            # print(f'    {intra_head}  {inter_head} {i_line_struct}')\n",
        "            # conll_content.append(CoNLL_Node(i_line_struct, output_triple_object.property_label, '_', '_', '_', '_', '0', 'ROOT'))\n",
        "            node_conll_idx = i_line_struct\n",
        "            node_conll_head = '0'\n",
        "            node_conll_deprel = 'ROOT'\n",
        "            # Update the heads for the next elements of the same sentence and of the next one\n",
        "            intra_head = i_line_struct\n",
        "            inter_head = i_line_struct\n",
        "            i_line_struct += 1\n",
        "          # For the non-initial sentences\n",
        "          elif i_triple >= 0:\n",
        "            # print(f'    {intra_head}  {inter_head} {i_line_struct}')\n",
        "            # If we are starting a new sentence, the first node will be attached to the head of the last sentence with the relation \"inter\"\n",
        "            if intra_head == None:\n",
        "              # conll_content.append(CoNLL_Node(i_line_struct, output_triple_object.property_label, '_', '_', '_', '_', str(inter_head), 'inter'))\n",
        "              node_conll_idx = i_line_struct\n",
        "              node_conll_head = str(inter_head)\n",
        "              node_conll_deprel = 'inter'\n",
        "              # The present node thus becomes the new head for within the sentence and for the next one\n",
        "              intra_head = i_line_struct\n",
        "              inter_head = i_line_struct\n",
        "              i_line_struct += 1\n",
        "            # If we are in the same sentence as before, we attach the next node with the \"intra\" relation\n",
        "            else:\n",
        "              # conll_content.append(CoNLL_Node(i_line_struct, output_triple_object.property_label, '_', '_', '_', '_', str(intra_head), 'intra'))\n",
        "              node_conll_idx = i_line_struct\n",
        "              node_conll_head = str(intra_head)\n",
        "              node_conll_deprel = 'intra'\n",
        "              intra_head = i_line_struct\n",
        "              i_line_struct += 1\n",
        "\n",
        "          # In this block (now it's a function), get form, lemma, pos and feats\n",
        "          node_conll_form, node_conll_lemma, node_conll_pos, node_conll_feats = extract_formLemmaPosFeats(unique_properties_train, output_triple_object, property_label_in_conll, property_lemma_in_conll, property_class_in_conll, subject_in_conll, object_in_conll, coreference_in_conll)\n",
        "\n",
        "          # Create conll node object with ID and empty columns\n",
        "          node_conll = CoNLL_Node(node_conll_idx, node_conll_form, node_conll_lemma, node_conll_pos, '_', node_conll_feats, node_conll_head, node_conll_deprel)\n",
        "          conll_content.append(node_conll)\n",
        "\n",
        "      # Add missing properties for test set after the last grouping; for these, is it possible to get the feats too?\n",
        "      # If this is activated, there should be a part of the code to do the same thing on the gold data (see last cell below)\n",
        "      if add_missing_properties == True:\n",
        "        if data_split == 'test':\n",
        "          if len(missing_properties) > 0:\n",
        "            print(f'  Added {len(missing_properties)} missing properties to #{str(len(conll_contents))} ({unique_input_ID})')\n",
        "            for missing_property in missing_properties:\n",
        "              conll_content.append(CoNLL_Node(i_line_struct, missing_property, '_', '_', '_', [], str(intra_head), 'intra'))\n",
        "              intra_head = i_line_struct\n",
        "              i_line_struct += 1\n",
        "      conll_contents.append(conll_content)\n",
        "    else:\n",
        "      count_duplicate_inputs += 1\n",
        "  message = f'There are {len(conll_contents)} data points in {data_split} ({count_duplicate_inputs} duplicate inputs'\n",
        "  if inOutPairs_kept == 'all':\n",
        "    print(message+').')\n",
        "  else:\n",
        "    print(message+' were removed).')\n",
        "  return conll_contents\n",
        "\n",
        "conll_contents_train = ''\n",
        "conll_contents_dev = ''\n",
        "conll_contents_test = ''\n",
        "\n",
        "if use_only_conllNode_class == False:\n",
        "  conll_contents_train = build_CoNLL_Lines(final_data, 'train', unique_properties_train, inOutPairs_kept, print_groupings)\n",
        "  conll_contents_dev = build_CoNLL_Lines(final_data, 'dev', unique_properties_train, inOutPairs_kept, print_groupings)\n",
        "  conll_contents_test = build_CoNLL_Lines(final_data, 'test', unique_properties_train, inOutPairs_kept, print_groupings)\n",
        "\n",
        "# print(len(conll_contents_test))\n",
        "# Print output\n",
        "# for conll_2be in conll_contents_test:\n",
        "#   for line_2be in conll_2be:\n",
        "#     print(f'{line_2be.idx} {line_2be.form} {line_2be.lemma} {line_2be.pos} {line_2be.feats} {line_2be.head} {line_2be.deprel}')\n"
      ],
      "metadata": {
        "id": "un4Hcl-1xQm3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Debug: Print sample datapoint\n",
        "for i, test_check_data in enumerate(conll_contents_test[:5]):\n",
        "  print(f'---{i}---')\n",
        "  for tcl in test_check_data:\n",
        "    print(f'{tcl.idx} {tcl.form} {tcl.lemma} {tcl.pos} {tcl.feats} {tcl.head} {tcl.deprel}')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PfuQuk9MEd4V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount drive if you want to save data to drive in the next cells\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "KyMf2P5kDfpf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Write file in CoNLL format\n",
        "import codecs\n",
        "import os\n",
        "\n",
        "conll_format = 'CoNLL-U'#@param['CoNLL09', 'CoNLL-U']\n",
        "# If True, will put all the contents in the same conll with a bubble around each sentence (False for Text Structuring)\n",
        "same_conll = False#@param {type:\"boolean\"}\n",
        "# In case you want to just load the funtion without running the code in this cell\n",
        "use_only_instantiate_conll_function = False#@param {type:\"boolean\"}\n",
        "\n",
        "conllFolder = '/content/conllOut'\n",
        "conllSplitFolder= os.path.join(conllFolder, 'conllu2conll')\n",
        "if not os.path.exists(conllSplitFolder):\n",
        "  os.makedirs(conllSplitFolder)\n",
        "\n",
        "# To keep here the code of the dataset\n",
        "final_folder_ID = ''\n",
        "\n",
        "# Moved in cell above\n",
        "# class CoNLL_Node:\n",
        "#   def __init__(self, idx, form, feats, head, deprel):\n",
        "#     self.idx = idx\n",
        "#     self.form = form\n",
        "#     self.feats = feats\n",
        "#     self.head = head\n",
        "#     self.deprel = deprel\n",
        "\n",
        "def instantiate_conll_template(sentences, conll_format, same_conll):\n",
        "  \"\"\"\n",
        "  Takes as input a list of sentences. Each sentence is a list of tokens of the CoNLL_Node class.\n",
        "  Returns a CoNLL structure in the 14-column 2009 format (tree or graph, with or without bubbles), or in the 10-column CoNLL-U format (tree, no bubbles).\n",
        "  \"\"\"\n",
        "  if conll_format == 'CoNLL-U' and same_conll == True:\n",
        "    print(f'Cannot create bubbles in CoNL-U format; keeping sentences separated.')\n",
        "\n",
        "  conll_str = ''\n",
        "  if conll_format == 'CoNLL09' and same_conll == True:\n",
        "    conll_str = \"0\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\"\n",
        "\n",
        "  contentTemplate = ''\n",
        "  if conll_format == 'CoNLL09':\n",
        "    contentTemplate = \"{0.idx}\t{0.form}\t{0.lemma}\t_\t{0.pos}\t_\t{0.feats}\t_\t{0.head}\t_\t{0.deprel}\t_\t_\t_\"\n",
        "  if conll_format == 'CoNLL-U':\n",
        "    contentTemplate = \"{0.idx}\t{0.form}\t{0.lemma}\t{0.pos}\t{0.cpos}\t{0.feats}\t{0.head}\t{0.deprel}\t_\t_\"\n",
        "\n",
        "  sentence_count = len(sentences)\n",
        "  if sentence_count == 0:\n",
        "    conll_str += \"\\tslex=Sentence\\tslex=Text\\n1\\tNo_relevant_information_found\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\ttrue\\ttrue\"\n",
        "\n",
        "  if conll_format == 'CoNLL09' and same_conll == True:\n",
        "    bubbles = []\n",
        "    if sentence_count > 1:\n",
        "      conll_str += \"\\tslex=Text\"\n",
        "      bubbles.append(\"true\")\n",
        "\n",
        "    offset = len(bubbles)\n",
        "    for i in range(sentence_count):\n",
        "      conll_str += \"\\tslex=Sentence\"\n",
        "      bubbles.append(\"_\")\n",
        "    conll_str += \"\\n\"\n",
        "\n",
        "  line_count = 0\n",
        "  for idx, sentence in enumerate(sentences):\n",
        "    if conll_format == 'CoNLL09' and same_conll == True:\n",
        "      bubbles[idx + offset] = \"true\"\n",
        "      bubbles_str = \"\\t\" + \"\\t\".join(bubbles)\n",
        "      bubbles[idx + offset] = \"_\"\n",
        "    for id_tok, token in enumerate(sentence):\n",
        "      token.idx = str(line_count+1)\n",
        "      conll_str += contentTemplate.format(token)\n",
        "      if conll_format == 'CoNLL09' and same_conll == True:\n",
        "        conll_str += bubbles_str\n",
        "      else:\n",
        "        # If we are at the last token of a sentence and we don't want bubbles, add linebreak except for last structure of file\n",
        "        if id_tok == len(sentence)-1 and idx < len(sentences) - 1:\n",
        "          conll_str += \"\\n\"\n",
        "          # Also set back counter to -1 (not 0 because just below it gets a +1 increase already)\n",
        "          line_count = -1\n",
        "      line_count += 1\n",
        "      conll_str += \"\\n\"\n",
        "  # Don't add linebrak after last structure (edit: actually it seems to break the scrambling for some reason)\n",
        "  # if idx < len(sentences) - 1:\n",
        "  conll_str += \"\\n\"\n",
        "\n",
        "  return conll_str\n",
        "\n",
        "# Toy data structure to test conll building\n",
        "# word1 = CoNLL_Node(1, 'born', 'tense=PAST', '0', 'root')\n",
        "# word2 = CoNLL_Node(2, 'Jesus', 'class=Person|dpos=NP', '1', 'A1')\n",
        "# word3 = CoNLL_Node(3, 'Dec.25', '_', '1', 'Time')\n",
        "# sentences = [[word1, word2, word3], [word1, word2]]\n",
        "# conll = instantiate_conll_template(sentences, conll_format, same_conll)\n",
        "\n",
        "# print(conll)\n",
        "\n",
        "ext1 = ''\n",
        "ext2 = ''\n",
        "if use_only_instantiate_conll_function == False:\n",
        "  if exclude_input_size == 'size1&2&3':\n",
        "    ext1 = '4'\n",
        "  elif exclude_input_size == 'size1&2':\n",
        "    ext1 = '3'\n",
        "  elif exclude_input_size == 'size1':\n",
        "    ext1 = '2'\n",
        "  if inOutPairs_kept == 'all':\n",
        "    ext1 = ext1+'A'\n",
        "  elif inOutPairs_kept == '1perTripleSet':\n",
        "    ext1 = ext1+'T'\n",
        "  elif inOutPairs_kept == 'onlyTrulyUnique':\n",
        "    ext1 = ext1+'U'\n",
        "  if property_label_in_conll == 'original':\n",
        "    ext1 = ext1+'O'\n",
        "  elif property_label_in_conll == 'split':\n",
        "    ext1 = ext1+'S'\n",
        "  if property_lemma_in_conll == True:\n",
        "    ext1 = ext1+'e1'\n",
        "  if property_class_in_conll == True:\n",
        "    ext1 = ext1+'p1'\n",
        "  if subject_in_conll == 'merged':\n",
        "    ext1 = ext1+'d1'\n",
        "  elif subject_in_conll == 'abstract':\n",
        "    ext1 = ext1+'d2'\n",
        "  elif subject_in_conll == 'original':\n",
        "    ext1 = ext1+'d3'\n",
        "  if object_in_conll == 'merged':\n",
        "    ext1 = ext1+'r1'\n",
        "  elif object_in_conll == 'abstract':\n",
        "    ext1 = ext1+'r2'\n",
        "  elif object_in_conll == 'original':\n",
        "    ext1 = ext1+'r3'\n",
        "  if coreference_in_conll == 'sbj':\n",
        "    ext1 = ext1+'c1'\n",
        "  elif coreference_in_conll == 'obj':\n",
        "    ext1 = ext1+'c2'\n",
        "  elif coreference_in_conll == 'sbj&obj':\n",
        "    ext1 = ext1+'c3'\n",
        "  if conll_format == 'CoNLL-U':\n",
        "    ext2 = '.conllu'\n",
        "  elif conll_format == 'CoNLL09':\n",
        "    ext2 = '.conll'\n",
        "\n",
        "  final_folder_ID = ext1\n",
        "\n",
        "  conll_train = instantiate_conll_template(conll_contents_train, conll_format, same_conll)\n",
        "  conll_dev = instantiate_conll_template(conll_contents_dev, conll_format, same_conll)\n",
        "  conll_test = instantiate_conll_template(conll_contents_test, conll_format, same_conll)\n",
        "\n",
        "  with codecs.open(os.path.join(conllSplitFolder, 'train-TextStruct_'+ext1+ext2), 'w', 'utf-8') as fo1:\n",
        "    fo1.write(conll_train)\n",
        "\n",
        "  with codecs.open(os.path.join(conllSplitFolder, 'dev-TextStruct_'+ext1+ext2), 'w', 'utf-8') as fo2:\n",
        "    fo2.write(conll_dev)\n",
        "\n",
        "  with codecs.open(os.path.join(conllSplitFolder, 'test-TextStruct_'+ext1+ext2), 'w', 'utf-8') as fo3:\n",
        "    fo3.write(conll_test)\n",
        "\n",
        "# print('Download your file from the left!')"
      ],
      "metadata": {
        "id": "Rg2fZEAdiNVj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Debug: Print conll structures\n",
        "print(conll_test)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_-ldQ4oLFQF8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Scramble the CoNLLs for learning and move to final folder\n",
        "\n",
        "move_to_custom_folder = True#@param{type:\"boolean\"}\n",
        "\n",
        "# Paths to update:\n",
        "custom_folder_conllu = '/content/drive/MyDrive/M-FleNS/Papers-Slides/M-FleNS_papers/2024-05_Fluency_Improvements/Parsing4'#@param{type:\"string\"}\n",
        "\n",
        "path_jars = '/content/UD_Converter/Resources'\n",
        "path_conllScramble = os.path.join(path_jars, 'conllScramble.py')\n",
        "new_path_conllScramble = os.path.join(path_jars, 'conllScramble_TS.py')\n",
        "\n",
        "# Need to deactivate the introduction of original_id feat in the script:\n",
        "conll_scram_lines = codecs.open(path_conllScramble, 'r', 'utf-8').readlines()\n",
        "with codecs.open(new_path_conllScramble, 'w', 'utf-8') as fo_scram:\n",
        "  id_last_commented_line = 0\n",
        "  for i, conll_scram_line in enumerate(conll_scram_lines):\n",
        "    if re.search(\"^\\t\\t\\t\\tif dType == '1'\", conll_scram_line):\n",
        "      new_line = re.subn(\"^\\t\\t\\t\\tif dType == '1'\", \"#\\t\\t\\t\\tif dType == '1'\", conll_scram_line)[0]\n",
        "      id_last_commented_line = i\n",
        "      fo_scram.write(new_line)\n",
        "    # All next lines that are indented should be commented too\n",
        "    elif i == id_last_commented_line + 1 and re.search(\"^\\t\\t\\t\\t\\t\", conll_scram_line):\n",
        "      new_line = re.subn(\"^\\t\\t\\t\\t\\t\", \"#\\t\\t\\t\\t\\t\", conll_scram_line)[0]\n",
        "      id_last_commented_line = i\n",
        "      fo_scram.write(new_line)\n",
        "    else:\n",
        "      fo_scram.write(conll_scram_line)\n",
        "\n",
        "trainfile = 'train-TextStruct_'+ext1+'.conllu'\n",
        "devfile = 'dev-TextStruct_'+ext1+'.conllu'\n",
        "testfile = 'test-TextStruct_'+ext1+'.conllu'\n",
        "\n",
        "!python {new_path_conllScramble} {trainfile} {conllFolder} 't1' '1' {conllFolder}\n",
        "!python {new_path_conllScramble} {devfile} {conllFolder} 't1' '1' {conllFolder}\n",
        "!python {new_path_conllScramble} {testfile} {conllFolder} 't1' '1' {conllFolder}\n",
        "\n",
        "\n",
        "final_folder_path_scrambled = os.path.join(custom_folder_conllu, final_folder_ID)\n",
        "final_folder_path_lin = os.path.join(custom_folder_conllu, final_folder_ID+'-lin')\n",
        "if not os.path.exists(final_folder_path_scrambled):\n",
        "  os.makedirs(final_folder_path_scrambled)\n",
        "if not os.path.exists(final_folder_path_lin):\n",
        "  os.makedirs(final_folder_path_lin)\n",
        "\n",
        "# Rename original files with the extension \"-lin\" so as to avoid confusions and move to final folder\n",
        "for filepath in glob.glob(os.path.join(conllFolder, 'conllu2conll', '*.conllu')):\n",
        "  filepath_noext = filepath.rsplit('.', 1)[0]\n",
        "  new_filepath = filepath_noext+'-lin.conllu'\n",
        "  os.rename(filepath, new_filepath)\n",
        "  if move_to_custom_folder == True:\n",
        "    shutil.move(new_filepath, final_folder_path_lin)\n",
        "\n",
        "# Finally, move the scrambled files too\n",
        "if move_to_custom_folder == True:\n",
        "  for filepath2 in glob.glob(os.path.join(conllFolder, '*.conllu')):\n",
        "    shutil.move(filepath2, final_folder_path_scrambled)\n",
        "\n"
      ],
      "metadata": {
        "id": "dys-krUBQELy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert gold structuring file into CoNLL for eval with LAS and move to drive (only use if 1 input per triple set (T) above)\n",
        "import json\n",
        "import codecs\n",
        "import random\n",
        "import sys\n",
        "import re\n",
        "\n",
        "random.seed(4242)\n",
        "\n",
        "# Just to make sure we have the right file infix at this point (we want a T file).\n",
        "# Could also just manipulate the ext1 string but simpler like this for now, since we always need the T files anyway\n",
        "if not re.search('T', ext1):\n",
        "  sys.exit(\"Please use this cell only after generating the 'T' files for a combination of feats. \")\n",
        "\n",
        "\n",
        "# This is for making the reference file for the evaluation with LAS of the M-FleNS structuring step using Gold ordering as input (in a similar fashion to what was done in Thiago's experiments)\n",
        "# This file is used as input for the parser too (the first columns only; the gov and deprel columns are only used for evaluation as far as I know)\n",
        "data_split_gold_str = 'test'#@param['dev', 'test']\n",
        "\n",
        "# The following file is in C:\\Users\\sfmil\\OneDrive\\Desktop\\DCU\\Papers\\2024-05_Fluency-improvements\\TextPlanning\\thiago_eval\n",
        "path_gold_structuring = '/content/drive/MyDrive/M-FleNS/Papers-Slides/M-FleNS_papers/2024-05_Fluency_Improvements/Thiago-files/structuring_gold-'+data_split_gold_str+'.json'\n",
        "# The json looks like this inside:\n",
        "# [{\"eid\": \"Id285\", \"category\": \"Airport\", \"size\": \"2\", \"source\": [\"<TRIPLE>\", \"Aarhus_Airport\", \"operatingOrganisation\", \"\\\"Aarhus_Lufthavn_A/S\\\"\", \"</TRIPLE>\", \"<TRIPLE>\", \"Aarhus_Airport\", \"runwayLength\", \"2777.0\", \"</TRIPLE>\"], \"targets\": [{\"lid\": \"Id1\", \"comment\": \"good\", \"output\": [\"<SNT>\", \"operatingOrganisation\", \"runwayLength\", \"</SNT>\"]}, {\"lid\": \"Id2\", \"comment\": \"good\", \"output\": [\"<SNT>\", \"operatingOrganisation\", \"runwayLength\", \"</SNT>\"]}]}...]\n",
        "# To control if we add feats or not in the file, it will use the feats selected in the cell \"Create CoNLL line contents\" above.\n",
        "use_feats = 'yes'#@param['yes', 'no']\n",
        "\n",
        "contents_gold_struct = ''\n",
        "with codecs.open(path_gold_structuring, 'r', 'utf-8') as json_file:\n",
        "  contents_gold_struct = json.load(json_file)\n",
        "\n",
        "conll_contents_gold_struct = []\n",
        "for i, datapoint in enumerate(contents_gold_struct):\n",
        "  # print(f'---{i}---')\n",
        "  # Create a list to keep track of entities and use the list index as coreferring ID\n",
        "  list_entities_data_point = []\n",
        "  # Create conll content with the deprel and the property name\n",
        "  conll_content = []\n",
        "  inter_gov = 0\n",
        "  intra_gov = 0\n",
        "  last_element = ''\n",
        "  # Use the first target as reference\n",
        "  # (Sample target ['<SNT>', 'location', 'elevationAboveTheSeaLevel_(in_metres)', '</SNT>', '<SNT>', '</SNT>'])\n",
        "  idy = 0\n",
        "  prop_label = ''\n",
        "  gov = ''\n",
        "  deprel = ''\n",
        "  for target_element in datapoint['targets'][0]['output']:\n",
        "    # If an element is a <SNT> tag, update last_element\n",
        "    if target_element == '<SNT>':\n",
        "      last_element = '<SNT>'\n",
        "    # If it's a property, create a line with the available data\n",
        "    elif not target_element == '</SNT>':\n",
        "      idy += 1\n",
        "      prop_label = target_element\n",
        "      # Get position of property in the source field, where the subject (-1) and object (+1) can be found too\n",
        "      id_prop_in_source = datapoint['source'].index(target_element)\n",
        "      # Subject and Object labels all have underscores, instead of spaces in the original data, so I reestablish the spaces here\n",
        "      # Actually I added a version of all entities with underscores in the all_entities_classes_by_entity list\n",
        "      subject_label = datapoint['source'][id_prop_in_source-1]#.replace('_', ' ')\n",
        "      object_label = datapoint['source'][id_prop_in_source+1]#.replace('_', ' ')\n",
        "      if last_element == '<SNT>':\n",
        "        gov = inter_gov\n",
        "        if inter_gov == 0:\n",
        "          deprel = 'ROOT'\n",
        "        else:\n",
        "          deprel = 'inter'\n",
        "        inter_gov = idy\n",
        "        intra_gov = idy\n",
        "      else:\n",
        "        gov = intra_gov\n",
        "        deprel = 'intra'\n",
        "        intra_gov = idy\n",
        "      last_element = target_element\n",
        "\n",
        "      if use_feats == 'yes':\n",
        "        # Make the current data into a triple object\n",
        "        goldStruct_metadata = [data_split_gold_str, datapoint['category'], datapoint['eid'], datapoint['size']]\n",
        "        goldStruct_triple_object = Triple(f'{subject_label} | {target_element} | {object_label}', goldStruct_metadata)\n",
        "        # Add subjects to the list of entities\n",
        "        if goldStruct_triple_object.subject_label not in list_entities_data_point:\n",
        "          list_entities_data_point.append(goldStruct_triple_object.subject_label)\n",
        "        # Add objects to the list of entities\n",
        "        if goldStruct_triple_object.object_label not in list_entities_data_point:\n",
        "          list_entities_data_point.append(goldStruct_triple_object.object_label)\n",
        "        # Shuffle the list of entities so the coref IDs have the same shape as the training/dev data\n",
        "        random.shuffle(list_entities_data_point)\n",
        "        # Update the triple object with the class information\n",
        "        add_class_info_triple_objects(goldStruct_triple_object, new_all_entities_classes_by_entity, list_entities_data_point)\n",
        "        # Get form, lemma, pos and feats as defined earlier in the pipeline\n",
        "        node_conll_form, node_conll_lemma, node_conll_pos, node_conll_feats = extract_formLemmaPosFeats(unique_properties_train, goldStruct_triple_object,  property_label_in_conll, property_lemma_in_conll, property_class_in_conll, subject_in_conll, object_in_conll, coreference_in_conll)\n",
        "        conll_content.append(CoNLL_Node(idy, node_conll_form, node_conll_lemma, node_conll_pos, '_', node_conll_feats, str(gov), deprel))\n",
        "      else:\n",
        "        conll_content.append(CoNLL_Node(idy, prop_label, '_', '_', '_', [], str(gov), deprel))\n",
        "  conll_contents_gold_struct.append(conll_content)\n",
        "\n",
        "# for line_conll_struct in conll_contents_gold_struct[2000]:\n",
        "#   print (line_conll_struct.idx, line_conll_struct.form)\n",
        "\n",
        "# Create conll file\n",
        "conll_gold_struct = instantiate_conll_template(conll_contents_gold_struct, 'CoNLL-U', False)\n",
        "with codecs.open(os.path.join(custom_folder_conllu, '0_structuring_gold_input', 'conll_gold_struct_'+ext1+'-'+data_split_gold_str+'.conllu'), 'w', 'utf-8') as fo1:\n",
        "  fo1.write(conll_gold_struct)"
      ],
      "metadata": {
        "id": "dtKPEekdxTwS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}