{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/Text_Structuring/blob/main/Parsers_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WcHKFA-LBlM"
      },
      "source": [
        "# Stanza: retrain parser on UD data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5YP6_AGILAra"
      },
      "outputs": [],
      "source": [
        "#@title Prepare and setup repo\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "\n",
        "#https://stanfordnlp.github.io/stanza/retrain_ud.html\n",
        "! git clone 'https://github.com/stanfordnlp/stanza.git'\n",
        "udbase = '/content/Universal_Dependencies/git'\n",
        "ubdase_EN_EWT = os.path.join(udbase, 'UD_English-EWT')\n",
        "! git clone 'https://github.com/UniversalDependencies/UD_English-EWT.git' {ubdase_EN_EWT}\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['PYTHONPATH'] = '/content'\n",
        "os.environ['STANZA_RESOURCES_DIR'] = '/content/stanza_resources'\n",
        "os.environ['TOKENIZE_DATA_DIR'] = '/content/data/tokenize'\n",
        "os.environ['DEPPARSE_DATA_DIR'] = '/content/data/depparse'\n",
        "os.environ['UDBASE'] = udbase\n",
        "\n",
        "# Needed to run some of the code\n",
        "! pip install emoji\n",
        "\n",
        "# Copy files in stanza/stanza to folder above (stanza) otherwise the code doesn't find them\n",
        "! cp '/content/stanza/stanza/_version.py' '/content/stanza/_version.py'\n",
        "! cp '/content/stanza/stanza/__init__.py' '/content/stanza/__init__.py'\n",
        "\n",
        "stanza_folder =  '/content/stanza'\n",
        "! cp -r '/content/stanza/stanza/models' {stanza_folder}\n",
        "! cp -r '/content/stanza/stanza/pipeline' {stanza_folder}\n",
        "! cp -r '/content/stanza/stanza/protobuf' {stanza_folder}\n",
        "! cp -r '/content/stanza/stanza/resources' {stanza_folder}\n",
        "! cp -r '/content/stanza/stanza/server' {stanza_folder}\n",
        "! cp -r '/content/stanza/stanza/tests' {stanza_folder}\n",
        "! cp -r '/content/stanza/stanza/utils' {stanza_folder}\n",
        "\n",
        "clear_output()\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8zkrmumbD3Z"
      },
      "source": [
        "## Train tokenizer (not needed for parser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PiwN7Ov5N5-T"
      },
      "outputs": [],
      "source": [
        "#@title Read the dataset from UDBASE and write it to TOKENIZE_DATA_DIR\n",
        "# The last element (UD_English-EWT) is searched in the udbase subfolder of the same name\n",
        "! python -m stanza.utils.datasets.prepare_tokenizer_treebank UD_English-EWT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "T6u9HWYGYTaa"
      },
      "outputs": [],
      "source": [
        "#@title Train the tokenizer model\n",
        "# This puts the result in saved_models/tokenize/en_ewt_tokenizer.pt\n",
        "! python -m stanza.utils.training.run_tokenizer UD_English-EWT\n",
        "\n",
        "# Note: Every 200 epochs, the model is saved if better than the previous one; the training seems to continue even though the Dev doesn't improve\n",
        "# 400 epochs:   [~5 min CPU] 89.486 Dev;  [~7 sec GPU] 88.603 Dev\n",
        "# 1,000 epochs: [~10 min CPU] 92.681 Dev; [~18 sec GPU] 92.701 Dev\n",
        "# 1,800 epochs: [~20 min CPU] 93.596 Dev; [~30 sec GPU] 93.539 Dev\n",
        "# 2,800 epochs: [~30 min CPU] 93.934 Dev; [~50 sec GPU] 93.709 Dev\n",
        "# 5,000 epochs:                           [~90 sec GPU] 94.420 Dev\n",
        "# 9,800 epochs:                           [~200 sec GPU] 94.474 Dev\n",
        "# 14200 epochs:                           [~5 min GPU] 94.568 Dev\n",
        "# 2024-03-28 14:52:32 INFO: Dev score: 94.390\tStopping training after 5200 steps with no improvement\n",
        "# 2024-03-28 14:52:32 INFO: Best dev score=0.9479763240178896 at step 15400\n",
        "\n",
        "# If you already have a model saved, the training script will not overwrite that model. You can make that happen with --force:\n",
        "# ! python -m stanza.utils.training.run_tokenizer UD_English-EWT --force\n",
        "\n",
        "# If you want a different save name:\n",
        "# ! python -m stanza.utils.training.run_tokenizer UD_English-EWT --save_name en_ewt_variant_tokenizer.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-KBahigMZGks"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate tokenizer model\n",
        "! python -m stanza.utils.training.run_tokenizer UD_English-EWT --score_dev\n",
        "! python -m stanza.utils.training.run_tokenizer UD_English-EWT --score_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGaJ56e8lGGg"
      },
      "source": [
        "## Train parser (GPU needed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Copy my own files\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "path_textStruct = '/content/drive/MyDrive/M-FleNS/Papers-Slides/M-FleNS_papers/2024-05_Fluency_Improvements/Parsing4/'\n",
        "path_fake_UD = os.path.join(udbase, 'UD_English-EWT2')\n",
        "if not os.path.exists(path_fake_UD):\n",
        "  os.makedirs(path_fake_UD)\n",
        "\n",
        "data_code = '2AOe1p1d2r2c3'\n",
        "\n",
        "def copyFiles(path_in, data_code, path_out, split):\n",
        "  shutil.copy(os.path.join(path_in, data_code, split+'-TextStruct_'+data_code+'.conllu'), os.path.join(path_out, 'en_ewt-ud-'+split+'.conllu'))\n",
        "\n",
        "copyFiles(path_textStruct, data_code, path_fake_UD, 'train')\n",
        "copyFiles(path_textStruct, data_code, path_fake_UD, 'dev')\n",
        "copyFiles(path_textStruct, data_code, path_fake_UD, 'test')"
      ],
      "metadata": {
        "id": "whrZRFTSB-MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nqtxXkRllJSU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Read the dataset from UDBASE and write it to DEPPARSE_DATA_DIR (RUN CELL TWICE!)\n",
        "\n",
        "pos_tags = 'gold'#@param['predicted', 'gold']\n",
        "\n",
        "if pos_tags == 'predicted':\n",
        "  # The first time the cell is run, it will download files but then won't find them; the second time running the cell will copy the files where expected and it will run\n",
        "  # The code looks for the following file for an existing pos tagger\n",
        "  ! cp '/content/stanza_resources/en/pos/ewt_charlm.pt' '/content/stanza_resources/en/pos/ewt.pt'\n",
        "  # To apply PoS tagger to get predicted PoS for training\n",
        "  ! python -m stanza.utils.datasets.prepare_depparse_treebank UD_English-EWT\n",
        "elif pos_tags == 'gold':\n",
        "  # To use the gold PoS tags, use the --gold flag\n",
        "  # ! python -m stanza.utils.datasets.prepare_depparse_treebank UD_English-EWT --gold\n",
        "  ! python -m stanza.utils.datasets.prepare_depparse_treebank path_fake_UD --gold\n",
        "\n",
        "# Use this flag to use a different pretrained embeddings file: -⁠-⁠wordvec_pretrain_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYWKySkHngf0"
      },
      "outputs": [],
      "source": [
        "#@title Train the parser\n",
        "# This puts the result in saved_models/depparse/en_ewt_charlm_parser.pt\n",
        "! python -m stanza.utils.training.run_depparse UD_English-EWT\n",
        "\n",
        "# 100 epochs: [~70 sec GPU] 24.90 LAS Dev\n",
        "# 200 epochs: [~150 sec GPU] 64.89 LAS Dev\n",
        "# 400 epochs: [~5 min GPU] 78.03 LAS Dev\n",
        "# 600 epochs: [~8 min GPU] 82.92 LAS Dev\n",
        "# 800 epochs: [~10 min GPU] 84.79 LAS Dev\n",
        "# 1000 epochs: [~13 min GPU] 86.06 LAS Dev\n",
        "# 1200 epochs: [~16 min GPU] 86.64 LAS Dev\n",
        "# 1400 epochs: [~19 min GPU] 86.87 LAS Dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wHNEkaOosELX"
      },
      "outputs": [],
      "source": [
        " #@title Evaluate parser\n",
        "! python -m stanza.utils.training.run_depparse UD_English-EWT --score_dev\n",
        "! python -m stanza.utils.training.run_depparse UD_English-EWT --score_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IvCLrg87VCQ"
      },
      "source": [
        "# Maltparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rvqyUrzA7Q_v"
      },
      "outputs": [],
      "source": [
        "#@title Maven (just for testing, not needed))\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# !apt-get install maven\n",
        "# !apt-get install openjdk-8-jdk\n",
        "# !apt-get install openjfx\n",
        "# # ! mvn --version\n",
        "\n",
        "# # Download POM for MaltParser\n",
        "# ! gdown 1GquY2cUDUPLMGUu6GQAO2czkBbrmlztk\n",
        "\n",
        "# clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "pB37WKrATmoy"
      },
      "outputs": [],
      "source": [
        "#@title Prepare repo\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "\n",
        "# def install_java():\n",
        "#   !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "#   os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "#   !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "#   !java -version       #check java version\n",
        "# install_java()\n",
        "\n",
        "# Download and unzip MaltParser code from my gmail drive My Drive/M-FleNS/Parsing\n",
        "# Local files in C:\\Users\\sfmil\\OneDrive\\Desktop\\DCU\\Papers\\2024-05_Fluency-improvements\\TextPlanning\\parsing\n",
        "# The version of Maltparser I use has paths changed in the options.xml file, so as to find the local Colab feats files for liblinear/conllu\n",
        "! gdown 14iRdWjzmzl5QJkEaNE8mukpJxaa3i614\n",
        "! unzip /content/maltparser-1.9.2.zip\n",
        "\n",
        "# Download and unzip MaltEval code\n",
        "! gdown 11Wfoel-N9A3uqD1gsR0wI5UuRIYxRDNw\n",
        "! unzip /content/MaltEval-dist.zip\n",
        "\n",
        "# Download and unzip MaltOptimizer\n",
        "! gdown 1F3WHHAvO35FMSROaWFf38cSUWHYcL-Z6\n",
        "! unzip /content/MaltOptimizer-1.0.3.zip\n",
        "\n",
        "# Clone UD EN-EWT repo or testing\n",
        "udbase = '/content/Universal_Dependencies/git'\n",
        "ubdase_EN_EWT = os.path.join(udbase, 'UD_English-EWT')\n",
        "! git clone 'https://github.com/UniversalDependencies/UD_English-EWT.git' {ubdase_EN_EWT}\n",
        "\n",
        "# Clone repo for conversion of parser output to format needed for evaluation\n",
        "! git clone 'https://github.com/mille-s/Text_Structuring.git'\n",
        "\n",
        "clear_output()\n",
        "\n",
        "path_maltParser = '/content/maltparser-1.9.2/maltparser-1.9.2.jar'\n",
        "path_maltEval = '/content/dist-20141005/lib/MaltEval.jar'\n",
        "path_TS = '/content/Text_Structuring'\n",
        "\n",
        "! java -jar {path_maltParser}\n",
        "! java -jar {path_maltEval}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pfM-OS9HXpdk"
      },
      "outputs": [],
      "source": [
        "#@title Mount drive ans set path data repos\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "mflens_local_data_folder = '/content/Text_Structuring/data'\n",
        "if not os.path.exists(mflens_local_data_folder):\n",
        "  os.makedirs(mflens_local_data_folder)\n",
        "\n",
        "mflens_data_folder = '/content/drive/MyDrive/M-FleNS/Papers-Slides/M-FleNS_papers/2024-05_Fluency_Improvements/Parsing4'\n",
        "gold_structuring_folder = '/content/drive/MyDrive/M-FleNS/Papers-Slides/M-FleNS_papers/2024-05_Fluency_Improvements/Parsing4/0_structuring_gold_input'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and run parser on test/dev data"
      ],
      "metadata": {
        "id": "wjbLHIo0j2CK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "mWOZN2yWU2dT"
      },
      "outputs": [],
      "source": [
        "#@title Train parser and/or set parameters for eval (for eval only, uncheck both train_... boxes)\n",
        "# https://www.maltparser.org/userguide.html\n",
        "# https://www.maltparser.org/options.html\n",
        "import shutil\n",
        "import codecs\n",
        "\n",
        "# model_name = 'ud-en-ewt_parser'\n",
        "# training_data = '/content/Universal_Dependencies/git/UD_English-EWT/en_ewt-ud-train.conllu'\n",
        "\n",
        "learner = 'liblinear'#@param['liblinear', 'libsvm']\n",
        "parsing_algorithm = 'stackeager'#@param['nivreeager', 'stackproj', 'covnonproj', 'stackeager', 'stacklazy', 'planar']\n",
        "features = 'allFeats3_Mrg1'#@param['test', 'default', 'libsvmGuide', 'libsvmGuide_noPos', 'libsvmGuide_noFeats', 'MaltOptDefault_2AOe1p1d1r1c3', 'MaltOptDefault_2AOe1p1d2r2c3', 'MaltOptDefault_2AOe1d2r2c3', 'allFeats3', 'allFeats3_noPos', 'allFeats3_noPos_noLem', 'allFeats3_noFeats', 'allFeats3_Mrg1', 'allFeats3_Mrg1_noFeats', 'allFeats3_Mrg2', 'UposDeprel', 'FormDeprel', 'FormUposDeprel', 'FormUposDeprel3', 'FormUposDeprel5', 'FormUposDeprel7']\n",
        "mflens_data_fileID_train = '2AOe1p1d3r3c3'#@param['2AO', '2AOc1', '2AOc2', '2AOc3', '2AOe1p1', '2AOe1c3', '2AOe1p1c3', '2AOe1p1d1r1c3', '2AOe1p1d2r2', '2AOe1p1d2r2c3', '2AOe1p1d3r3c3', '2TO', '2TOc1', '2TOc2', '2TOc3', '2TOe1p1', '2TOe1c3', '2TOe1p1d1r1c3', '2TOe1p1d2r2c3', '2UO', '3AO', '3AOe1p1d1r1c1', '3TO', '3UO']\n",
        "# For evaluating text structuring only\n",
        "train_parser_scrambled_data = True #@param{type:\"boolean\"}\n",
        "train_parser_lin_data = True #@param{type:\"boolean\"}\n",
        "\n",
        "parameters_concat = learner+'-'+parsing_algorithm+'_'+features\n",
        "\n",
        "def train_parser(path_maltParser, parsing_algorithm, learner, mflens_data_fileID_train, use_lin_data):\n",
        "  model_name = 'textStruct_parser_'+mflens_data_fileID_train\n",
        "  training_data = ''\n",
        "  if use_lin_data == True:\n",
        "    training_data = os.path.join(mflens_data_folder, mflens_data_fileID_train+'-lin', 'train-TextStruct_'+mflens_data_fileID_train+'-lin.conllu')\n",
        "    model_name = model_name+'-lin'\n",
        "  else:\n",
        "    training_data = os.path.join(mflens_data_folder, mflens_data_fileID_train, 'train-TextStruct_'+mflens_data_fileID_train+'.conllu')\n",
        "  # For covnonproj, we need to allow for Shift transition\n",
        "  if parsing_algorithm == 'covnonproj':\n",
        "    ! java -jar {path_maltParser} -c {model_name} -i {training_data} -if conllu -l {learner} -a {parsing_algorithm} -m learn -F {parsing_algorithm} -cs true\n",
        "  else:\n",
        "    ! java -jar {path_maltParser} -c {model_name} -i {training_data} -if conllu -l {learner} -a {parsing_algorithm} -m learn -F {parsing_algorithm}\n",
        "  return model_name\n",
        "\n",
        "# Copy feats file to liblinear/conllu (this path is hardcoded, use it even if training libsvm)\n",
        "# The first name corresponds to the static files that contain the features\n",
        "feature_xml_filename_toCopy = ''\n",
        "# The second name is the name needed by MaltParser\n",
        "feature_xml_filename_malt = ''\n",
        "if parsing_algorithm == 'nivreeager':\n",
        "  feature_xml_filename_toCopy = 'NivreEager'\n",
        "  feature_xml_filename_malt = 'NivreEager'\n",
        "elif parsing_algorithm == 'stackproj':\n",
        "  feature_xml_filename_toCopy = 'StackProjective'\n",
        "  feature_xml_filename_malt = 'StackProjective'\n",
        "elif parsing_algorithm == 'stackeager' and features.startswith('MaltOptDefault'):\n",
        "  feature_xml_filename_toCopy = 'StackEager'\n",
        "  feature_xml_filename_malt = 'StackSwap'\n",
        "elif parsing_algorithm == 'stacklazy' and features.startswith('MaltOptDefault'):\n",
        "  feature_xml_filename_toCopy = 'StackLazy'\n",
        "  feature_xml_filename_malt = 'StackSwap'\n",
        "elif parsing_algorithm == 'stackeager' or parsing_algorithm == 'stacklazy':\n",
        "  feature_xml_filename_toCopy = 'StackSwap'\n",
        "  feature_xml_filename_malt = 'StackSwap'\n",
        "elif parsing_algorithm == 'covnonproj':\n",
        "  feature_xml_filename_toCopy = 'CovingtonNonProjective'\n",
        "  feature_xml_filename_malt = 'CovingtonNonProjective'\n",
        "elif parsing_algorithm == 'planar':\n",
        "  feature_xml_filename_toCopy = 'PlanarEager'\n",
        "  feature_xml_filename_malt = 'PlanarEager'\n",
        "# Get the feature file needed to train the parser\n",
        "path_feature_xml_toCopy_filename = os.path.join(path_TS, 'data', 'malt_feats', feature_xml_filename_toCopy+'-'+features+'.xml')\n",
        "# Copy the file to where MaltParser looks for it; reminder: I hardcoded the path to the features file and we always use the one in liblinear\n",
        "feature_xml_path = os.path.join('/content/maltparser-1.9.2/appdata/features', 'liblinear', 'conllu', feature_xml_filename_malt+'.xml')\n",
        "# Remove default file and copy new file\n",
        "if os.path.exists(feature_xml_path):\n",
        "  os.remove(feature_xml_path)\n",
        "shutil.copy(path_feature_xml_toCopy_filename, feature_xml_path)\n",
        "print(f'File:\\n  {feature_xml_path}\\ncopied from:\\n  {path_feature_xml_toCopy_filename}')\n",
        "\n",
        "model_name = ''\n",
        "if train_parser_scrambled_data == True:\n",
        "  model_name = train_parser(path_maltParser, parsing_algorithm, learner, mflens_data_fileID_train, False)\n",
        "\n",
        "if train_parser_lin_data == True:\n",
        "  model_name = train_parser(path_maltParser, parsing_algorithm, learner, mflens_data_fileID_train, True)\n",
        "\n",
        "#Print configuration file\n",
        "! java -jar '/content/maltparser-1.9.2/maltparser-1.9.2.jar' -c {model_name} -m info > log.txt\n",
        "\n",
        "# clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "yN9N764MblfL"
      },
      "outputs": [],
      "source": [
        "#@title Save models to drive (before changing parser parameters)\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "# Custom path\n",
        "path_save_models_drive = os.path.join(mflens_data_folder, 'MaltModels', parameters_concat)\n",
        "if not os.path.exists(path_save_models_drive):\n",
        "  os.makedirs(path_save_models_drive)\n",
        "\n",
        "# Save feature configuration file in the same folder\n",
        "print(feature_xml_path)\n",
        "shutil.copy(feature_xml_path, path_save_models_drive)\n",
        "\n",
        "paths_models = glob.glob(os.path.join('/content', '*.mco'))\n",
        "for path_model in paths_models:\n",
        "  # Save model\n",
        "  print(path_model)\n",
        "  shutil.copy(path_model, path_save_models_drive)\n",
        "  # Delete model so it isn't copied by mistake somewhere else\n",
        "  os.remove(path_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WKyjUIQf--2D"
      },
      "outputs": [],
      "source": [
        "#@title Parameters for evaluation (before evaluating, SAVE PARAMS!)\n",
        "\n",
        "# Need to run the evaluation 3 times to get all the numbers:\n",
        "# - parser + corresponding test file NO LIN: for LAS same\n",
        "# - parser + 2TO test file NO LIN: for LAS 2T1.4K and ordering (+structuring) evals\n",
        "# - parser + 2TO test file  LIN: for LAS 2T2.7K and structuring only eval\n",
        "\n",
        "parser_to_use = '2AOe1p1d3r3c3'#@param['2AO', '2AOc1', '2AOc2', '2AOc3', '2AOe1p1', '2AOe1c3', '2AOe1p1c3', '2AOe1p1d1r1c3', '2AOe1p1d2r2', '2AOe1p1d2r2c3', '2AOe1p1d3r3c3', '2TO', '2TOc1', '2TOc2', '2TOc3', '2TOe1p1', '2TOe1c3', '2TOe1p1d1r1c3', '2UO', '3AO', '3TO', '3AOe1p1d1r1c1', '3UO']\n",
        "mflens_data_fileID_eval = '2TOe1p1d3r3c3'#@param['2AO', '2AOc1', '2AOc2', '2AOc3', '2AOe1p1', '2AOe1c3', '2AOe1p1c3', '2AOe1p1d1r1c3', '2AOe1p1d2r2', '2AOe1p1d2r2c3', '2AOe1p1d3r3c3', '2TO', '2TOc1', '2TOc2', '2TOc3', '2TOe1p1', '2TOe1c3', '2TOe1p1c3', '2TOe1p1d1r1c3', '2TOe1p1d2r2', '2TOe1p1d2r2c3', '2TOe1p1d3r3c3', '2UO', '3AO', '3AOe1p1d1r1c1', '3TO', '3UO']\n",
        "data_split_eval = 'test'#@param['dev', 'test']\n",
        "use_parser_lin_data = True #@param{type:\"boolean\"}\n",
        "eval_on_lin_file = True #@param{type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhBmC8orzpGU"
      },
      "source": [
        "## Evaluate and convert output format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "842dW0_CQKV8"
      },
      "outputs": [],
      "source": [
        "#@title Run parser on dev/test data\n",
        "import sys\n",
        "\n",
        "if use_parser_lin_data == True:\n",
        "  if not parser_to_use.endswith('-lin'):\n",
        "    parser_to_use = parser_to_use+'-lin'\n",
        "\n",
        "model_name_run = 'textStruct_parser_'+parser_to_use\n",
        "# From what I see the parser need to be on the root for MaltEval to be able to use it, so I coy it from the repo it is saved in\n",
        "path_saved_model = os.path.join(mflens_data_folder, 'MaltModels', learner+'-'+parsing_algorithm+'_'+features, model_name_run+'.mco')\n",
        "dest_path_model = os.path.join('/content', model_name_run+'.mco')\n",
        "shutil.copy(path_saved_model, dest_path_model)\n",
        "print(f'Copied model from {path_saved_model}')\n",
        "\n",
        "eval_data = ''\n",
        "# eval_data = '/content/Universal_Dependencies/git/UD_English-EWT/en_ewt-ud-test.conllu'\n",
        "\n",
        "folder_Malt_drive = os.path.join(mflens_data_folder, parser_to_use, 'Malt_'+parameters_concat)\n",
        "if not os.path.exists(folder_Malt_drive):\n",
        "  os.makedirs(folder_Malt_drive)\n",
        "\n",
        "pred_file = ''\n",
        "if eval_on_lin_file == True:\n",
        "  if not mflens_data_fileID_eval.startswith('2TO'):\n",
        "    sys.exit('You should choose a file that has the prefix 2T0 for this evaluation.')\n",
        "  eval_data = os.path.join(gold_structuring_folder, 'conll_gold_struct_'+mflens_data_fileID_eval+'-'+data_split_eval+'.conllu')\n",
        "  # Save file in folder of parser to use, but the with name of the 2T file\n",
        "  pred_file = os.path.join(folder_Malt_drive, data_split_eval+'-out_'+mflens_data_fileID_eval+'-lin_fromParser-'+parser_to_use+'.conll')\n",
        "else:\n",
        "  eval_data = os.path.join(mflens_data_folder, mflens_data_fileID_eval, data_split_eval+'-TextStruct_'+mflens_data_fileID_eval+'.conllu')\n",
        "  if parser_to_use == mflens_data_fileID_eval:\n",
        "    pred_file = os.path.join(mflens_data_folder, mflens_data_fileID_eval, 'Malt_'+parameters_concat, data_split_eval+'-out_'+mflens_data_fileID_eval+'_fromParser-'+parser_to_use+'.conll')\n",
        "  else:\n",
        "    pred_file = os.path.join(folder_Malt_drive, data_split_eval+'-out_'+mflens_data_fileID_eval+'_fromParser-'+parser_to_use+'.conll')\n",
        "\n",
        "! java -jar {path_maltParser} -c {model_name_run} -i {eval_data} -o {pred_file} -m parse -nt true\n",
        "\n",
        "# clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "HbBZiL5bG1g3"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate parser (MaltEval)\n",
        "import os\n",
        "import codecs\n",
        "import re\n",
        "\n",
        "def convertToCoNLL(filepath):\n",
        "  filepath_noExt = filepath.rsplit('.', 1)[0]\n",
        "  file_lines = codecs.open(filepath, 'r', 'utf-8').readlines()\n",
        "  out_file_path = filepath_noExt+'_maltEval.conll'\n",
        "  with codecs.open(out_file_path, 'w', 'utf-8') as fo:\n",
        "    for line in file_lines:\n",
        "      # Only write lines that are not metedata or multiple words (starting with 22-23)\n",
        "      if re.search('^[0-9]+\\t', line):\n",
        "        # Copy columns 7-8 in 9-10 for MaltEval\n",
        "        new_line = re.subn('^([^\\t\\n]+\\t[^\\t\\n]+\\t[^\\t\\n]+\\t[^\\t\\n]+\\t[^\\t\\n]+\\t[^\\t\\n]+\\t)([^\\t\\n]+\\t[^\\t\\n]+\\t)[^\\t\\n]+\\t[^\\t\\n]+', '\\g<1>\\g<2>\\g<2>', line)[0]\n",
        "        fo.write(new_line)\n",
        "      elif line.strip() == '':\n",
        "        fo.write('\\n')\n",
        "  return out_file_path\n",
        "\n",
        "# Convert files to pure conll\n",
        "pred_file_maltEval = convertToCoNLL(pred_file)\n",
        "gold_file_maltEval = convertToCoNLL(eval_data)\n",
        "\n",
        "! java -jar {path_maltEval} -s {pred_file_maltEval} -g {gold_file_maltEval}\n",
        "\n",
        "# Running default everything: ! java -jar {path_maltParser} -c {model_name} -i {training_data} -m learn, ! java -jar {path_maltParser} -c {model_name} -i {test_data} -o {pred_file} -m parse, ! java -jar {path_maltEval} -s {pred_file_maltEval} -g {gold_file_maltEval}\n",
        "# ====================================================\n",
        "# GroupBy-> Token\n",
        "# Metric-> LAS\n",
        "# ====================================================\n",
        "# accuracy /    Token\n",
        "# -----------------------\n",
        "# 0.827         Row mean\n",
        "# 25094         Row count\n",
        "# -----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1fGWft9Ckwt"
      },
      "outputs": [],
      "source": [
        "print(pred_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "d2o5XnZ3SRzO"
      },
      "outputs": [],
      "source": [
        "#@title Convert CoNLLs to Thiago's format for evaluation and copy to drive\n",
        "path_save_out = os.path.join('/content/drive/MyDrive/M-FleNS/Papers-Slides/M-FleNS_papers/2024-05_Fluency_Improvements/Parsing4/Files4CastroEval', parameters_concat)\n",
        "folder_save_out = parser_to_use.split('-', 1)[0]\n",
        "path_converter = '/content/Text_Structuring/code/conll2castro.py'\n",
        "pathConllFile = pred_file\n",
        "print_debug = False #@param {type:\"boolean\"}\n",
        "\n",
        "if not os.path.exists(path_save_out):\n",
        "  os.makedirs(path_save_out)\n",
        "\n",
        "path_out_converted_files = os.path.join(path_save_out, folder_save_out)\n",
        "if not os.path.exists(path_out_converted_files):\n",
        "  os.makedirs(path_out_converted_files)\n",
        "\n",
        "if mflens_data_fileID_eval.startswith('2TO'):\n",
        "  pathOutFileOrdering = os.path.join(path_out_converted_files, 'ordering_'+parser_to_use+'-'+data_split_eval+'.out.postprocessed')\n",
        "  pathOutFileStructuring = os.path.join(path_out_converted_files, 'structuring_'+parser_to_use+'-'+data_split_eval+'.out.postprocessed')\n",
        "  ! python {path_converter} {pathConllFile} {print_debug} {pathOutFileOrdering} {pathOutFileStructuring}\n",
        "  # If we use linearised files, the ordering output is not relevant\n",
        "  if use_parser_lin_data == True and eval_on_lin_file == True:\n",
        "    ! rm {pathOutFileOrdering}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Erase models\n",
        "import glob\n",
        "paths_models_post = glob.glob(os.path.join('/content', '*.mco'))\n",
        "for path_model_post in paths_models_post:\n",
        "  # Delete model so it isn't copied by mistake somewhere else\n",
        "  os.remove(path_model_post)"
      ],
      "metadata": {
        "id": "mrCHr439LYwy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare order properties converted file"
      ],
      "metadata": {
        "id": "ISYtADpmPITm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N3iR_Riv_gJV"
      },
      "outputs": [],
      "source": [
        "#@title Compare order properties converted file\n",
        "# I want to see if the structuring parsers always output the same order of properties as in the input\n",
        "import json\n",
        "import os\n",
        "import codecs\n",
        "import re\n",
        "\n",
        "learner2 = 'libsvm'#@param['liblinear', 'libsvm']\n",
        "parsing_algorithm2 = 'stackeager'#@param['nivreeager', 'stackproj', 'covnonproj', 'stackeager', 'stacklazy', 'planar']\n",
        "features2 = 'MaltOptDefault_2AOe1p1d1r1c3'#@param['test', 'default', 'libsvmGuide', 'libsvmGuide_noPos', 'libsvmGuide_noFeats', 'MaltOptDefault_2AOe1p1d1r1c3', 'MaltOptDefault_2AOe1p1d2r2c3', 'MaltOptDefault_2AOe1d2r2c3', 'allFeats3', 'allFeats3_noPos', 'allFeats3_noFeats', 'allFeats3_Mrg1', 'allFeats3_Mrg2', 'UposDeprel', 'FormDeprel', 'FormUposDeprel', 'FormUposDeprel3', 'FormUposDeprel5', 'FormUposDeprel7']\n",
        "mflens_data_fileID_eval2 = '2AOe1p1d1r1c3'#@param['2AO', '2AOe1p1c3', '2AOe1p1d1r1c3', '2AOe1p1d2r2', '2AOe1p1d2r2c3', '2AOe1p1d3r3c3', '2TO', '2TOe1p1d1r1c3', '2TOe1p1d2r2c3', '2UO', '3AO', '3AOe1p1d1r1c1', '3TO', '3UO']\n",
        "data_split_eval2 = 'test'#@param['dev', 'test']\n",
        "\n",
        "path_gold_struct_json = os.path.join('/content/drive/MyDrive/M-FleNS/Papers-Slides/M-FleNS_papers/2024-05_Fluency_Improvements/Thiago-files', 'structuring_gold-'+data_split_eval2+'.json')\n",
        "file_gold_struct_json = json.load(open(path_gold_struct_json))\n",
        "path_pred_struct = os.path.join(mflens_data_folder, 'Files4CastroEval', learner2+'-'+parsing_algorithm2+'_'+features2, mflens_data_fileID_eval2, 'structuring_'+mflens_data_fileID_eval2+'-lin-'+data_split_eval2+'.out.postprocessed')\n",
        "file_pred_struct = codecs.open(path_pred_struct, 'r', 'utf-8').readlines()\n",
        "\n",
        "x = 0\n",
        "count_identical_order = 0\n",
        "while x < len(file_gold_struct_json):\n",
        "  # Get the order from the first target (all targets have the same order)\n",
        "  # First remove empty <SNT> tags (<SNT> </SNT>) in the reference, then remove the internal <SNT> tags on both reference and prediction\n",
        "  # Eventually, we are left with the initial and final tags and, in between them, the list of properties\n",
        "  gold_struct = re.sub(' </SNT> <SNT>', '', re.sub(' <SNT> </SNT>', '', ' '.join(file_gold_struct_json[x]['targets'][0]['output']))).strip()\n",
        "  pred_struct = re.sub(' </SNT> <SNT>', '', file_pred_struct[x]).strip()\n",
        "  # If nothing is printed, it means that the properties in the pred file are always in the same order as the properties in the reference file\n",
        "  if gold_struct == pred_struct:\n",
        "    count_identical_order += 1\n",
        "  else:\n",
        "    print(f'Mismatch in input #{x}:\\n  {gold_struct}\\n  {pred_struct}')\n",
        "  x += 1\n",
        "\n",
        "if count_identical_order == len(file_gold_struct_json):\n",
        "  print('Properties are always in the same order in the predicted file and the gold file.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check data !!! RESTART RUNTIME AFTER USING, I'M CHANGING THE ROOT DIR (not sure why)"
      ],
      "metadata": {
        "id": "fPSx4Z_PjlW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check data\n",
        "import glob\n",
        "import codecs\n",
        "import re\n",
        "\n",
        "# Why is this next line needed by the way?\n",
        "os.chdir(mflens_data_folder)\n",
        "\n",
        "# Get names of folders that have training data (don't select -lin folders at this point, we'll check if they exist with this code)\n",
        "list_dataTrain_folders = [name for name in os.listdir(\".\") if os.path.isdir(name) and (name.startswith('2A') or name.startswith('3A')) and not name.endswith('-lin')]\n",
        "\n",
        "def createRegexCheck(stringWithFeats):\n",
        "  \"\"\" Build regex to check each line of the file based on a string (folder or filename) than contains codes for the CoNLL contents\"\"\"\n",
        "  # Build regex to check each line of the file\n",
        "  regex_search = '^[ 0-9]+\\t[^\\t]+\\t'\n",
        "  feats = []\n",
        "  if re.search('e[1-9]', stringWithFeats):\n",
        "    regex_search = regex_search+'[^_][^\\t]*\\t'\n",
        "  else:\n",
        "    regex_search = regex_search+'_\\t'\n",
        "  if re.search('p[1-9]', stringWithFeats):\n",
        "    regex_search = regex_search+'[^_][^\\t]*\\t_\\t'\n",
        "  else:\n",
        "    regex_search = regex_search+'_\\t_\\t'\n",
        "  # Feats\n",
        "  if re.search('d[1-9]', stringWithFeats):\n",
        "    feats.append('dom_class=[^\\t\\|]+')\n",
        "  if re.search('r[1-9]', stringWithFeats):\n",
        "    feats.append('ran_class=[^\\t\\|]+')\n",
        "  if re.search('c1', stringWithFeats):\n",
        "    feats.append('dom_ID=[^\\t\\|]+')\n",
        "  elif re.search('c2', stringWithFeats):\n",
        "    feats.append('ran_ID=[^\\t\\|]+')\n",
        "  elif re.search('c3', stringWithFeats):\n",
        "    feats.append('dom_ID=[^\\t\\|]+')\n",
        "    feats.append('ran_ID=[^\\t\\|]+')\n",
        "  if len(feats) == 0:\n",
        "    regex_search = regex_search+'_\\t'\n",
        "  else:\n",
        "    regex_search = regex_search+'\\|'.join(feats)+'\\t'\n",
        "  # Head and dependency\n",
        "  regex_search = regex_search+'[0-9]+\\t[^\\t]+\\t_\\t_$'\n",
        "  return regex_search\n",
        "\n",
        "def checkConllu(pathFolderToCheck, missing_contents):\n",
        "  for filepath_conllu in pathFolderToCheck:\n",
        "    head, tail = os.path.split(filepath_conllu)\n",
        "    print(f'  {tail}')\n",
        "    missing_contents[filepath_conllu] = []\n",
        "    conll_structures = codecs.open(filepath_conllu, 'r', 'utf-8').read().split('\\n\\n')\n",
        "    # Build regex to check each line of the file\n",
        "    regex_search = createRegexCheck(tail)\n",
        "    for i, conll_structure in enumerate(conll_structures):\n",
        "      lines_conll = conll_structure.split('\\n')\n",
        "      for line_conll in lines_conll:\n",
        "        if not line_conll == '':\n",
        "          if not re.search(regex_search, line_conll):\n",
        "            missing_contents[filepath_conllu].append(i)\n",
        "  return missing_contents\n",
        "\n",
        "missing_folders = []\n",
        "missing_contents_base = {}\n",
        "missing_contents_2T = {}\n",
        "missing_contents_lin = {}\n",
        "missing_contents_2Tlin = {}\n",
        "missing_contents_GoldStruct = {}\n",
        "\n",
        "for dataTrain_folder in sorted(list_dataTrain_folders):\n",
        "  dataTrain_folder_fullPath = os.path.join(mflens_data_folder, dataTrain_folder)\n",
        "  dataTrain_folder_fullPath_2T = re.sub('A', 'T', dataTrain_folder_fullPath)\n",
        "  dataTrain_folder_fullPath_lin = dataTrain_folder_fullPath+'-lin'\n",
        "  dataTrain_folder_fullPath_2Tlin = re.sub('A', 'T', dataTrain_folder_fullPath)+'-lin'\n",
        "  print(dataTrain_folder_fullPath)\n",
        "  if not os.path.exists(dataTrain_folder_fullPath_2T):\n",
        "    missing_folders.append(dataTrain_folder_fullPath_2T)\n",
        "  if not os.path.exists(dataTrain_folder_fullPath_lin):\n",
        "    missing_folders.append(dataTrain_folder_fullPath_lin)\n",
        "  if not os.path.exists(dataTrain_folder_fullPath_2Tlin):\n",
        "    missing_folders.append(dataTrain_folder_fullPath_2Tlin)\n",
        "\n",
        "  # Get dico with errors for each file\n",
        "  checkConllu(glob.glob(os.path.join(dataTrain_folder_fullPath, '*.conllu')), missing_contents_base)\n",
        "  checkConllu(glob.glob(os.path.join(dataTrain_folder_fullPath_2T, '*.conllu')), missing_contents_2T)\n",
        "  checkConllu(glob.glob(os.path.join(dataTrain_folder_fullPath_lin, '*.conllu')), missing_contents_lin)\n",
        "  checkConllu(glob.glob(os.path.join(dataTrain_folder_fullPath_2Tlin, '*.conllu')), missing_contents_2Tlin)\n",
        "\n",
        "print(gold_structuring_folder)\n",
        "checkConllu(glob.glob(os.path.join(gold_structuring_folder, '*.conllu')), missing_contents_GoldStruct)\n",
        "\n",
        "if len(missing_folders) > 0:\n",
        "  print(f'One or more folders missing: {str(missing_folders)}')\n",
        "else:\n",
        "  print('All required folders were found')\n"
      ],
      "metadata": {
        "id": "YCpaz2Wl9TZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print missing\n",
        "print_base = False#@param{type:\"boolean\"}\n",
        "print_Lin = False#@param{type:\"boolean\"}\n",
        "print_2T = False#@param{type:\"boolean\"}\n",
        "print_2TLin = False#@param{type:\"boolean\"}\n",
        "print_gold = True#@param{type:\"boolean\"}\n",
        "\n",
        "if print_base == True:\n",
        "  print('\\nBase\\n==========')\n",
        "  for dataset in sorted(missing_contents_base):\n",
        "    print(dataset)\n",
        "    if len(missing_contents_base[dataset]) > 0:\n",
        "      print(f'  {len(missing_contents_base[dataset])} missing: {missing_contents_base[dataset]}')\n",
        "    else:\n",
        "      print('  OK')\n",
        "\n",
        "if print_Lin == True:\n",
        "  print('\\nLin\\n==========')\n",
        "  for dataset in sorted(missing_contents_lin):\n",
        "    print(dataset)\n",
        "    if len(missing_contents_lin[dataset]) > 0:\n",
        "      print(f'  {len(missing_contents_lin[dataset])} missing: {missing_contents_lin[dataset]}')\n",
        "    else:\n",
        "      print('  OK')\n",
        "\n",
        "if print_2T == True:\n",
        "  print('\\n2T\\n==========')\n",
        "  for dataset in sorted(missing_contents_2T):\n",
        "    print(dataset)\n",
        "    if len(missing_contents_2T[dataset]) > 0:\n",
        "      print(f'  {len(missing_contents_2T[dataset])} missing: {missing_contents_2T[dataset]}')\n",
        "    else:\n",
        "      print('  OK')\n",
        "\n",
        "if print_2TLin == True:\n",
        "  print('\\n2T-Lin\\n==========')\n",
        "  for dataset in sorted(missing_contents_2Tlin):\n",
        "    print(dataset)\n",
        "    if len(missing_contents_2Tlin[dataset]) > 0:\n",
        "      print(f'  {len(missing_contents_2Tlin[dataset])} missing: {missing_contents_2Tlin[dataset]}')\n",
        "    else:\n",
        "      print('  OK')\n",
        "\n",
        "if print_gold == True:\n",
        "  print('\\n2T-Lin\\n==========')\n",
        "  for dataset in sorted(missing_contents_GoldStruct):\n",
        "    print(dataset)\n",
        "    if len(missing_contents_GoldStruct[dataset]) > 0:\n",
        "      print(f'  {len(missing_contents_GoldStruct[dataset])} missing: {missing_contents_GoldStruct[dataset]}')\n",
        "    else:\n",
        "      print('  OK')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kOHM0vETMcq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YEC99tlImVn"
      },
      "source": [
        "# NLLB MT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLY4NKZfY7SW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import codecs\n",
        "\n",
        "txt_folder = '/content/texts'\n",
        "out_folder = '/content/outs'\n",
        "\n",
        "if not os.path.exists(txt_folder):\n",
        "  os.makedirs(txt_folder)\n",
        "\n",
        "if not os.path.exists(out_folder):\n",
        "  os.makedirs(out_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoXA5NicIfu3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import codecs\n",
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-1.3B\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\")\n",
        "\n",
        "\n",
        "# Check if CUDA (GPU acceleration) is available on the system\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-1.3B\").to(device)\n",
        "\n",
        "# article = \"UN Chief says there is no military solution in Syria. This is also a problem for my little cat.\"\n",
        "# inputs = tokenizer(article, return_tensors=\"pt\")\n",
        "\n",
        "# for language in languages:\n",
        "#   translated_tokens = model.generate(\n",
        "#       **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[language], max_length=30\n",
        "  # )\n",
        "  # print(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMJipSHkcnq8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "path_drive_NLLB = '/content/drive/MyDrive/Colab-dump/NLLB'\n",
        "if not os.path.exists(path_drive_NLLB):\n",
        "  os.makedirs(path_drive_NLLB)\n",
        "\n",
        "txt_files_paths = glob.glob(os.path.join(txt_folder, '*.txt'))\n",
        "\n",
        "# languages = ['fra_Latn', 'arb_Arab', 'deu_Latn', 'hin_Deva', 'kor_Hang', 'rus_Cyrl', 'spa_Latn', 'swh_Latn', 'zho_Hans']\n",
        "languages = ['kor_Hang']\n",
        "# languages = ['hin_Deva', 'deu_Latn', 'kor_Hang']\n",
        "language_map = {'arb_Arab':'ar', 'deu_Latn':'de', 'fra_Latn':'fr', 'hin_Deva':'hi', 'kor_Hang':'ko', 'rus_Cyrl':'ru', 'spa_Latn':'es', 'swh_Latn':'sw', 'zho_Hans':'zh'}\n",
        "# languages = ['fra_Latn', 'spa_Latn']\n",
        "\n",
        "def extract_sentences(txt_file_path):\n",
        "  list_texts_split = []\n",
        "  list_texts = codecs.open(txt_file_path, 'r', 'utf-8').readlines()\n",
        "  for i, text in enumerate(list_texts):\n",
        "    sentences = text.strip().split('. ')\n",
        "    list_texts_split.append(sentences)\n",
        "    # if i == 0:\n",
        "    #   print(text.strip())\n",
        "    #   print(sentences)\n",
        "  return list_texts_split\n",
        "\n",
        "for language in languages:\n",
        "  for txt_file_path in txt_files_paths:\n",
        "    # Get filename without extension and language suffix\n",
        "    head, tail = os.path.split(txt_file_path)\n",
        "    filename_noExt = tail.rsplit('.', 1)[0].rsplit('_', 1)[0]\n",
        "    lang_out = language_map.get(language)\n",
        "    print(f'Translating {filename_noExt} to {lang_out}...')\n",
        "\n",
        "    list_texts_nllb = extract_sentences(txt_file_path)\n",
        "    # print(list_texts_nllb[0])\n",
        "\n",
        "    translated_texts = []\n",
        "\n",
        "    # NLLB takes only one sentence at a time as input.\n",
        "    # Split text file into texts, and texts in to sentences.\n",
        "    for j, text_nllb in enumerate(list_texts_nllb):\n",
        "      # if j < 3:\n",
        "      print(f'Text {j}...')\n",
        "      translated_sentences = []\n",
        "      for sentence_nllb in text_nllb:\n",
        "        inputs = tokenizer(sentence_nllb, return_tensors=\"pt\").to(device)\n",
        "        translated_tokens = model.generate(\n",
        "          **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[language], max_length=100\n",
        "        )\n",
        "        translated_sentences.append(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0])\n",
        "      # Combine the sentences of the same text with a dot and space, since that's what we removed above to build\n",
        "      out_text = '. '.join(translated_sentences)\n",
        "      out_text = re.subn('\\.\\.', '.', out_text)[0]\n",
        "      translated_texts.append(out_text)\n",
        "\n",
        "    # Write output file, with one text per line and a linebreak for all but last text\n",
        "    filename_out = os.path.join(path_drive_NLLB, filename_noExt+'_'+lang_out+'.txt')\n",
        "    with codecs.open(filename_out, 'w', 'utf-8') as fo:\n",
        "      for k, translated_text in enumerate(translated_texts):\n",
        "        fo.write(translated_text)\n",
        "        if k < len(translated_texts)-1:\n",
        "          fo.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "114Hmroz2qya"
      },
      "outputs": [],
      "source": [
        "#@title Zip and download conll inputs\n",
        "download_inputs = 'yes'#@param['yes', 'no']\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "if download_inputs == 'yes':\n",
        "  from google.colab import files\n",
        "  zip_name_log = '/content/outs_ar-de-hi-ko.zip'\n",
        "  !zip -r {zip_name_log} {out_folder}\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  files.download(zip_name_log)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8WcHKFA-LBlM",
        "ISYtADpmPITm",
        "fPSx4Z_PjlW9",
        "0YEC99tlImVn"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}