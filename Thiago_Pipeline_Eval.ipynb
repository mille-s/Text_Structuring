{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-Gm2KjRasFvt",
        "4HT9DxnwsSQd",
        "dbubVheWsbQk",
        "bdKjogJaslzH",
        "ZuMAwBPMsuD-",
        "DkRtQCUgs8iy"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/Text_Structuring/blob/main/Thiago_Pipeline_Eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare folders and files"
      ],
      "metadata": {
        "id": "EcMa8qPpREkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare repo (SM)\n",
        "# IPython.display allows clearing the output in a Jupyter notebook cell\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "\n",
        "# Clone original repo\n",
        "! git clone https://github.com/ThiagoCF05/DeepNLG.git\n",
        "\n",
        "# Clone repo for conversion of parser output to format needed for evaluation\n",
        "! git clone 'https://github.com/mille-s/Text_Structuring.git'\n",
        "mflens_data_folder = '/content/Text_Structuring/data'\n",
        "if not os.path.exists(mflens_data_folder):\n",
        "  os.makedirs(mflens_data_folder)\n",
        "\n",
        "# Make new output folder\n",
        "# os.makedirs('out_SM')\n",
        "\n",
        "# Requirements\n",
        "# Some get installed, some don't\n",
        "# ! pip install absl-py==0.7.1\n",
        "# ! pip install astor==0.8.0\n",
        "# ! pip install certifi==2019.3.9\n",
        "# ! pip install chardet==3.0.4\n",
        "# ! pip install Cython==0.29.7\n",
        "# ! pip install dyNET==2.1\n",
        "# ! pip install gast==0.2.2\n",
        "# ! pip install grpcio==1.20.1\n",
        "# ! pip install h5py==2.9.0\n",
        "# ! pip install idna==2.8\n",
        "# ! pip install Keras-Applications==1.0.7\n",
        "# ! pip install Keras-Preprocessing==1.0.9\n",
        "# ! pip install Markdown==3.1.1\n",
        "# ! pip install mock==3.0.5\n",
        "# ! pip install numpy==1.16.3\n",
        "# ! pip install protobuf==3.7.1\n",
        "# ! pip install psutil==5.6.6\n",
        "# ! pip install requests==2.22.0\n",
        "# ! pip install six==1.12.0\n",
        "# ! pip install stanfordcorenlp==3.9.1.1\n",
        "# ! pip install tensorboard==1.13.1\n",
        "# ! pip install tensorflow==2.5.1\n",
        "# ! pip install tensorflow-estimator==1.13.0\n",
        "# ! pip install termcolor==1.1.0\n",
        "# ! pip install urllib3==1.26.5\n",
        "# ! pip install Werkzeug==0.15.4\n",
        "clear_output()\n",
        "print('Repo ready!')\n"
      ],
      "metadata": {
        "id": "TBnxVwo1hMbW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Process scripts (SM)\n",
        "# Adapt import paths to Colab repo\n",
        "import codecs\n",
        "\n",
        "# Define paths\n",
        "path_process_ordering = '/content/DeepNLG/ordering/preprocess.py'\n",
        "path_process_structuring = '/content/DeepNLG/structing/preprocess.py'\n",
        "path_data = '/content/DeepNLG/versions/v1.4/en'\n",
        "path_out_ordering = 'out_SM/ordering'\n",
        "path_out_structuring = 'out_SM/structing'\n",
        "\n",
        "preproc_order_code_lines = codecs.open(path_process_ordering, 'r', 'utf-8').readlines()\n",
        "preproc_structing_code_lines = codecs.open(path_process_structuring, 'r', 'utf-8').readlines()\n",
        "\n",
        "path_process_ordering_Colab = '/content/DeepNLG/ordering/preprocess_Colab.py'\n",
        "path_process_structuring_Colab = '/content/DeepNLG/structing/preprocess_Colab.py'\n",
        "\n",
        "with codecs.open(path_process_ordering_Colab, 'w', 'utf-8') as fo_order:\n",
        "  for line in preproc_order_code_lines:\n",
        "    if line == 'import load\\n':\n",
        "      fo_order.write('from content.DeepNLG import load\\n')\n",
        "    elif line == 'import parsing\\n':\n",
        "      fo_order.write('from content.DeepNLG import parsing\\n')\n",
        "    elif line == 'import utils\\n':\n",
        "      fo_order.write('from content.DeepNLG import utils\\n')\n",
        "    elif line == 'from superpreprocess import Preprocess\\n':\n",
        "      fo_order.write('from content.DeepNLG.superpreprocess import Preprocess\\n')\n",
        "    else:\n",
        "      fo_order.write(line)\n",
        "\n",
        "with codecs.open(path_process_structuring_Colab, 'w', 'utf-8') as fo_struct:\n",
        "  for line in preproc_order_code_lines:\n",
        "    if line == 'import load\\n':\n",
        "      fo_struct.write('from content.DeepNLG import load\\n')\n",
        "    elif line == 'import parsing\\n':\n",
        "      fo_struct.write('from content.DeepNLG import parsing\\n')\n",
        "    elif line == 'import utils\\n':\n",
        "      fo_struct.write('from content.DeepNLG import utils\\n')\n",
        "    elif line == 'from superpreprocess import Preprocess\\n':\n",
        "      fo_struct.write('from content.DeepNLG.superpreprocess import Preprocess\\n')\n",
        "    else:\n",
        "      fo_struct.write(line)"
      ],
      "metadata": {
        "id": "ic0GDYdzX_Go",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make reference file for joint eval of ordering and structuring (SM)\n",
        "\n",
        "path_ordANDstructRefCode = '/content/Text_Structuring/code/orderANDstruct_makeRefFile.py'\n",
        "path_gold_structuring = '/content/DeepNLG/evaluation/data/structing/test.json'\n",
        "\n",
        "! python {path_ordANDstructRefCode} {path_gold_structuring}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Q2mAeVcUAgxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract Thiago's predicted files for joint eval of ordering and structuring (SM)\n",
        "import os\n",
        "\n",
        "path_ordANDstructPredCode = '/content/Text_Structuring/code/orderANDstruct_makePredFiles.py'\n",
        "path_gold_structuring = '/content/DeepNLG/evaluation/data/structing/test.json'\n",
        "path_pred = '/content/DeepNLG/evaluation/results'\n",
        "# path_pred = '/content/Text_Structuring/data'\n",
        "\n",
        "# model_name = 'transformer'#@param['major', 'random', 'rnn', 'transformer']\n",
        "model_names = ['major', 'random', 'rnn', 'transformer']\n",
        "\n",
        "for model_name in model_names:\n",
        "  ! python {path_ordANDstructPredCode} {path_gold_structuring} {path_pred} {model_name}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aRQzi1fVFicF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract ordering and structuring info from FORGe intermediate reps (drag and drop file in /content/Text_Structuring/data) (SM)\n",
        "\n",
        "path_forge2castro_code = '/content/Text_Structuring/code/forge2castro.py'\n",
        "forge_out_file = 'none' #@param['11-SMorphText.conllu', 'none']\n",
        "\n",
        "if not forge_out_file == 'none':\n",
        "  ! python {path_forge2castro_code} {mflens_data_folder} {forge_out_file}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EfP5d3VI2Ndo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add new outputs (SM, drag and drop files [task]_[system_name]-test.out.postprocessed in /content/Text_Structuring/data)\n",
        "import os\n",
        "import glob\n",
        "import sys\n",
        "\n",
        "# I need 3 files for the evaluation (example with 2AO files)\n",
        "# - ordering_2AO-test.out.postprocessed (for evaluation of ordering only)\n",
        "# - structuring_2AO-lin-test.out.postprocessed (for evaluation of structuring only)\n",
        "# - structuring_2AO-test.out.postprocessed ( for evaluation of ordering AND structuring together)\n",
        "\n",
        "files_in = mflens_data_folder\n",
        "\n",
        "def create_folder(folder_path):\n",
        "  if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "files_in_paths = glob.glob(os.path.join(files_in, '*.postprocessed'))\n",
        "for file_in_path in files_in_paths:\n",
        "  head, tail = os.path.split(file_in_path)\n",
        "  task = tail.split('_', 1)[0]\n",
        "  if not (task == 'ordering' or task == 'structuring'):\n",
        "    sys.exit(f'Task in input file name not recognised!')\n",
        "  new_file_name = tail.rsplit('-', 1)[1]\n",
        "  if not new_file_name == 'test.out.postprocessed':\n",
        "    sys.exit(f'The file name should end with -test.out.postprocessed!')\n",
        "  system_name = tail.split('_', 1)[1].rsplit('-', 1)[0]\n",
        "  system = ''\n",
        "  if system_name.startswith('2') or system_name.startswith('3'):\n",
        "    system = 'mflens'\n",
        "  else:\n",
        "    system = system_name\n",
        "  orderingFolder = os.path.join(('/content/DeepNLG/evaluation/results/steps/ordering'), system)\n",
        "  create_folder(orderingFolder)\n",
        "  srtructuringFolder = os.path.join(('/content/DeepNLG/evaluation/results/steps/structing'), system)\n",
        "  create_folder(srtructuringFolder)\n",
        "  orderingANDsrtructuringFolder = os.path.join(('/content/DeepNLG/evaluation/results/steps/orderingANDstructing'), system)\n",
        "  create_folder(orderingANDsrtructuringFolder)\n",
        "\n",
        "  # to copy a file to a different location with the same name cp file new/location/file\n",
        "  if task == 'ordering':\n",
        "    dest_path = os.path.join(orderingFolder, new_file_name)\n",
        "    ! cp {file_in_path} {dest_path}\n",
        "    print(f'Copied file {dest_path} from {file_in_path}')\n",
        "  elif task == 'structuring':\n",
        "    if tail.split('-')[1] == 'lin':\n",
        "      dest_path = os.path.join(srtructuringFolder, new_file_name)\n",
        "      ! cp {file_in_path} {dest_path}\n",
        "      print(f'Copied file {dest_path} from {file_in_path}')\n",
        "    else:\n",
        "      dest_path = os.path.join(orderingANDsrtructuringFolder, new_file_name)\n",
        "      ! cp {file_in_path} {dest_path}\n",
        "      print(f'Copied file {dest_path} from {file_in_path}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2fA9ACKkWNcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract reference data (SM, not used)\n",
        "# Running this code, I don't obtain exactly the same data as in the repo; at least the order of the datapoints is different (haven't checked the contents; filesize is the same).\n",
        "# ! python {path_process_ordering_Colab} {path_data} {path_out_ordering}\n",
        "# ! python {path_process_structuring_Colab} {path_data} {path_out_structuring}"
      ],
      "metadata": {
        "id": "w0y8MvewQhnX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of Ordering and Structuring"
      ],
      "metadata": {
        "id": "4nn5FLtDuFOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set folder path and unseen domains (TCF)\n",
        "import json\n",
        "import os\n",
        "import nltk\n",
        "import numpy as np\n",
        "import subprocess\n",
        "\n",
        "unseen_domains = ['Artist', 'Politician', 'CelestialBody', 'Athlete', 'MeanOfTransportation']\n",
        "# root path\n",
        "path='/content/DeepNLG/evaluation/'"
      ],
      "metadata": {
        "id": "sAlqti5or4Kp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose datasets, tasks and models to evaluate (SM, add new models here)\n",
        "\n",
        "def addToList(list_to_fill, param, value):\n",
        "  if param == True:\n",
        "    list_to_fill.append(value)\n",
        "\n",
        "# dev is not supported for some tasks\n",
        "data_list = ['test']#['dev', 'test']\n",
        "# data_dev = False#@param{type:\"boolean\"}\n",
        "# addToList(data_list, data_dev, 'dev')\n",
        "# data_test = True#@param{type:\"boolean\"}\n",
        "# addToList(data_list, data_test, 'test')\n",
        "\n",
        "task_list = []#['ordering', 'structing', 'orderingANDstructing']\n",
        "task_ordering = True#@param{type:\"boolean\"}\n",
        "addToList(task_list, task_ordering, 'ordering')\n",
        "task_structuring = True#@param{type:\"boolean\"}\n",
        "addToList(task_list, task_structuring, 'structing')\n",
        "task_orderingANDstructuring = False#@param{type:\"boolean\"}\n",
        "addToList(task_list, task_orderingANDstructuring, 'orderingANDstructing')\n",
        "\n",
        "model_list = []#['major', 'rnn', 'transformer', 'random', 'mflens', 'forge']\n",
        "model_major = True#@param{type:\"boolean\"}\n",
        "addToList(model_list, model_major, 'major')\n",
        "model_rnn = True#@param{type:\"boolean\"}\n",
        "addToList(model_list, model_rnn, 'rnn')\n",
        "model_transformer = True#@param{type:\"boolean\"}\n",
        "addToList(model_list, model_transformer, 'transformer')\n",
        "model_random = True#@param{type:\"boolean\"}\n",
        "addToList(model_list, model_random, 'random')\n",
        "model_mflens = False#@param{type:\"boolean\"}\n",
        "addToList(model_list, model_mflens, 'mflens')\n",
        "model_forge = False#@param{type:\"boolean\"}\n",
        "addToList(model_list, model_forge, 'forge')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2kB4mdaRlHbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title All domains (TCF)\n",
        "\n",
        "print('All domains:\\n-----------')\n",
        "# model_thiag = 'mflens'#@param['major', 'random', 'transformer', 'rnn', 'mflens' ]\n",
        "print_inOut_pairs = False#@param {type:\"boolean\"}\n",
        "\n",
        "# SM: replaced dataet, task and model lists by variables that we fill in teh cell above\n",
        "for _set in data_list:\n",
        "\n",
        "    for task in task_list:\n",
        "        gold_path=os.path.join(path, 'data', task, _set + '.json')\n",
        "        gold = json.load(open(gold_path))\n",
        "        print('\\n')\n",
        "\n",
        "        for model in model_list:\n",
        "            # SM: added condition (FORGe cannot easily do only structuring, so skipped for now)\n",
        "            if task == 'structing' and model == 'forge':\n",
        "              pass\n",
        "            else:\n",
        "              p = os.path.join(path, 'results/steps', task, model, _set + '.out.postprocessed')\n",
        "              with open(p) as f:\n",
        "                  y_pred_ = f.read().split('\\n')[:-1]\n",
        "              # y_pred: runwayLength 3rd_runway_SurfaceType\n",
        "              # y_real: ['3rd_runway_SurfaceType runwayLength', 'runwayLength 3rd_runway_SurfaceType']\n",
        "              y_real, y_pred = [], []\n",
        "              for i, g in enumerate(gold):\n",
        "                  t = [' '.join(target['output']) for target in g['targets']]\n",
        "                  y_real.append(t)\n",
        "                  y_pred.append(y_pred_[i].strip())\n",
        "\n",
        "              num, dem = 0.0, 0\n",
        "              for i, y_ in enumerate(y_pred):\n",
        "                  y = y_real[i]\n",
        "                  # count 1 each time a prediction for input X is found in the list of references for X\n",
        "                  if y_.strip() in y:\n",
        "                      num += 1\n",
        "                      # SM: added print\n",
        "                      if print_inOut_pairs == True:\n",
        "                          print(f'#{i}[+]: {y_.strip()} - {y}')\n",
        "                  # SM: added else\n",
        "                  else:\n",
        "                      if print_inOut_pairs == True:\n",
        "                          print(f'#{i}[-]: {y_.strip()} - {y}')\n",
        "                  # dem = total count of predictions made\n",
        "                  dem += 1\n",
        "              print('Task: ', task)\n",
        "              print('Set: ', _set)\n",
        "              print('Model: ', model)\n",
        "              # for c, (pred, ref) in enumerate(zip(y_pred, y_real)):\n",
        "              #   print(f'#{c}: {y_pred[c]} - {y_real[c]}')\n",
        "              # print(f'References ({len(y_real)}):')\n",
        "              # print(y_real)\n",
        "              # print(f'Predictions ({len(y_pred)}):')\n",
        "              # print(y_pred)\n",
        "              print('Accuracy: ', round(num/dem, 2))\n",
        "              print(10 * '-')"
      ],
      "metadata": {
        "id": "jle6XRQVr-lJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Seen domains (TCF)\n",
        "print('Seen domains:\\n-----------')\n",
        "\n",
        "# SM: replaced dataet, task and model lists by variables that we fill in teh cell above\n",
        "for _set in data_list:\n",
        "\n",
        "    for task in task_list:\n",
        "        gold_path=os.path.join(path, 'data', task, _set + '.json')\n",
        "        gold = json.load(open(gold_path))\n",
        "        print('\\n')\n",
        "\n",
        "        for model in model_list:\n",
        "            p = os.path.join(path, 'results/steps', task, model, _set + '.out.postprocessed')\n",
        "            with open(p) as f:\n",
        "                y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "            y_real, y_pred = [], []\n",
        "            for i, g in enumerate(gold):\n",
        "                if g['category'] not in unseen_domains:\n",
        "                    t = [' '.join(target['output']) for target in g['targets']]\n",
        "                    y_real.append(t)\n",
        "                    y_pred.append(y_pred_[i].strip())\n",
        "\n",
        "            num, dem = 0.0, 0\n",
        "            for i, y_ in enumerate(y_pred):\n",
        "                y = y_real[i]\n",
        "                if y_.strip() in y:\n",
        "                    num += 1\n",
        "                dem += 1\n",
        "            print('Task: ', task)\n",
        "            print('Set: ', _set)\n",
        "            print('Model: ', model)\n",
        "            print('Accuracy: ', round(num/dem, 2))\n",
        "            print(10 * '-')\n"
      ],
      "metadata": {
        "id": "874uiV6tsBvV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unseen domains (TCF)\n",
        "print('Unseen domains:\\n-----------')\n",
        "\n",
        "# SM: replaced dataet, task and model lists by variables that we fill in teh cell above\n",
        "for _set in data_list:\n",
        "\n",
        "    for task in task_list:\n",
        "        gold_path=os.path.join(path, 'data', task, _set + '.json')\n",
        "        gold = json.load(open(gold_path))\n",
        "        print('\\n')\n",
        "\n",
        "        for model in model_list:\n",
        "            p = os.path.join(path, 'results/steps', task, model, _set + '.out.postprocessed')\n",
        "            with open(p) as f:\n",
        "                y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "            y_real, y_pred = [], []\n",
        "            for i, g in enumerate(gold):\n",
        "                if g['category'] in unseen_domains:\n",
        "                    t = [' '.join(target['output']) for target in g['targets']]\n",
        "                    y_real.append(t)\n",
        "\n",
        "                    y_pred.append(y_pred_[i].strip())\n",
        "\n",
        "            num, dem = 0.0, 0\n",
        "            for i, y_ in enumerate(y_pred):\n",
        "                y = y_real[i]\n",
        "                if y_.strip() in y:\n",
        "                    num += 1\n",
        "                dem += 1\n",
        "            print('Task: ', task)\n",
        "            print('Set: ', _set)\n",
        "            print('Model: ', model)\n",
        "            print('Accuracy: ', round(num/dem, 2) if dem > 0 else 0)\n",
        "            print(10 * '-')"
      ],
      "metadata": {
        "id": "sdRT1WN9sDz-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the rest of the pipeline (not tested on Colab)"
      ],
      "metadata": {
        "id": "lj9Evq44nxT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Referring Expressions"
      ],
      "metadata": {
        "id": "-Gm2KjRasFvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('All domains:')\n",
        "for _set in ['dev', 'test']:\n",
        "    gold_path=os.path.join(path, 'data', 'reg', _set + '.json')\n",
        "    gold = json.load(open(gold_path))\n",
        "\n",
        "    p = os.path.join(path, 'results/steps/reg', _set + '.out.postprocessed')\n",
        "    with open(p) as f:\n",
        "        y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "    y_real, y_pred, y_baseline = [], [], []\n",
        "    for i, g in enumerate(gold):\n",
        "        y_real.append(' '.join(g['refex']).strip().lower())\n",
        "        y_pred.append(y_pred_[i].strip().lower())\n",
        "\n",
        "    num, dem = 0.0, 0\n",
        "    baseline = 0\n",
        "    for i, y_ in enumerate(y_pred):\n",
        "        refex = ' '.join(nltk.word_tokenize(gold[i]['entity'].replace('\\'', ' ').replace('\\\"', ' ').replace('_', ' ')))\n",
        "        y = y_real[i]\n",
        "        if y_.strip() == y:\n",
        "            num += 1\n",
        "        if refex.strip().lower() == y:\n",
        "            baseline += 1\n",
        "        dem += 1\n",
        "    print('REG Task:')\n",
        "    print('Set: ', _set)\n",
        "    print('Baseline Accuracy: ', round(baseline/dem, 2) if dem > 0 else 0)\n",
        "    print('NeuralREG Accuracy: ', round(num/dem, 2) if dem > 0 else 0)\n",
        "    print(10 * '-')"
      ],
      "metadata": {
        "id": "AIxvCLpssGl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Seen domains:')\n",
        "for _set in ['dev', 'test']:\n",
        "    gold_path=os.path.join(path, 'data', 'reg', _set + '.json')\n",
        "    gold = json.load(open(gold_path))\n",
        "\n",
        "    p = os.path.join(path, 'results/steps/reg', _set + '.out.postprocessed')\n",
        "    with open(p) as f:\n",
        "        y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "    y_real, y_pred, y_baseline = [], [], []\n",
        "    for i, g in enumerate(gold):\n",
        "    #     if g['category'] in unseen_domains:\n",
        "        y_real.append(' '.join(g['refex']).strip().lower())\n",
        "        y_pred.append(y_pred_[i].strip().lower())\n",
        "\n",
        "    num, dem = 0.0, 0\n",
        "    baseline = 0\n",
        "    for i, y_ in enumerate(y_pred):\n",
        "        if gold[i]['category'] not in unseen_domains:\n",
        "            refex = ' '.join(nltk.word_tokenize(gold[i]['entity'].replace('\\'', ' ').replace('\\\"', ' ').replace('_', ' ')))\n",
        "            y = y_real[i]\n",
        "            if y_.strip() == y:\n",
        "                num += 1\n",
        "            if refex.strip().lower() == y:\n",
        "                baseline += 1\n",
        "            dem += 1\n",
        "    print('REG Task:')\n",
        "    print('Set: ', _set)\n",
        "    print('Baseline Accuracy: ', round(baseline/dem, 2) if dem > 0 else 0)\n",
        "    print('NeuralREG Accuracy: ', round(num/dem, 2) if dem > 0 else 0)\n",
        "    print(10 * '-')"
      ],
      "metadata": {
        "id": "1gWMlLvmsOBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Unseen domains:')\n",
        "for _set in ['dev', 'test']:\n",
        "    gold_path=os.path.join(path, 'data', 'reg', _set + '.json')\n",
        "    gold = json.load(open(gold_path))\n",
        "\n",
        "    p = os.path.join(path, 'results/steps/reg', _set + '.out.postprocessed')\n",
        "    with open(p) as f:\n",
        "        y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "    y_real, y_pred, y_baseline = [], [], []\n",
        "    for i, g in enumerate(gold):\n",
        "        y_real.append(' '.join(g['refex']).strip().lower())\n",
        "        y_pred.append(y_pred_[i].strip().lower())\n",
        "\n",
        "    num, dem = 0.0, 0\n",
        "    baseline = 0\n",
        "    for i, y_ in enumerate(y_pred):\n",
        "        if gold[i]['category'] in unseen_domains:\n",
        "            refex = ' '.join(nltk.word_tokenize(gold[i]['entity'].replace('\\'', ' ').replace('\\\"', ' ').replace('_', ' ')))\n",
        "            y = y_real[i]\n",
        "            if y_.strip() == y:\n",
        "                num += 1\n",
        "            if refex.strip().lower() == y:\n",
        "                baseline += 1\n",
        "            dem += 1\n",
        "    print('REG Task:')\n",
        "    print('Set: ', _set)\n",
        "    print('Baseline Accuracy: ', round(baseline/dem, 2) if dem > 0 else 0)\n",
        "    print('NeuralREG Accuracy: ', round(num/dem, 2) if dem > 0 else 0)\n",
        "    print(10 * '-')"
      ],
      "metadata": {
        "id": "Zxsm4yAUsQS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Lexicalization"
      ],
      "metadata": {
        "id": "4HT9DxnwsSQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('All domains:')\n",
        "for _set in ['dev', 'test']:\n",
        "    for model in ['random', 'major', 'transformer', 'rnn']:\n",
        "        gold_path=os.path.join(path, 'data', 'lexicalization', _set + '.json')\n",
        "        gold = json.load(open(gold_path))\n",
        "\n",
        "        p = os.path.join(path, 'results/steps/lexicalization', model, _set + '.out.postprocessed')\n",
        "        with open(p) as f:\n",
        "            y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "        y_real, y_pred = [], []\n",
        "        for i, g in enumerate(gold):\n",
        "        #     if g['category'] in unseen_domains:\n",
        "            t = [' '.join(target['output']).lower() for target in g['targets']]\n",
        "            y_real.append(t)\n",
        "            y_pred.append(y_pred_[i].strip().lower())\n",
        "\n",
        "        with open('predictions', 'w') as f:\n",
        "            f.write('\\n'.join(y_pred))\n",
        "\n",
        "        nfiles = max([len(refs) for refs in y_real])\n",
        "        for i in range(nfiles):\n",
        "            with open('reference' + str(i+1), 'w') as f:\n",
        "                for refs in y_real:\n",
        "                    if i < len(refs):\n",
        "                        f.write(refs[i])\n",
        "                    f.write('\\n')\n",
        "\n",
        "        nematus = '/roaming/tcastrof/workspace/nematus/data/multi-bleu.perl'\n",
        "        command = 'perl ' + nematus + ' reference1 reference2 reference3 reference4 reference5 reference6 reference7 reference8 < predictions'\n",
        "        result = subprocess.check_output(command, shell=True)\n",
        "        print('Lexicalization: ')\n",
        "        print('Set: ', _set)\n",
        "        print('Model: ', model)\n",
        "        print(result)\n",
        "        print(10 * '-')\n",
        "\n",
        "        os.remove('reference1')\n",
        "        os.remove('reference2')\n",
        "        os.remove('reference3')\n",
        "        os.remove('reference4')\n",
        "        os.remove('reference5')\n",
        "        os.remove('reference6')\n",
        "        os.remove('reference7')\n",
        "        os.remove('reference8')\n",
        "        os.remove('predictions')\n"
      ],
      "metadata": {
        "id": "Ke7onNxusUsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Seen domains:')\n",
        "for _set in ['dev', 'test']:\n",
        "    for model in ['random', 'major', 'transformer', 'rnn']:\n",
        "        gold_path=os.path.join(path, 'data', 'lexicalization', _set + '.json')\n",
        "        gold = json.load(open(gold_path))\n",
        "\n",
        "        p = os.path.join(path, 'results/steps/lexicalization', model, _set + '.out.postprocessed')\n",
        "        with open(p) as f:\n",
        "            y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "        y_real, y_pred = [], []\n",
        "        for i, g in enumerate(gold):\n",
        "            if g['category'] not in unseen_domains:\n",
        "                t = [' '.join(target['output']).lower() for target in g['targets']]\n",
        "                y_real.append(t)\n",
        "                y_pred.append(y_pred_[i].strip().lower())\n",
        "\n",
        "        with open('predictions', 'w') as f:\n",
        "            f.write('\\n'.join(y_pred))\n",
        "\n",
        "        nfiles = max([len(refs) for refs in y_real])\n",
        "        for i in range(nfiles):\n",
        "            with open('reference' + str(i+1), 'w') as f:\n",
        "                for refs in y_real:\n",
        "                    if i < len(refs):\n",
        "                        f.write(refs[i])\n",
        "                    f.write('\\n')\n",
        "\n",
        "        nematus = '/roaming/tcastrof/workspace/nematus/data/multi-bleu.perl'\n",
        "        command = 'perl ' + nematus + ' reference1 reference2 reference3 reference4 reference5 reference6 reference7 reference8 < predictions'\n",
        "        result = subprocess.check_output(command, shell=True)\n",
        "        print('Lexicalization: ')\n",
        "        print('Set: ', _set)\n",
        "        print('Model: ', model)\n",
        "        print(result)\n",
        "        print(10 * '-')\n",
        "\n",
        "        os.remove('predictions')\n",
        "        os.remove('reference1')\n",
        "        os.remove('reference2')\n",
        "        os.remove('reference3')\n",
        "        os.remove('reference4')\n",
        "        os.remove('reference5')\n",
        "        os.remove('reference6')\n",
        "        os.remove('reference7')\n",
        "        os.remove('reference8')"
      ],
      "metadata": {
        "id": "Q-F51_iJsW0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Unseen domains:')\n",
        "for _set in ['dev', 'test']:\n",
        "    for model in ['random', 'major', 'transformer', 'rnn']:\n",
        "        gold_path=os.path.join(path, 'data', 'lexicalization', _set + '.json')\n",
        "        gold = json.load(open(gold_path))\n",
        "\n",
        "        p = os.path.join(path, 'results/steps/lexicalization', model, _set + '.out.postprocessed')\n",
        "        with open(p) as f:\n",
        "            y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "        y_real, y_pred = [], []\n",
        "        for i, g in enumerate(gold):\n",
        "            if g['category'] in unseen_domains:\n",
        "                t = [' '.join(target['output']).lower() for target in g['targets']]\n",
        "                y_real.append(t)\n",
        "                y_pred.append(y_pred_[i].strip().lower())\n",
        "\n",
        "        with open('predictions', 'w') as f:\n",
        "            f.write('\\n'.join(y_pred))\n",
        "\n",
        "        try:\n",
        "            nfiles = max([len(refs) for refs in y_real])\n",
        "            for i in range(nfiles):\n",
        "                with open('reference' + str(i+1), 'w') as f:\n",
        "                    for refs in y_real:\n",
        "                        if i < len(refs):\n",
        "                            f.write(refs[i])\n",
        "                        f.write('\\n')\n",
        "\n",
        "            nematus = '/roaming/tcastrof/workspace/nematus/data/multi-bleu.perl'\n",
        "            command = 'perl ' + nematus + ' reference1 reference2 reference3 reference4 reference5 reference6 reference7 reference8 < predictions'\n",
        "            result = subprocess.check_output(command, shell=True)\n",
        "            print('Lexicalization: ')\n",
        "            print('Set: ', _set)\n",
        "            print('Model: ', model)\n",
        "            print(result)\n",
        "            print(10 * '-')\n",
        "\n",
        "            os.remove('predictions')\n",
        "            os.remove('reference1')\n",
        "            os.remove('reference2')\n",
        "            os.remove('reference3')\n",
        "            os.remove('reference4')\n",
        "            os.remove('reference5')\n",
        "            os.remove('reference6')\n",
        "            os.remove('reference7')\n",
        "            os.remove('reference8')\n",
        "        except:\n",
        "            pass\n"
      ],
      "metadata": {
        "id": "MgDPoMjzsZdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Final Texts (BLEU)"
      ],
      "metadata": {
        "id": "dbubVheWsbQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('All domains:')\n",
        "for _set in ['dev', 'test']:\n",
        "    gold_path=os.path.join(path, 'data', 'end2end', _set + '.json')\n",
        "    gold = json.load(open(gold_path))\n",
        "    for kind in ['pipeline', 'end2end']:\n",
        "        for model in ['rand', 'major', 'transformer', 'rnn']:\n",
        "            if kind == 'end2end' and model in ['rand', 'major']:\n",
        "                continue\n",
        "            else:\n",
        "                p = os.path.join(path, 'results', kind, model, _set + '.out.postprocessed')\n",
        "                with open(p) as f:\n",
        "                    y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "                y_real, y_pred = [], []\n",
        "                for i, g in enumerate(gold):\n",
        "                #     if g['category'] in unseen_domains:\n",
        "                    targets = [nltk.word_tokenize(' '.join(target['output'])) for target in g['targets']]\n",
        "                    t = [' '.join(target).lower() for target in targets]\n",
        "                    y_real.append(t)\n",
        "                    pred = ' '.join(nltk.word_tokenize(y_pred_[i])).lower()\n",
        "                    y_pred.append(pred)\n",
        "\n",
        "                with open('predictions', 'w') as f:\n",
        "                    f.write('\\n'.join(y_pred))\n",
        "\n",
        "                nfiles = max([len(refs) for refs in y_real])\n",
        "                for i in range(nfiles):\n",
        "                    with open('reference' + str(i+1), 'w') as f:\n",
        "                        for refs in y_real:\n",
        "                            if i < len(refs):\n",
        "                                f.write(refs[i])\n",
        "                            f.write('\\n')\n",
        "\n",
        "                nematus = '/roaming/tcastrof/workspace/nematus/data/multi-bleu.perl'\n",
        "                command = 'perl ' + nematus + ' reference1 reference2 reference3 reference4 reference5 reference6 reference7 reference8 < predictions'\n",
        "                result = subprocess.check_output(command, shell=True)\n",
        "                print('Final: ')\n",
        "                print('Set: ', _set)\n",
        "                print('Approach:', kind)\n",
        "                print('Model: ', model)\n",
        "                print(result)\n",
        "                print(10 * '-')\n",
        "\n",
        "                os.remove('reference1')\n",
        "                os.remove('reference2')\n",
        "                os.remove('reference3')\n",
        "                os.remove('reference4')\n",
        "                os.remove('reference5')\n",
        "                os.remove('reference6')\n",
        "                os.remove('reference7')\n",
        "                os.remove('reference8')\n",
        "                os.remove('predictions')"
      ],
      "metadata": {
        "id": "I_daMeAtsdwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Seen domains:')\n",
        "for _set in ['dev', 'test']:\n",
        "    gold_path=os.path.join(path, 'data', 'end2end', _set + '.json')\n",
        "    gold = json.load(open(gold_path))\n",
        "    for kind in ['pipeline', 'end2end']:\n",
        "        for model in ['rand', 'major', 'transformer', 'rnn']:\n",
        "            if kind == 'end2end' and model in ['rand', 'major']:\n",
        "                continue\n",
        "            else:\n",
        "                p = os.path.join(path, 'results', kind, model, _set + '.out.postprocessed')\n",
        "                with open(p) as f:\n",
        "                    y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "                y_real, y_pred = [], []\n",
        "                for i, g in enumerate(gold):\n",
        "                    if g['category'] not in unseen_domains:\n",
        "                        targets = [nltk.word_tokenize(' '.join(target['output'])) for target in g['targets']]\n",
        "                        t = [' '.join(target).lower() for target in targets]\n",
        "                        y_real.append(t)\n",
        "                        pred = ' '.join(nltk.word_tokenize(y_pred_[i])).lower()\n",
        "                        y_pred.append(pred)\n",
        "\n",
        "                with open('predictions', 'w') as f:\n",
        "                    f.write('\\n'.join(y_pred))\n",
        "\n",
        "                nfiles = max([len(refs) for refs in y_real])\n",
        "                for i in range(nfiles):\n",
        "                    with open('reference' + str(i+1), 'w') as f:\n",
        "                        for refs in y_real:\n",
        "                            if i < len(refs):\n",
        "                                f.write(refs[i])\n",
        "                            f.write('\\n')\n",
        "\n",
        "                nematus = '/roaming/tcastrof/workspace/nematus/data/multi-bleu.perl'\n",
        "                command = 'perl ' + nematus + ' reference1 reference2 reference3 reference4 reference5 reference6 reference7 reference8 < predictions'\n",
        "                result = subprocess.check_output(command, shell=True)\n",
        "                print('Final: ')\n",
        "                print('Set: ', _set)\n",
        "                print('Approach:', kind)\n",
        "                print('Model: ', model)\n",
        "                print(result)\n",
        "                print(10 * '-')\n",
        "\n",
        "                os.remove('reference1')\n",
        "                os.remove('reference2')\n",
        "                os.remove('reference3')\n",
        "                os.remove('reference4')\n",
        "                os.remove('reference5')\n",
        "                os.remove('reference6')\n",
        "                os.remove('reference7')\n",
        "                os.remove('reference8')\n",
        "                os.remove('predictions')"
      ],
      "metadata": {
        "id": "wY4huBufsgHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Unseen domains:')\n",
        "for _set in ['test']:\n",
        "    gold_path=os.path.join(path, 'data', 'end2end', _set + '.json')\n",
        "    gold = json.load(open(gold_path))\n",
        "    for kind in ['pipeline', 'end2end']:\n",
        "        for model in ['rand', 'major', 'transformer', 'rnn']:\n",
        "            if kind == 'end2end' and model in ['rand', 'major']:\n",
        "                continue\n",
        "            else:\n",
        "                p = os.path.join(path, 'results', kind, model, _set + '.out.postprocessed')\n",
        "                with open(p) as f:\n",
        "                    y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "                y_real, y_pred = [], []\n",
        "                for i, g in enumerate(gold):\n",
        "                    if g['category'] in unseen_domains:\n",
        "                        targets = [nltk.word_tokenize(' '.join(target['output'])) for target in g['targets']]\n",
        "                        t = [' '.join(target).lower() for target in targets]\n",
        "                        y_real.append(t)\n",
        "                        pred = ' '.join(nltk.word_tokenize(y_pred_[i])).lower()\n",
        "                        y_pred.append(pred)\n",
        "\n",
        "                with open('predictions', 'w') as f:\n",
        "                    f.write('\\n'.join(y_pred))\n",
        "\n",
        "                nfiles = max([len(refs) for refs in y_real])\n",
        "                for i in range(nfiles):\n",
        "                    with open('reference' + str(i+1), 'w') as f:\n",
        "                        for refs in y_real:\n",
        "                            if i < len(refs):\n",
        "                                f.write(refs[i])\n",
        "                            f.write('\\n')\n",
        "\n",
        "                nematus = '/roaming/tcastrof/workspace/nematus/data/multi-bleu.perl'\n",
        "                command = 'perl ' + nematus + ' reference1 reference2 reference3 reference4 reference5 reference6 reference7 reference8 < predictions'\n",
        "                result = subprocess.check_output(command, shell=True)\n",
        "                print('Final: ')\n",
        "                print('Set: ', _set)\n",
        "                print('Approach:', kind)\n",
        "                print('Model: ', model)\n",
        "                print(result)\n",
        "                print(10 * '-')\n",
        "\n",
        "                try:\n",
        "                    os.remove('predictions')\n",
        "                    os.remove('reference1')\n",
        "                    os.remove('reference2')\n",
        "                    os.remove('reference3')\n",
        "                    os.remove('reference4')\n",
        "                    os.remove('reference5')\n",
        "                    os.remove('reference6')\n",
        "                    os.remove('reference7')\n",
        "                    os.remove('reference8')\n",
        "                except:\n",
        "                    pass"
      ],
      "metadata": {
        "id": "9W17trp8sic0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Final Texts (METEOR)"
      ],
      "metadata": {
        "id": "bdKjogJaslzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('All domains:')\n",
        "for _set in ['dev', 'test']:\n",
        "    gold_path=os.path.join(path, 'data', 'end2end', _set + '.json')\n",
        "    gold = json.load(open(gold_path))\n",
        "    for kind in ['pipeline', 'end2end']:\n",
        "        for model in ['rand', 'major', 'transformer', 'rnn']:\n",
        "            if kind == 'end2end' and model in ['rand', 'major']:\n",
        "                continue\n",
        "            else:\n",
        "                p = os.path.join(path, 'results', kind, model, _set + '.out.postprocessed')\n",
        "                with open(p) as f:\n",
        "                    y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "                y_real, y_pred = [], []\n",
        "                for i, g in enumerate(gold):\n",
        "#                     if g['category'] in unseen_domains:\n",
        "                    targets = [nltk.word_tokenize(' '.join(target['output'])) for target in g['targets']]\n",
        "                    t = [' '.join(target).lower() for target in targets]\n",
        "                    y_real.append(t)\n",
        "                    pred = ' '.join(nltk.word_tokenize(y_pred_[i])).lower()\n",
        "                    y_pred.append(pred)\n",
        "\n",
        "\n",
        "                with open('predictions', 'w') as f:\n",
        "                    f.write('\\n'.join(y_pred))\n",
        "\n",
        "                with open('reference', 'w') as f:\n",
        "                    for refs in y_real:\n",
        "                        for i in range(8):\n",
        "                            if i < len(refs):\n",
        "                                f.write(refs[i])\n",
        "                            else:\n",
        "                                f.write('')\n",
        "                            f.write('\\n')\n",
        "\n",
        "                java = '/roaming/tcastrof/workspace/java/jre1.8.0_181/bin/java -Xmx2G -jar '\n",
        "                java += '/home/tcastrof/workspace/meteor-1.5/meteor-1.5.jar predictions reference -l en -norm -r 8'\n",
        "                result = subprocess.check_output(java, shell=True)\n",
        "                print('Final: ')\n",
        "                print('Set: ', _set)\n",
        "                print('Approach:', kind)\n",
        "                print('Model: ', model)\n",
        "                print(result.split(b'\\n')[-2])\n",
        "                print(10 * '-')\n",
        "\n",
        "                os.remove('reference')\n",
        "                os.remove('predictions')\n"
      ],
      "metadata": {
        "id": "wkwKYIJqsjye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Seen domains:')\n",
        "for _set in ['dev', 'test']:\n",
        "    gold_path=os.path.join(path, 'data', 'end2end', _set + '.json')\n",
        "    gold = json.load(open(gold_path))\n",
        "    for kind in ['pipeline', 'end2end']:\n",
        "        for model in ['rand', 'major', 'transformer', 'rnn']:\n",
        "            if kind == 'end2end' and model in ['rand', 'major']:\n",
        "                continue\n",
        "            else:\n",
        "                p = os.path.join(path, 'results', kind, model, _set + '.out.postprocessed')\n",
        "                with open(p) as f:\n",
        "                    y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "                y_real, y_pred = [], []\n",
        "                for i, g in enumerate(gold):\n",
        "                    if g['category'] not in unseen_domains:\n",
        "                        targets = [nltk.word_tokenize(' '.join(target['output'])) for target in g['targets']]\n",
        "                        t = [' '.join(target).lower() for target in targets]\n",
        "                        y_real.append(t)\n",
        "                        pred = ' '.join(nltk.word_tokenize(y_pred_[i])).lower()\n",
        "                        y_pred.append(pred)\n",
        "\n",
        "\n",
        "                with open('predictions', 'w') as f:\n",
        "                    f.write('\\n'.join(y_pred))\n",
        "\n",
        "                with open('reference', 'w') as f:\n",
        "                    for refs in y_real:\n",
        "                        for i in range(8):\n",
        "                            if i < len(refs):\n",
        "                                f.write(refs[i])\n",
        "                            else:\n",
        "                                f.write('')\n",
        "                            f.write('\\n')\n",
        "\n",
        "                java = '/roaming/tcastrof/workspace/java/jre1.8.0_181/bin/java -Xmx2G -jar '\n",
        "                java += '/home/tcastrof/workspace/meteor-1.5/meteor-1.5.jar predictions reference -l en -norm -r 8'\n",
        "                result = subprocess.check_output(java, shell=True)\n",
        "                print('Final: ')\n",
        "                print('Set: ', _set)\n",
        "                print('Approach:', kind)\n",
        "                print('Model: ', model)\n",
        "                print(result.split(b'\\n')[-2])\n",
        "                print(10 * '-')\n",
        "\n",
        "                os.remove('reference')\n",
        "                os.remove('predictions')"
      ],
      "metadata": {
        "id": "s81SdyWysqNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Unseen domains:')\n",
        "for _set in ['test']:\n",
        "    gold_path=os.path.join(path, 'data', 'end2end', _set + '.json')\n",
        "    gold = json.load(open(gold_path))\n",
        "    for kind in ['pipeline', 'end2end']:\n",
        "        for model in ['rand', 'major', 'transformer', 'rnn']:\n",
        "            if kind == 'end2end' and model in ['rand', 'major']:\n",
        "                continue\n",
        "            else:\n",
        "                p = os.path.join(path, 'results', kind, model, _set + '.out.postprocessed')\n",
        "                with open(p) as f:\n",
        "                    y_pred_ = f.read().split('\\n')[:-1]\n",
        "\n",
        "                y_real, y_pred = [], []\n",
        "                for i, g in enumerate(gold):\n",
        "                    if g['category'] in unseen_domains:\n",
        "                        targets = [nltk.word_tokenize(' '.join(target['output'])) for target in g['targets']]\n",
        "                        t = [' '.join(target).lower() for target in targets]\n",
        "                        y_real.append(t)\n",
        "                        pred = ' '.join(nltk.word_tokenize(y_pred_[i])).lower()\n",
        "                        y_pred.append(pred)\n",
        "\n",
        "\n",
        "                with open('predictions', 'w') as f:\n",
        "                    f.write('\\n'.join(y_pred))\n",
        "\n",
        "                with open('reference', 'w') as f:\n",
        "                    for refs in y_real:\n",
        "                        for i in range(8):\n",
        "                            if i < len(refs):\n",
        "                                f.write(refs[i])\n",
        "                            else:\n",
        "                                f.write('')\n",
        "                            f.write('\\n')\n",
        "\n",
        "                java = '/roaming/tcastrof/workspace/java/jre1.8.0_181/bin/java -Xmx2G -jar '\n",
        "                java += '/home/tcastrof/workspace/meteor-1.5/meteor-1.5.jar predictions reference -l en -norm -r 8'\n",
        "                result = subprocess.check_output(java, shell=True)\n",
        "                print('Final: ')\n",
        "                print('Set: ', _set)\n",
        "                print('Approach:', kind)\n",
        "                print('Model: ', model)\n",
        "                print(result.split(b'\\n')[-2])\n",
        "                print(10 * '-')\n",
        "\n",
        "                os.remove('reference')\n",
        "                os.remove('predictions')\n"
      ],
      "metadata": {
        "id": "bUhJAWG-sson"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Final Texts (Fluency and Semantic)"
      ],
      "metadata": {
        "id": "ZuMAwBPMsuD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats\n",
        "\n",
        "\n",
        "def mean_confidence_interval(data, confidence=0.95):\n",
        "    a = 1.0 * np.array(data)\n",
        "    n = len(a)\n",
        "    m, se = np.mean(a), scipy.stats.sem(a)\n",
        "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
        "    return m, h, m-h, m+h\n",
        "\n",
        "path = 'evaluation/human/grades.json'\n",
        "grades = json.load(open(path))\n",
        "\n",
        "path = 'evaluation/human/participants.json'\n",
        "participants = json.load(open(path))"
      ],
      "metadata": {
        "id": "MCr9jfiMswPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "grades = json.load(open('evaluation/human/ngrades.json'))\n",
        "participant_ids = set([w['participant_id'] for w in grades])\n",
        "print('Number of participants: ', len(participant_ids))\n",
        "\n",
        "participants = [p for p in participants if p['id'] in participant_ids]\n",
        "\n",
        "print('***Gender:***')\n",
        "print(Counter([p['gender'] for p in participants]))\n",
        "print('***English Proficiency Level:***')\n",
        "print(Counter([p['english_proficiency_level'] for p in participants]))\n",
        "print('***Age:***')\n",
        "print(round(np.mean([int(p['age']) for p in participants]), 2))\n",
        "print('\\n')\n",
        "\n",
        "print('All Domains')\n",
        "models = set([g['model'] for g in grades])\n",
        "print('Fluency: ')\n",
        "for model in ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']:\n",
        "    fluency = [float(g['fluency']) for g in grades if g['model'] == model]# and g['category'] in unseen_domains]\n",
        "    print('{0}: {1} +-{2}'.format(model, round(np.mean(fluency), 2), round(mean_confidence_interval(fluency)[1], 2)))\n",
        "print('\\n')\n",
        "print('Semantics: ')\n",
        "for model in ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']:\n",
        "    semantic = [float(g['semantic']) for g in grades if g['model'] == model]# and g['category'] in unseen_domains]\n",
        "    print('{0}: {1} +-{2}'.format(model, round(np.mean(semantic), 2), round(mean_confidence_interval(semantic)[1], 2)))"
      ],
      "metadata": {
        "id": "eg_D4BgGszy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Seen Domains')\n",
        "models = set([g['model'] for g in grades])\n",
        "print('Fluency: ')\n",
        "for model in ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']:\n",
        "    fluency = [float(g['fluency']) for g in grades if g['model'] == model and g['category'] not in unseen_domains]\n",
        "    print('{0}: {1} +-{2}'.format(model, round(np.mean(fluency), 2), round(mean_confidence_interval(fluency)[1], 2)))\n",
        "print('\\n')\n",
        "print('Semantics: ')\n",
        "for model in ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']:\n",
        "    semantic = [float(g['semantic']) for g in grades if g['model'] == model and g['category'] not in unseen_domains]\n",
        "    print('{0}: {1} +-{2}'.format(model, round(np.mean(semantic), 2), round(mean_confidence_interval(semantic)[1], 2)))"
      ],
      "metadata": {
        "id": "qb4-_j9hs2VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Unseen Domains')\n",
        "models = set([g['model'] for g in grades])\n",
        "print('Fluency: ')\n",
        "for model in ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']:\n",
        "    fluency = [float(g['fluency']) for g in grades if g['model'] == model and g['category'] in unseen_domains]\n",
        "    print('{0}: {1} +-{2}'.format(model, round(np.mean(fluency), 2), round(mean_confidence_interval(fluency)[1], 2)))\n",
        "print('\\n')\n",
        "print('Semantics: ')\n",
        "for model in ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']:\n",
        "    semantic = [float(g['semantic']) for g in grades if g['model'] == model and g['category'] in unseen_domains]\n",
        "    print('{0}: {1} +-{2}'.format(model, round(np.mean(semantic), 2), round(mean_confidence_interval(semantic)[1], 2)))"
      ],
      "metadata": {
        "id": "BM_JabUTs5g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mannwhitneyu, wilcoxon\n",
        "\n",
        "models = ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']\n",
        "for i, model1 in enumerate(models):\n",
        "    for j, model2 in enumerate(models):\n",
        "        if model1 != model2:\n",
        "            fluency1 = [float(g['semantic']) for g in grades if g['model'] == model1]# and g['category'] in unseen_domains]\n",
        "            fluency2 = [float(g['semantic']) for g in grades if g['model'] == model2]# and g['category'] in unseen_domains]\n",
        "            print(model1, 'x', model2, ':', round(mannwhitneyu(fluency1, fluency2)[1], 2) < 0.05)"
      ],
      "metadata": {
        "id": "chCOLSAHs7Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Annotations"
      ],
      "metadata": {
        "id": "DkRtQCUgs8iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gold_path='evaluation/questionaire/trials/gold.json'\n",
        "gold = json.load(open(gold_path))\n",
        "\n",
        "path='evaluation/questionaire/annotations.json'\n",
        "annotations = json.load(open(path))\n"
      ],
      "metadata": {
        "id": "vhIwfwt9s-E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in annotations:\n",
        "    values = {\n",
        "        'detmistake': 0,\n",
        "        'fluency': [],\n",
        "        'moreinformation': 0,\n",
        "        'numpreds': 0,\n",
        "        'referencemistake': 0,\n",
        "        'semantics': [],\n",
        "        'structurefollowed': 0,\n",
        "        'verbmistake': 0\n",
        "    }\n",
        "    dem = 0\n",
        "    for trial in annotations[model]:\n",
        "        g = [g for g in gold if g['eid'] == trial][0]\n",
        "        category = g['category']\n",
        "        size = g['size']\n",
        "#         if category in unseen_domains:\n",
        "        if annotations[model][trial]['structurefollowed']:\n",
        "            values['structurefollowed'] += 1\n",
        "        if annotations[model][trial]['moreinformation']:\n",
        "            values['moreinformation'] += 1\n",
        "        if annotations[model][trial]['referencemistake']:\n",
        "            values['referencemistake'] += 1\n",
        "        if annotations[model][trial]['verbmistake']:\n",
        "            values['verbmistake'] += 1\n",
        "        if annotations[model][trial]['detmistake']:\n",
        "            values['detmistake'] += 1\n",
        "        if int(size) == int(annotations[model][trial]['numpreds']):\n",
        "            values['numpreds'] += 1\n",
        "        dem += 1\n",
        "        values['fluency'].append(float(annotations[model][trial]['fluency']))\n",
        "        values['semantics'].append(float(annotations[model][trial]['semantics']))\n",
        "    values['structurefollowed'] /= dem\n",
        "    values['moreinformation'] /= dem\n",
        "    values['referencemistake'] /= dem\n",
        "    values['verbmistake'] /= dem\n",
        "    values['detmistake'] /= dem\n",
        "\n",
        "    print('Model: ', model)\n",
        "    print('Structured followed in {0} of the cases'.format(round(values['structurefollowed'], 2)))\n",
        "    print('More information in {0} of the cases'.format(round(values['moreinformation'], 2)))\n",
        "    print('Exact number of predicates in {0} out of {1} of the cases ({2})'.format(values['numpreds'], dem, round(values['numpreds']/dem, 2)))\n",
        "    print('Reference mistakes in {0} of the cases'.format(round(values['referencemistake'], 2)))\n",
        "    print('Verb mistakes in {0} of the cases'.format(round(values['verbmistake'], 2)))\n",
        "    print('Determiner mistakes in {0} of the cases'.format(round(values['detmistake'], 2)))\n",
        "    print('Fluency: {0}'.format(np.mean(values['fluency'])))\n",
        "    print('Semantics: {0}'.format(np.mean(values['semantics'])))\n",
        "    print(10 * '-')"
      ],
      "metadata": {
        "id": "4w97-LbOtBn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inter-annotator Agreement\n"
      ],
      "metadata": {
        "id": "NnXomrgVtCvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from nltk.metrics.agreement import AnnotationTask\n",
        "\n",
        "def kappa(obs):\n",
        "    t = AnnotationTask(obs)\n",
        "    print(\"\\nWeighted kappa as per NLTK:\\t\", t.weighted_kappa(),\n",
        "          \"\\nRegular kappa as per NLTK:\\t\", t.kappa(),\n",
        "          \"\\nKrippendorff alpha as per NLTK:\\t\", t.alpha(),\n",
        "          \"\\n===========================================\\n\")\n",
        "\n",
        "path='evaluation/questionaire/observations/'\n",
        "if not os.path.exists(path):\n",
        "    os.mkdir(path)\n",
        "\n",
        "ann1_model1 = annotations['models\\\\model1.xml']\n",
        "ann2_model1 = annotations['model1.xml']\n",
        "ann1_model3 = annotations['models\\\\model3.xml']\n",
        "ann2_model3 = annotations['model3.xml']\n",
        "\n",
        "obs_fluency = []\n",
        "for trial in ann1_model1:\n",
        "    obs_fluency.append(('ann1', 'model1_'+trial, int(ann1_model1[trial]['fluency'])))\n",
        "for trial in ann1_model3:\n",
        "    obs_fluency.append(('ann1', 'model3_'+trial, int(ann1_model3[trial]['fluency'])))\n",
        "for trial in ann2_model1:\n",
        "    obs_fluency.append(('ann2', 'model1_'+trial, int(ann2_model1[trial]['fluency'])))\n",
        "for trial in ann2_model3:\n",
        "    obs_fluency.append(('ann2', 'model3_'+trial, int(ann2_model3[trial]['fluency'])))\n",
        "\n",
        "print('Fluency:')\n",
        "kappa(obs_fluency)\n",
        "\n",
        "with open(os.path.join(path, 'fluency.csv'), 'w') as f:\n",
        "    for i, o in enumerate(obs_fluency):\n",
        "        obs_fluency[i] = list(obs_fluency[i])\n",
        "        obs_fluency[i][2] = str(obs_fluency[i][2])\n",
        "        obs_fluency[i] = ','.join(obs_fluency[i])\n",
        "    f.write('\\n'.join(obs_fluency))\n",
        "########################################################################################################\n",
        "obs_semantic = []\n",
        "for trial in ann1_model1:\n",
        "    obs_semantic.append(('ann1', 'model1_'+trial, int(ann1_model1[trial]['semantics'])))\n",
        "for trial in ann1_model3:\n",
        "    obs_semantic.append(('ann1', 'model3_'+trial, int(ann1_model3[trial]['semantics'])))\n",
        "for trial in ann2_model1:\n",
        "    obs_semantic.append(('ann2', 'model1_'+trial, int(ann2_model1[trial]['semantics'])))\n",
        "for trial in ann2_model3:\n",
        "    obs_semantic.append(('ann2', 'model3_'+trial, int(ann2_model3[trial]['semantics'])))\n",
        "\n",
        "print('Semantic:')\n",
        "kappa(obs_semantic)\n",
        "\n",
        "with open(os.path.join(path, 'semantic.csv'), 'w') as f:\n",
        "    for i, o in enumerate(obs_semantic):\n",
        "        obs_semantic[i] = list(obs_semantic[i])\n",
        "        obs_semantic[i][2] = str(obs_semantic[i][2])\n",
        "        obs_semantic[i] = ','.join(obs_semantic[i])\n",
        "    f.write('\\n'.join(obs_semantic))\n",
        "########################################################################################################\n",
        "obs_preds = []\n",
        "for trial in ann1_model1:\n",
        "    obs_preds.append(('1', 'model1_'+trial, int(ann1_model1[trial]['numpreds'])))\n",
        "for trial in ann1_model3:\n",
        "    obs_preds.append(('1', 'model3_'+trial, int(ann1_model3[trial]['numpreds'])))\n",
        "for trial in ann2_model1:\n",
        "    obs_preds.append(('2', 'model1_'+trial, int(ann2_model1[trial]['numpreds'])))\n",
        "for trial in ann2_model3:\n",
        "    obs_preds.append(('2', 'model3_'+trial, int(ann2_model3[trial]['numpreds'])))\n",
        "\n",
        "print('Predicates:')\n",
        "kappa(obs_preds)\n",
        "\n",
        "with open(os.path.join(path, 'predicates.csv'), 'w') as f:\n",
        "    for i, o in enumerate(obs_preds):\n",
        "        obs_preds[i] = list(obs_preds[i])\n",
        "        obs_preds[i][2] = str(obs_preds[i][2])\n",
        "        obs_preds[i] = ','.join(obs_preds[i])\n",
        "    f.write('\\n'.join(obs_preds))\n",
        "########################################################################################################\n",
        "obs = []\n",
        "for trial in ann1_model1:\n",
        "    obs.append(('1', 'model1_'+trial, 1 if ann1_model1[trial]['structurefollowed'] else 0))\n",
        "for trial in ann1_model3:\n",
        "    obs.append(('1', 'model3_'+trial, 1 if ann1_model3[trial]['structurefollowed'] else 0))\n",
        "for trial in ann2_model1:\n",
        "    obs.append(('2', 'model1_'+trial, 1 if ann2_model1[trial]['structurefollowed'] else 0))\n",
        "for trial in ann2_model3:\n",
        "    obs.append(('2', 'model3_'+trial, 1 if ann2_model3[trial]['structurefollowed'] else 0))\n",
        "\n",
        "print('Structure Followed:')\n",
        "kappa(obs)\n",
        "\n",
        "with open(os.path.join(path, 'structfollowed.csv'), 'w') as f:\n",
        "    for i, o in enumerate(obs):\n",
        "        obs[i] = list(obs[i])\n",
        "        obs[i][2] = str(obs[i][2])\n",
        "        obs[i] = ','.join(obs[i])\n",
        "    f.write('\\n'.join(obs))\n",
        "########################################################################################################\n",
        "obs = []\n",
        "for trial in ann1_model1:\n",
        "    obs.append(('1', 'model1_'+trial, 1 if ann1_model1[trial]['moreinformation'] else 0))\n",
        "for trial in ann1_model3:\n",
        "    obs.append(('1', 'model3_'+trial, 1 if ann1_model3[trial]['moreinformation'] else 0))\n",
        "for trial in ann2_model1:\n",
        "    obs.append(('2', 'model1_'+trial, 1 if ann2_model1[trial]['moreinformation'] else 0))\n",
        "for trial in ann2_model3:\n",
        "    obs.append(('2', 'model3_'+trial, 1 if ann2_model3[trial]['moreinformation'] else 0))\n",
        "\n",
        "print('Overgeneration:')\n",
        "kappa(obs)\n",
        "\n",
        "with open(os.path.join(path, 'overgeneration.csv'), 'w') as f:\n",
        "    for i, o in enumerate(obs):\n",
        "        obs[i] = list(obs[i])\n",
        "        obs[i][2] = str(obs[i][2])\n",
        "        obs[i] = ','.join(obs[i])\n",
        "    f.write('\\n'.join(obs))\n",
        "########################################################################################################\n",
        "obs = []\n",
        "for trial in ann1_model1:\n",
        "    obs.append(('1', 'model1_'+trial, 1 if ann1_model1[trial]['verbmistake'] else 0))\n",
        "for trial in ann1_model3:\n",
        "    obs.append(('1', 'model3_'+trial, 1 if ann1_model3[trial]['verbmistake'] else 0))\n",
        "for trial in ann2_model1:\n",
        "    obs.append(('2', 'model1_'+trial, 1 if ann2_model1[trial]['verbmistake'] else 0))\n",
        "for trial in ann2_model3:\n",
        "    obs.append(('2', 'model3_'+trial, 1 if ann2_model3[trial]['verbmistake'] else 0))\n",
        "\n",
        "print('Verb mistakes:')\n",
        "kappa(obs)\n",
        "\n",
        "with open(os.path.join(path, 'verbmistakes.csv'), 'w') as f:\n",
        "    for i, o in enumerate(obs):\n",
        "        obs[i] = list(obs[i])\n",
        "        obs[i][2] = str(obs[i][2])\n",
        "        obs[i] = ','.join(obs[i])\n",
        "    f.write('\\n'.join(obs))\n",
        "########################################################################################################\n",
        "obs = []\n",
        "for trial in ann1_model1:\n",
        "    obs.append(('1', 'model1_'+trial, 1 if ann1_model1[trial]['detmistake'] else 0))\n",
        "for trial in ann1_model3:\n",
        "    obs.append(('1', 'model3_'+trial, 1 if ann1_model3[trial]['detmistake'] else 0))\n",
        "for trial in ann2_model1:\n",
        "    obs.append(('2', 'model1_'+trial, 1 if ann2_model1[trial]['detmistake'] else 0))\n",
        "for trial in ann2_model3:\n",
        "    obs.append(('2', 'model3_'+trial, 1 if ann2_model3[trial]['detmistake'] else 0))\n",
        "\n",
        "print('Determiner mistakes:')\n",
        "kappa(obs)\n",
        "\n",
        "with open(os.path.join(path, 'detmistakes.csv'), 'w') as f:\n",
        "    for i, o in enumerate(obs):\n",
        "        obs[i] = list(obs[i])\n",
        "        obs[i][2] = str(obs[i][2])\n",
        "        obs[i] = ','.join(obs[i])\n",
        "    f.write('\\n'.join(obs))\n",
        "########################################################################################################\n",
        "obs = []\n",
        "for trial in ann1_model1:\n",
        "    obs.append(('1', 'model1_'+trial, 1 if ann1_model1[trial]['referencemistake'] else 0))\n",
        "for trial in ann1_model3:\n",
        "    obs.append(('1', 'model3_'+trial, 1 if ann1_model3[trial]['referencemistake'] else 0))\n",
        "for trial in ann2_model1:\n",
        "    obs.append(('2', 'model1_'+trial, 1 if ann2_model1[trial]['referencemistake'] else 0))\n",
        "for trial in ann2_model3:\n",
        "    obs.append(('2', 'model3_'+trial, 1 if ann2_model3[trial]['referencemistake'] else 0))\n",
        "\n",
        "print('Reference mistakes:')\n",
        "kappa(obs)\n",
        "\n",
        "with open(os.path.join(path, 'refmistakes.csv'), 'w') as f:\n",
        "    for i, o in enumerate(obs):\n",
        "        obs[i] = list(obs[i])\n",
        "        obs[i][2] = str(obs[i][2])\n",
        "        obs[i] = ','.join(obs[i])\n",
        "    f.write('\\n'.join(obs))"
      ],
      "metadata": {
        "id": "LaPkwAQLtKV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_triples(text):\n",
        "    triples, triple = [], []\n",
        "    for w in text:\n",
        "        if w not in ['<TRIPLE>', '</TRIPLE>']:\n",
        "            triple.append(w)\n",
        "        elif w == '</TRIPLE>':\n",
        "            triples.append(triple)\n",
        "            triple = []\n",
        "    return triples\n",
        "\n",
        "def ordering_analysis(ordering, gold):\n",
        "    for i, entry in enumerate(gold):\n",
        "        triples = split_triples(entry['source'])\n",
        "\n",
        "        num, visited = 0, []\n",
        "        for triple in triples:\n",
        "            for j, predicate in enumerate(ordering[i]):\n",
        "                if predicate == triple[1] and j not in visited:\n",
        "                    num += 1\n",
        "                    visited.append(j)\n",
        "        # How many predicates in the modified tripleset are present in the result?\n",
        "        entry['ordering'] = num\n",
        "    return gold\n",
        "\n",
        "\n",
        "def structing_analysis(structing, gold):\n",
        "    for i, entry in enumerate(self.gold):\n",
        "        triples = split_triples(entry['source'])\n",
        "\n",
        "        num, visited = 0, []\n",
        "        for triple in triples:\n",
        "            for j, predicate in enumerate(structing[i]):\n",
        "                if predicate == triple[1] and j not in visited:\n",
        "                    num += 1\n",
        "                    visited.append(j)\n",
        "        # How many predicates in the modified tripleset are present in the result?\n",
        "        entry['structing'] = num\n",
        "    return gold"
      ],
      "metadata": {
        "id": "8w6rZEUAtQTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'results/questionaire/partial.txt'\n",
        "with open(path) as f:\n",
        "    eids = f.read().split()\n",
        "\n",
        "gold_path='evaluation/questionaire/gold.json'\n",
        "gold = json.load(open(gold_path))\n",
        "\n",
        "p = 'evaluation/results/pipeline/transformer/test.structing.postprocessed'\n",
        "with open(p) as f:\n",
        "    ordering = f.read().split('\\n')[:-1]\n",
        "\n",
        "pos, dem = 0, 0\n",
        "for i, entry in enumerate(gold):\n",
        "    if entry['eid'] in eids:\n",
        "        category = entry['category']\n",
        "#         if category in unseen_domains:\n",
        "        triples = split_triples(entry['source'])\n",
        "        num, visited = 0, []\n",
        "        for triple in triples:\n",
        "            for j, predicate in enumerate(ordering[i].split()):\n",
        "                if predicate == triple[1] and j not in visited:\n",
        "                    num += 1\n",
        "                    visited.append(j)\n",
        "                if len(visited) == len(triples):\n",
        "                    break\n",
        "        if len(triples) == len(visited):\n",
        "            pos += 1\n",
        "        dem += 1\n",
        "\n",
        "print('Exact number of predicates in {0} out of {1} of the cases ({2})'.format(pos, dem, round(pos/dem, 2)))\n"
      ],
      "metadata": {
        "id": "C1m94ri2quaz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}